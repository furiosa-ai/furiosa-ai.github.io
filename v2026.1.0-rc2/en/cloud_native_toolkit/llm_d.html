
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HTTHGM3MD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-0HTTHGM3MD');
    </script>
    
    <title>Deploying Furiosa-LLM with llm-d &#8212; FuriosaAI Developer Center 2026.1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=37f7f57c" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=d0d2eeda"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'cloud_native_toolkit/llm_d';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/furiosa-ai/furiosa-ai.github.io/refs/heads/main/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '2026.1.0';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            true;
        </script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Furiosa SMI" href="../device_management/system_management_interface.html" />
    <link rel="prev" title="Installing Furiosa NPU Operator" href="kubernetes/npu_operator.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="2026.1.0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/doc-logo-dark.svg" class="logo__image only-light" alt=""/>
    <img src="../_static/doc-logo-light.svg" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">
            <div class='sidebar-title mr-auto'>
                Furiosa Docs
            </div>
        </p>
  
</a></div>
        <div class="sidebar-primary-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/rngd.html">FuriosaAI RNGD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/software_stack.html">FuriosaAI’s Software Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/supported_models.html">Supported Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../whatsnew/index.html">What’s New</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../whatsnew/release-2026.1.html">Announcing Furiosa SDK Release 2026.1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../whatsnew/release-2025.html">Release Notes for Furiosa SDK Release 2025.X</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/roadmap.html">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/prerequisites.html">Installing Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/furiosa_llm.html">Quick Start with Furiosa-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/upgrade_guide.html">Upgrading FuriosaAI’s Software</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Furiosa-LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/intro.html">Furiosa-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/furiosa-llm-serve.html">OpenAI-Compatible Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/structured-output.html">Structured Output</a></li>
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/prefix-caching.html">Prefix Caching</a></li>
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/model-preparation.html">Model Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/model-parallelism.html">Model Parallelism</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../furiosa_llm/reference.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/reference/llm.html">LLM class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/reference/sampling_params.html">SamplingParams class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/reference/pooling_params.html">PoolingParams class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/reference/artifact_builder.html">ArtifactBuilder</a></li>


<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/reference/llm_engine.html">LLMEngine class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/reference/async_llm_engine.html">AsyncLLMEngine class</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../furiosa_llm/examples.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/examples/llm_chat.html">Chat</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/examples/llm_chat_with_tools.html">Chat with tools</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/examples/llm_embed.html">Embedding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/examples/llm_score.html">Scoring (Similarity Scoring)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/examples/llm_rerank.html">Reranking (Document Reranking)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../furiosa_llm/examples/online_chat_completion_logprobs.html">OpenAI-Compatible API with Logprobs</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../furiosa_llm/k8s_deployment.html">Deploying Furiosa-LLM on Kubernetes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cloud Native Toolkit</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Cloud Native Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="container.html">Container Support</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="kubernetes.html">Kubernetes Plugins</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="kubernetes/feature_discovery.html">Installing Furiosa Feature Discovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes/device_plugin.html">Installing Furiosa Device Plugin</a></li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes/dra_driver.html">Installing Furiosa DRA Driver</a></li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes/metrics_exporter.html">Installing Furiosa Metrics Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="kubernetes/npu_operator.html">Installing Furiosa NPU Operator</a></li>
</ul>
</details></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Deploying Furiosa-LLM with llm-d</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../device_management/system_management_interface.html">Furiosa SMI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_cli.html">Furiosa SMI CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_lib.html">Furiosa SMI Library</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../device_management/host_tuning.html">Host PCI Optimization Tuning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Tutorials and Examples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://github.com/furiosa-ai/sdk-cookbook">FuriosaAI SDK CookBook</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://forums.furiosa.ai">Forums</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">Customer Support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://furiosa.ai">FuriosaAI Homepage</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/latest/en/">Furiosa Gen 1 NPU SDK Doc</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item"><img id='furiosa_logo' width="100" /></div>
      <div class="sidebar-primary-item">

  <p class="copyright">
    
      © Copyright 2026 FuriosaAI Inc.
      <br/>
    
  </p>
</div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button></div>
      
        <div class="header-article-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="header-article-item"><button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Deploying Furiosa-LLM with llm-d</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2>  </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-d-integration-with-furiosa-llm">llm-d integration with Furiosa-LLM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deploying Furiosa-LLM with llm-d</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-d-modelservice-and-helm-repository">llm-d-modelservice and Helm repository</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-set-up-gateway-api-crds">Step 1: Set up Gateway API CRDs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-deploy-gie-compatible-gateway">Step 2: Deploy GIE-compatible Gateway</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-prepare-kubernetes-secret">Step 3: Prepare Kubernetes Secret</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-download-the-target-llm-model">Step 4: Download the Target LLM Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-deploy-llm-d">Step 5: Deploy llm-d</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-expose-the-service-with-an-httproute">Step 6: Expose the service with an HTTPRoute</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="deploying-furiosa-llm-with-llm-d">
<span id="llm-d"></span><h1>Deploying Furiosa-LLM with llm-d<a class="headerlink" href="#deploying-furiosa-llm-with-llm-d" title="Link to this heading">#</a></h1>
<p><a class="reference external" href="https://llm-d.ai/">llm-d</a> is a Kubernetes-native distributed inference framework that lets you serve LLM models across a cluster. Adopting llm-d as a distributed inference framework can provide the following benefits:</p>
<ul class="simple">
<li><p><strong>Intelligent Inference Scheduling</strong>: llm-d provides a configurable load balancer and pluggable scorers to route serving requests to optimal pods. llm-d provides metrics-based scoring and prefix-cache-aware scorers.</p></li>
<li><p><strong>Prefill/Decode Disaggregation</strong>: llm-d selects optimal Prefill and Decode pods and relays KV Cache transfers between the designated pods.</p></li>
<li><p><strong>Wide Expert-Parallelism</strong>: llm-d supports wide expert parallelism to deploy large Mixture-of-Experts (MoE) models.</p></li>
</ul>
<section id="llm-d-integration-with-furiosa-llm">
<h2>llm-d integration with Furiosa-LLM<a class="headerlink" href="#llm-d-integration-with-furiosa-llm" title="Link to this heading">#</a></h2>
<p>Furiosa-LLM can be integrated with llm-d to support distributed serving of LLM models using RNGDs.
Currently, the following integrations are supported:</p>
<ul>
<li><p><strong>Intelligent Inference Scheduling</strong>
Furiosa-LLM implements Model Server Protocol’s <a class="reference external" href="https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/main/docs/proposals/003-model-server-protocol#metrics-reporting">metrics reporting</a> to support Intelligent Inference Scheduling. The corresponding metrics are as follows:</p>
<div class="pst-scrollable-table-container"><table class="table" id="id2">
<caption><span class="caption-text">Model Server Protocol metrics</span><a class="headerlink" href="#id2" title="Link to this table">#</a></caption>
<colgroup>
<col style="width: 25.0%" />
<col style="width: 75.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Furiosa-LLM Metric</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>TotalQueuedRequests</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm_num_requests_waiting</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>TotalRunningRequests</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm_num_requests_running</span></code></p></td>
</tr>
<tr class="row-even"><td><p>KVCacheUtilization</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm_kv_cache_usage_percent</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>BlockSize</p></td>
<td><div class="line-block">
<div class="line">Name: <code class="docutils literal notranslate"><span class="pre">furiosa_llm_cache_config_info</span></code></div>
<div class="line">Label: <code class="docutils literal notranslate"><span class="pre">block_size</span></code></div>
</div>
</td>
</tr>
<tr class="row-even"><td><p>NumGPUBlocks</p></td>
<td><div class="line-block">
<div class="line">Name: <code class="docutils literal notranslate"><span class="pre">furiosa_llm_cache_config_info</span></code></div>
<div class="line">Label: <code class="docutils literal notranslate"><span class="pre">num_gpu_blocks</span></code></div>
</div>
</td>
</tr>
</tbody>
</table>
</div>
</li>
</ul>
<p>The following integrations are not currently supported:</p>
<ul class="simple">
<li><p><strong>Precise Prefix-Cache-Aware Scoring</strong>: Furiosa-LLM currently does not implement KV Cache events.</p></li>
<li><p><strong>Prefill/Decode Disaggregation</strong>: Furiosa-LLM currently does not support prefill/decode disaggregation.</p></li>
<li><p><strong>Wide Expert-Parallelism</strong>: Furiosa-LLM currently does not support wide expert parallelism.</p></li>
</ul>
</section>
<section id="id1">
<h2>Deploying Furiosa-LLM with llm-d<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
<p>This section describes how to deploy Furiosa-LLM with llm-d. The deployed llm-d will have Intelligent Inference Scheduling enabled, enabling metric-based request routing. This guide is based on llm-d’s <a class="reference external" href="https://github.com/llm-d/llm-d/blob/v0.5.0/guides/inference-scheduling/README.md">Well-lit Path: Intelligent Inference Scheduling</a> guide.</p>
<section id="prerequisites">
<h3>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>A Kubernetes cluster equipped with two or more Furiosa RNGD devices.</p></li>
<li><p>Hugging Face account and access token.</p></li>
<li><p>A Kubernetes storage class which supports dynamic volume provisioning.</p></li>
</ul>
<p>For detailed instructions on setting up an RNGD cluster, please refer to <a class="reference internal" href="../get_started/prerequisites.html#installingprerequisites"><span class="std std-ref">Installing Prerequisites</span></a> and <a class="reference internal" href="kubernetes.html#kubernetes"><span class="std std-ref">Kubernetes Plugins</span></a>.</p>
<p>You will also install Gateway API and a GIE-compatible gateway (Istio) in the steps below.</p>
</section>
<section id="llm-d-modelservice-and-helm-repository">
<h3>llm-d-modelservice and Helm repository<a class="headerlink" href="#llm-d-modelservice-and-helm-repository" title="Link to this heading">#</a></h3>
<p>llm-d provides the llm-d-modelservice Helm chart to simplify LLM deployment. We provide a fork of that chart to run Furiosa-LLM on RNGDs. Add the Furiosa Helm repository now so it is available when you deploy the model server in Step 5:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>furiosa<span class="w"> </span>https://furiosa-ai.github.io/helm-charts
helm<span class="w"> </span>repo<span class="w"> </span>update
</pre></div>
</div>
</section>
<section id="step-1-set-up-gateway-api-crds">
<h3>Step 1: Set up Gateway API CRDs<a class="headerlink" href="#step-1-set-up-gateway-api-crds" title="Link to this heading">#</a></h3>
<p>llm-d utilizes Kubernetes’ <a class="reference external" href="https://gateway-api-inference-extension.sigs.k8s.io/">Gateway API Inference Extension (GIE)</a>. Appropriate CRDs need to be installed in the cluster.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># Install Gateway API CRDs</span>
kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>https://github.com/kubernetes-sigs/gateway-api/releases/download/v1.4.1/standard-install.yaml
<span class="c1"># Install Gateway API Inference Extension CRDs</span>
kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>https://github.com/kubernetes-sigs/gateway-api-inference-extension/releases/download/v1.3.0/manifests.yaml
</pre></div>
</div>
</section>
<section id="step-2-deploy-gie-compatible-gateway">
<h3>Step 2: Deploy GIE-compatible Gateway<a class="headerlink" href="#step-2-deploy-gie-compatible-gateway" title="Link to this heading">#</a></h3>
<p>A list of GIE-compatible gateways can be found <a class="reference external" href="https://gateway-api-inference-extension.sigs.k8s.io/implementations/gateways/">here</a>.
For this guide, we will deploy Istio as the GIE-compatible gateway. Install the base chart first, then the discovery chart (istiod) with GIE enabled.</p>
<p>Add the Istio Helm repository and install istio-base:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>istio<span class="w"> </span>https://istio-release.storage.googleapis.com/charts
helm<span class="w"> </span>repo<span class="w"> </span>update
helm<span class="w"> </span>install<span class="w"> </span>istio-base<span class="w"> </span>istio/base<span class="w"> </span>-n<span class="w"> </span>istio-system<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--set<span class="w"> </span><span class="nv">defaultRevision</span><span class="o">=</span>default<span class="w"> </span>--create-namespace
</pre></div>
</div>
<p>Install the Istio discovery chart with GIE support:</p>
<div class="literal-block-wrapper docutils container" id="id3">
<div class="code-block-caption"><span class="caption-text">istiod.yaml</span><a class="headerlink" href="#id3" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">meshConfig</span><span class="p">:</span>
<span class="w">  </span><span class="nt">defaultConfig</span><span class="p">:</span>
<span class="w">    </span><span class="nt">proxyMetadata</span><span class="p">:</span>
<span class="w">      </span><span class="nt">ENABLE_GATEWAY_API_INFERENCE_EXTENSION</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<span class="nt">pilot</span><span class="p">:</span>
<span class="w">  </span><span class="nt">env</span><span class="p">:</span>
<span class="w">    </span><span class="nt">ENABLE_GATEWAY_API_INFERENCE_EXTENSION</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm<span class="w"> </span>install<span class="w"> </span>istiod<span class="w"> </span>istio/istiod<span class="w"> </span>-f<span class="w"> </span>/path/to/istiod.yaml<span class="w"> </span>-n<span class="w"> </span>istio-system<span class="w"> </span>--wait
</pre></div>
</div>
</section>
<section id="step-3-prepare-kubernetes-secret">
<h3>Step 3: Prepare Kubernetes Secret<a class="headerlink" href="#step-3-prepare-kubernetes-secret" title="Link to this heading">#</a></h3>
<p>We will use <code class="docutils literal notranslate"><span class="pre">llm-d</span></code> namespace to deploy llm-d with llm-d-modelservice Helm chart.
Create a Kubernetes secret for your Hugging Face token:</p>
<div class="literal-block-wrapper docutils container" id="id4">
<div class="code-block-caption"><span class="caption-text">hf-secret.yaml</span><a class="headerlink" href="#id4" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Secret</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hf-token-secret</span>
<span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Opaque</span>
<span class="nt">data</span><span class="p">:</span>
<span class="w">  </span><span class="nt">HF_TOKEN</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">&lt;your_base64_encoded_hf_token&gt;</span>
</pre></div>
</div>
</div>
<p>Encode your token using the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="nb">echo</span><span class="w"> </span>-n<span class="w"> </span><span class="s1">&#39;&lt;your_HF_TOKEN&gt;&#39;</span><span class="w"> </span><span class="p">|</span><span class="w"> </span>base64
</pre></div>
</div>
<p>Then apply the secret:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>create<span class="w"> </span>namespace<span class="w"> </span>llm-d
kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>/path/to/hf-secret.yaml<span class="w"> </span>-n<span class="w"> </span>llm-d
</pre></div>
</div>
</section>
<section id="step-4-download-the-target-llm-model">
<h3>Step 4: Download the Target LLM Model<a class="headerlink" href="#step-4-download-the-target-llm-model" title="Link to this heading">#</a></h3>
<p>Furiosa-LLM deployed using llm-d-modelservice can utilize PVCs to pre-download Hugging Face models.
In this guide, we will pre-download the <code class="docutils literal notranslate"><span class="pre">furiosa-ai/Llama-3.1-8B-Instruct</span></code> model from Hugging Face Hub.</p>
<p>Create a PVC to store the model:</p>
<div class="literal-block-wrapper docutils container" id="id5">
<div class="code-block-caption"><span class="caption-text">model-pvc.yaml</span><a class="headerlink" href="#id5" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PersistentVolumeClaim</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-pvc</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">accessModes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ReadWriteOnce</span>
<span class="w">  </span><span class="nt">resources</span><span class="p">:</span>
<span class="w">    </span><span class="nt">requests</span><span class="p">:</span>
<span class="w">      </span><span class="nt">storage</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20Gi</span>
<span class="w">  </span><span class="c1"># The storage class &quot;default&quot; is an example, use an appropriate one for your cluster.</span>
<span class="w">  </span><span class="nt">storageClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">default</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>/path/to/model-pvc.yaml<span class="w"> </span>-n<span class="w"> </span>llm-d
</pre></div>
</div>
<p>Create a pod to download the model:</p>
<div class="literal-block-wrapper docutils container" id="id6">
<div class="code-block-caption"><span class="caption-text">model-downloader.yaml</span><a class="headerlink" href="#id6" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Pod</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-downloader</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">volumes</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-storage</span>
<span class="w">      </span><span class="nt">persistentVolumeClaim</span><span class="p">:</span>
<span class="w">        </span><span class="nt">claimName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-pvc</span>

<span class="w">  </span><span class="nt">initContainers</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">download-model</span>
<span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">python:3.10-slim</span>
<span class="w">      </span><span class="nt">env</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HF_TOKEN</span>
<span class="w">          </span><span class="nt">valueFrom</span><span class="p">:</span>
<span class="w">            </span><span class="nt">secretKeyRef</span><span class="p">:</span>
<span class="w">              </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">hf-token-secret</span>
<span class="w">              </span><span class="nt">key</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HF_TOKEN</span>
<span class="w">      </span><span class="nt">command</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/bin/bash</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">-c</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="p p-Indicator">&gt;</span>
<span class="w">          </span><span class="no">pip install --no-cache-dir huggingface_hub &amp;&amp;</span>
<span class="w">          </span><span class="no">python -c &quot;from huggingface_hub import snapshot_download; snapshot_download(repo_id=&#39;furiosa-ai/Llama-3.1-8B-Instruct&#39;, cache_dir=&#39;/models&#39;)&quot;</span>

<span class="w">      </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-storage</span>
<span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/models</span>

<span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-consumer</span>
<span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ubuntu</span>
<span class="w">      </span><span class="nt">command</span><span class="p">:</span><span class="w"> </span><span class="p p-Indicator">[</span><span class="s">&quot;sleep&quot;</span><span class="p p-Indicator">,</span><span class="w"> </span><span class="s">&quot;infinity&quot;</span><span class="p p-Indicator">]</span>
<span class="w">      </span><span class="nt">volumeMounts</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">model-storage</span>
<span class="w">          </span><span class="nt">mountPath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/models</span>

<span class="w">  </span><span class="nt">restartPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Never</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>/path/to/model-downloader.yaml<span class="w"> </span>-n<span class="w"> </span>llm-d
</pre></div>
</div>
<p>Wait for the model download to complete before proceeding (e.g. check the pod status with <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">get</span> <span class="pre">pods</span> <span class="pre">-n</span> <span class="pre">llm-d</span></code> and logs with <code class="docutils literal notranslate"><span class="pre">kubectl</span> <span class="pre">logs</span> <span class="pre">model-downloader</span> <span class="pre">-n</span> <span class="pre">llm-d</span> <span class="pre">-c</span> <span class="pre">download-model</span></code>).
For detailed instructions on how to pre-download a model on PVCs, please refer to the <a class="reference external" href="https://github.com/llm-d-incubation/llm-d-modelservice/blob/llm-d-modelservice-v0.3.18/examples/pvc/README.md">upstream documentation</a>.</p>
</section>
<section id="step-5-deploy-llm-d">
<h3>Step 5: Deploy llm-d<a class="headerlink" href="#step-5-deploy-llm-d" title="Link to this heading">#</a></h3>
<p>First, we need to deploy the llm-d Inference Scheduler to the namespace using the Helm chart. Prior to deploying the Inference Scheduler, the llm-d-infra Helm chart must be deployed.</p>
<div class="literal-block-wrapper docutils container" id="id7">
<div class="code-block-caption"><span class="caption-text">llm-d-infra.yaml</span><a class="headerlink" href="#id7" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">gateway</span><span class="p">:</span>
<span class="w">  </span><span class="nt">gatewayClassName</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istio</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>bitnami<span class="w"> </span>https://charts.bitnami.com/bitnami
helm<span class="w"> </span>repo<span class="w"> </span>add<span class="w"> </span>llm-d-infra<span class="w"> </span>https://llm-d-incubation.github.io/llm-d-infra/
helm<span class="w"> </span>repo<span class="w"> </span>update
helm<span class="w"> </span>install<span class="w"> </span>llm-d-infra<span class="w"> </span>llm-d-infra/llm-d-infra<span class="w"> </span>-n<span class="w"> </span>llm-d<span class="w"> </span>-f<span class="w"> </span>/path/to/llm-d-infra.yaml
</pre></div>
</div>
<p>Then, deploy the llm-d Inference Scheduler using the Helm chart. Set the flags to the respective Model Server Protocol metrics of Furiosa-LLM to enable metric-based request routing.</p>
<div class="literal-block-wrapper docutils container" id="id8">
<div class="code-block-caption"><span class="caption-text">inference-scheduler.yaml</span><a class="headerlink" href="#id8" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">inferenceExtension</span><span class="p">:</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">  </span><span class="nt">flags</span><span class="p">:</span>
<span class="w">    </span><span class="nt">cache-info-metric</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa_llm_cache_config_info&quot;</span>
<span class="w">    </span><span class="nt">kv-cache-usage-percentage-metric</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa_llm_kv_cache_usage_percent&quot;</span>
<span class="w">    </span><span class="nt">lora-info-metric</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;&quot;</span>
<span class="w">    </span><span class="nt">total-queued-requests-metric</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa_llm_num_requests_waiting&quot;</span>
<span class="w">    </span><span class="nt">total-running-requests-metric</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa_llm_num_requests_running&quot;</span>
<span class="w">  </span><span class="nt">image</span><span class="p">:</span>
<span class="w">    </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm-d-inference-scheduler</span>
<span class="w">    </span><span class="nt">hub</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ghcr.io/llm-d</span>
<span class="w">    </span><span class="nt">tag</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">v0.5.0</span>
<span class="w">    </span><span class="nt">pullPolicy</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Always</span>
<span class="w">  </span><span class="nt">extProcPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">9002</span>
<span class="w">  </span><span class="nt">pluginsConfigFile</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;default-plugins.yaml&quot;</span>
<span class="w">  </span><span class="nt">monitoring</span><span class="p">:</span>
<span class="w">    </span><span class="nt">interval</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;10s&quot;</span>
<span class="w">    </span><span class="c1"># Prometheus ServiceMonitor will be created when enabled for EPP metrics collection</span>
<span class="w">    </span><span class="nt">secret</span><span class="p">:</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">inference-scheduling-gateway-sa-metrics-reader-secret</span>
<span class="w">    </span><span class="nt">prometheus</span><span class="p">:</span>
<span class="w">      </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">auth</span><span class="p">:</span>
<span class="w">        </span><span class="c1"># To allow unauthenticated /metrics access (e.g., for debugging with curl), set to false</span>
<span class="w">        </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="nt">inferencePool</span><span class="p">:</span>
<span class="w">  </span><span class="nt">targetPorts</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">number</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>
<span class="w">  </span><span class="nt">modelServerType</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">vllm</span>
<span class="w">  </span><span class="nt">modelServers</span><span class="p">:</span>
<span class="w">    </span><span class="nt">matchLabels</span><span class="p">:</span>
<span class="w">      </span><span class="nt">llm-d.ai/inferenceServing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<span class="nt">provider</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">istio</span>
<span class="w">  </span><span class="nt">istio</span><span class="p">:</span>
<span class="w">    </span><span class="nt">destinationRule</span><span class="p">:</span>
<span class="w">      </span><span class="nt">host</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;gaie-epp.llm-d.svc.cluster.local&quot;</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm<span class="w"> </span>install<span class="w"> </span>gaie<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>oci://registry.k8s.io/gateway-api-inference-extension/charts/inferencepool<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--version<span class="w"> </span>v1.3.0<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-n<span class="w"> </span>llm-d<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-f<span class="w"> </span>/path/to/inference-scheduler.yaml
</pre></div>
</div>
<p>llm-d’s Inference Scheduler is deployed using GIE’s InferencePool helm chart. For further information on customizing the deployment, please refer to the <a class="reference external" href="https://github.com/kubernetes-sigs/gateway-api-inference-extension/tree/v1.3.0/config/charts/inferencepool">InferencePool helm chart documentation</a>.</p>
<p>Finally, deploy the llm-d-modelservice Helm chart to run the Furiosa-LLM model server.</p>
<div class="literal-block-wrapper docutils container" id="id9">
<div class="code-block-caption"><span class="caption-text">llm-d-modelservice.yaml</span><a class="headerlink" href="#id9" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">multinode</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="nt">modelArtifacts</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa-ai/Llama-3.1-8B-Instruct&quot;</span>
<span class="w">  </span><span class="nt">uri</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;pvc+hf://model-pvc/furiosa-ai/Llama-3.1-8B-Instruct&quot;</span>
<span class="w">  </span><span class="nt">size</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">20Gi</span>
<span class="w">  </span><span class="nt">authSecretName</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;hf-token-secret&quot;</span>
<span class="w">  </span><span class="nt">labels</span><span class="p">:</span>
<span class="w">    </span><span class="nt">llm-d.ai/inferenceServing</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;true&quot;</span>
<span class="w">    </span><span class="nt">llm-d.ai/model</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;Llama-3.1-8B-Instruct&quot;</span>

<span class="nt">accelerator</span><span class="p">:</span>
<span class="w">  </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa&quot;</span>

<span class="nt">routing</span><span class="p">:</span>
<span class="w">  </span><span class="nt">proxy</span><span class="p">:</span>
<span class="w">    </span><span class="nt">enabled</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>
<span class="w">    </span><span class="nt">targetPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>

<span class="nt">prefill</span><span class="p">:</span>
<span class="w">  </span><span class="nt">create</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">false</span>

<span class="nt">decode</span><span class="p">:</span>
<span class="w">  </span><span class="nt">parallelism</span><span class="p">:</span>
<span class="w">    </span><span class="nt">tensor</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="w">  </span><span class="nt">replicas</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">  </span><span class="nt">containers</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="s">&quot;furiosa-llm&quot;</span>
<span class="w">      </span><span class="nt">image</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">furiosaai/furiosa-llm:latest</span>
<span class="w">      </span><span class="nt">modelCommand</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">furiosaLLMServe</span>
<span class="w">      </span><span class="nt">args</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--enable-prefix-caching</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">--disable-uvicorn-access-log</span>
<span class="w">      </span><span class="nt">ports</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">containerPort</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>
<span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">furiosa-llm</span>
<span class="w">          </span><span class="nt">protocol</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">TCP</span>
<span class="w">      </span><span class="nt">mountModelVolume</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="w">      </span><span class="nt">startupProbe</span><span class="p">:</span>
<span class="w">        </span><span class="nt">httpGet</span><span class="p">:</span>
<span class="w">          </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/v1/models</span>
<span class="w">          </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">furiosa-llm</span>
<span class="w">        </span><span class="nt">initialDelaySeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">15</span>
<span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">30</span>
<span class="w">        </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">        </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">60</span>
<span class="w">      </span><span class="nt">livenessProbe</span><span class="p">:</span>
<span class="w">        </span><span class="nt">httpGet</span><span class="p">:</span>
<span class="w">          </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/health</span>
<span class="w">          </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">furiosa-llm</span>
<span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">10</span>
<span class="w">        </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">        </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
<span class="w">      </span><span class="nt">readinessProbe</span><span class="p">:</span>
<span class="w">        </span><span class="nt">httpGet</span><span class="p">:</span>
<span class="w">          </span><span class="nt">path</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/v1/models</span>
<span class="w">          </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">furiosa-llm</span>
<span class="w">        </span><span class="nt">periodSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">5</span>
<span class="w">        </span><span class="nt">timeoutSeconds</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">2</span>
<span class="w">        </span><span class="nt">failureThreshold</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">3</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>helm<span class="w"> </span>install<span class="w"> </span>ms<span class="w"> </span>furiosa/llm-d-modelservice<span class="w"> </span>-n<span class="w"> </span>llm-d<span class="w"> </span>-f<span class="w"> </span>/path/to/llm-d-modelservice.yaml
</pre></div>
</div>
<p>llm-d-modelservice Helm chart provides more configuration knobs other than the ones shown in the example above. Please refer to the <a class="reference external" href="https://github.com/furiosa-ai/helm-charts/blob/llm-d-modelservice-2026.1.0/charts/llm-d-modelservice/README.md">llm-d-modelservice Helm chart documentation</a> for details.</p>
</section>
<section id="step-6-expose-the-service-with-an-httproute">
<h3>Step 6: Expose the service with an HTTPRoute<a class="headerlink" href="#step-6-expose-the-service-with-an-httproute" title="Link to this heading">#</a></h3>
<p>Create an HTTPRoute so that inference requests are routed to the llm-d Inference Scheduler:</p>
<div class="literal-block-wrapper docutils container" id="id10">
<div class="code-block-caption"><span class="caption-text">httproute.yaml</span><a class="headerlink" href="#id10" title="Link to this code">#</a></div>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">apiVersion</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gateway.networking.k8s.io/v1</span>
<span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">HTTPRoute</span>
<span class="nt">metadata</span><span class="p">:</span>
<span class="w">  </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm-d</span>
<span class="nt">spec</span><span class="p">:</span>
<span class="w">  </span><span class="nt">parentRefs</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gateway.networking.k8s.io</span>
<span class="w">      </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">Gateway</span>
<span class="w">      </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">llm-d-infra-inference-gateway</span>
<span class="w">  </span><span class="nt">rules</span><span class="p">:</span>
<span class="w">    </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">backendRefs</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">group</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">inference.networking.k8s.io</span>
<span class="w">          </span><span class="nt">kind</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">InferencePool</span>
<span class="w">          </span><span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">gaie</span>
<span class="w">          </span><span class="nt">port</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">8000</span>
<span class="w">          </span><span class="nt">weight</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">1</span>
<span class="w">      </span><span class="nt">timeouts</span><span class="p">:</span>
<span class="w">        </span><span class="nt">backendRequest</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0s</span>
<span class="w">        </span><span class="nt">request</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">0s</span>
<span class="w">      </span><span class="nt">matches</span><span class="p">:</span>
<span class="w">        </span><span class="p p-Indicator">-</span><span class="w"> </span><span class="nt">path</span><span class="p">:</span>
<span class="w">            </span><span class="nt">type</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">PathPrefix</span>
<span class="w">            </span><span class="nt">value</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">/</span>
</pre></div>
</div>
</div>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>kubectl<span class="w"> </span>apply<span class="w"> </span>-f<span class="w"> </span>/path/to/httproute.yaml<span class="w"> </span>-n<span class="w"> </span>llm-d
</pre></div>
</div>
<p>After the route is applied, you can send inference requests to the Gateway address (e.g. via the Istio ingress). For details on how to obtain the Gateway URL and call the API, see the llm-d <a class="reference external" href="https://github.com/llm-d/llm-d/blob/v0.5.0/docs/getting-started-inferencing.md">Inference Against llm-d</a> guide.</p>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="kubernetes/npu_operator.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Installing Furiosa NPU Operator</p>
      </div>
    </a>
    <a class="right-next"
       href="../device_management/system_management_interface.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Furiosa SMI</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-d-integration-with-furiosa-llm">llm-d integration with Furiosa-LLM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">Deploying Furiosa-LLM with llm-d</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#llm-d-modelservice-and-helm-repository">llm-d-modelservice and Helm repository</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-1-set-up-gateway-api-crds">Step 1: Set up Gateway API CRDs</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-2-deploy-gie-compatible-gateway">Step 2: Deploy GIE-compatible Gateway</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-3-prepare-kubernetes-secret">Step 3: Prepare Kubernetes Secret</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-4-download-the-target-llm-model">Step 4: Download the Target LLM Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-5-deploy-llm-d">Step 5: Deploy llm-d</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-6-expose-the-service-with-an-httproute">Step 6: Expose the service with an HTTPRoute</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By FuriosaAI, Inc.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2026 FuriosaAI Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>