
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HTTHGM3MD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-0HTTHGM3MD');
    </script>
    
    <title>furiosa_llm.api &#8212; FuriosaAI Developer Center 2024.2.dev0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=a746c00c" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=a5c4661c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=c81376e9" />
  
  <!-- So that users can add custom icons -->
  <script src="../../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../../_static/documentation_options.js?v=d2cfb411"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/tabs.js?v=3030b3cb"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_modules/furiosa_llm/api';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="2024.2.dev0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/furiosa-logo.webp" class="logo__image only-light" alt="FuriosaAI Developer Center 2024.2.dev0 documentation - Home"/>
    <img src="../../_static/furiosa-logo.webp" class="logo__image only-dark pst-js-only" alt="FuriosaAI Developer Center 2024.2.dev0 documentation - Home"/>
  
  
</a></div>
        <div class="sidebar-primary-item">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../overview/rngd.html">FuriosaAI RNGD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/software_stack.html">FuriosaAI’s Software Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/supported_models.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../whatsnew/index.html">What’s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../overview/roadmap.html">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/prerequisites.html">Installing Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/furiosa_llm.html">Quick Start with Furiosa LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../getting_started/furiosa_mlperf.html">Running MLPerf™ Inference Benchmark</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Furiosa LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../furiosa_llm/intro.html">Furiosa LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../furiosa_llm/furiosa-llm-serve.html">OpenAI Compatible Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../furiosa_llm/model-preparation-workflow.html">Model Preparation Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../furiosa_llm/model-parallelism.html">Model Parallelism</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../furiosa_llm/references.html">References</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../furiosa_llm/references/llm.html">LLM class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../furiosa_llm/references/sampling_params.html">SamplingParams class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../furiosa_llm/references/artifact_builder.html">ArtifactBuilder</a></li>



<li class="toctree-l2"><a class="reference internal" href="../../furiosa_llm/references/llm_engine.html">LLMEngine class</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../furiosa_llm/references/async_llm_engine.html">AsyncLLMEngine class</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cloud Native Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../cloud_native_toolkit/intro.html">Cloud Native Toolkit</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../cloud_native_toolkit/kubernetes.html">Kubernetes Support</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../cloud_native_toolkit/kubernetes/feature_discovery.html">Installing Furiosa Feature Discovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cloud_native_toolkit/kubernetes/device_plugin.html">Installing Furiosa Device Plugin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cloud_native_toolkit/kubernetes/metrics_exporter.html">Installing Furiosa Metrics Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../cloud_native_toolkit/kubernetes/scheduling_npus.html">Scheduling NPUs</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../device_management/system_management_interface.html">Furiosa SMI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../../device_management/system_management_interface/furiosa_smi_cli.html">Furiosa SMI CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../device_management/system_management_interface/furiosa_smi_lib.html">Furiosa SMI Library</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://furiosa.ai">FuriosaAI Homepage</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.discourse.group/">FuriosaAI Forum</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">FuriosaAI Customer Portal</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/latest/en/">FuriosaAI Warboy SDK Document</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">



<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button>


<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1></h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <h1>Source code for furiosa_llm.api</h1><div class="highlight"><pre>
<span></span><span class="kn">import</span> <span class="nn">copy</span>
<span class="kn">from</span> <span class="nn">functools</span> <span class="kn">import</span> <span class="n">cached_property</span>
<span class="kn">import</span> <span class="nn">glob</span>
<span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">chain</span><span class="p">,</span> <span class="n">product</span>
<span class="kn">import</span> <span class="nn">json</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">import</span> <span class="nn">tempfile</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Any</span><span class="p">,</span>
    <span class="n">AsyncGenerator</span><span class="p">,</span>
    <span class="n">Dict</span><span class="p">,</span>
    <span class="n">List</span><span class="p">,</span>
    <span class="n">Literal</span><span class="p">,</span>
    <span class="n">Mapping</span><span class="p">,</span>
    <span class="n">Optional</span><span class="p">,</span>
    <span class="n">Sequence</span><span class="p">,</span>
    <span class="n">Tuple</span><span class="p">,</span>
    <span class="n">Type</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">,</span>
    <span class="n">cast</span><span class="p">,</span>
    <span class="n">get_args</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">uuid</span>

<span class="kn">from</span> <span class="nn">furiosa_torch_ext.torch_ext</span> <span class="kn">import</span> <span class="n">preprocess</span>
<span class="kn">import</span> <span class="nn">pydantic</span>
<span class="kn">from</span> <span class="nn">pydantic</span> <span class="kn">import</span> <span class="n">RootModel</span>
<span class="kn">from</span> <span class="nn">pydantic.dataclasses</span> <span class="kn">import</span> <span class="n">dataclass</span>
<span class="kn">from</span> <span class="nn">torch._subclasses</span> <span class="kn">import</span> <span class="n">FakeTensorMode</span>
<span class="kn">import</span> <span class="nn">torch.distributed._tensor.ops.common_rules</span>
<span class="kn">from</span> <span class="nn">torch.fx</span> <span class="kn">import</span> <span class="n">GraphModule</span>
<span class="kn">from</span> <span class="nn">torch.fx.passes.shape_prop</span> <span class="kn">import</span> <span class="n">ShapeProp</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">MODEL_FOR_CAUSAL_LM_MAPPING</span><span class="p">,</span>
    <span class="n">BatchEncoding</span><span class="p">,</span>
    <span class="n">PretrainedConfig</span><span class="p">,</span>
    <span class="n">PreTrainedModel</span><span class="p">,</span>
    <span class="n">PreTrainedTokenizer</span><span class="p">,</span>
    <span class="n">PreTrainedTokenizerFast</span><span class="p">,</span>
    <span class="n">set_seed</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">import</span> <span class="nn">yaml</span>

<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.export.graphmodule</span> <span class="kn">import</span> <span class="n">deserialize_gm</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.model_creation_info</span> <span class="kn">import</span> <span class="n">ModelCreationInfo</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.mppp.api</span> <span class="kn">import</span> <span class="n">PipelineParallelismMppp</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.mppp.config</span> <span class="kn">import</span> <span class="n">DeviceId</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.pipeline.builder.converter</span> <span class="kn">import</span> <span class="n">GraphModuleConverter</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.pipeline.types</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">CompSuperTask</span><span class="p">,</span>
    <span class="n">DataBlobId</span><span class="p">,</span>
    <span class="n">Device</span><span class="p">,</span>
    <span class="n">ParamInfo</span><span class="p">,</span>
    <span class="n">SuperTaskKind</span><span class="p">,</span>
    <span class="n">TensorInfo</span><span class="p">,</span>
    <span class="n">load_partial_param</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.parallelize.trace</span> <span class="kn">import</span> <span class="n">get_param_file_with_cache</span>
<span class="kn">from</span> <span class="nn">furiosa_llm.utils</span> <span class="kn">import</span> <span class="n">zip_equal</span>

<span class="kn">from</span> <span class="nn">.artifact.helper</span> <span class="kn">import</span> <span class="n">build_pipelines</span><span class="p">,</span> <span class="n">get_buckets</span><span class="p">,</span> <span class="n">instantiate_and_save_model</span>
<span class="kn">from</span> <span class="nn">.artifact.types</span> <span class="kn">import</span> <span class="n">Artifact</span><span class="p">,</span> <span class="n">RuntimeConfig</span>
<span class="kn">from</span> <span class="nn">.device</span> <span class="kn">import</span> <span class="n">get_device_mesh</span><span class="p">,</span> <span class="n">parse_devices_str</span>
<span class="kn">from</span> <span class="nn">.models.config_types</span> <span class="kn">import</span> <span class="p">(</span>
    <span class="n">Bucket</span><span class="p">,</span>
    <span class="n">BucketConfig</span><span class="p">,</span>
    <span class="n">DeprecatedBucket</span><span class="p">,</span>
    <span class="n">GeneratorConfig</span><span class="p">,</span>
    <span class="n">KvCacheSharingAcrossBeamsConfig</span><span class="p">,</span>
    <span class="n">LLMBackend</span><span class="p">,</span>
    <span class="n">ManualBucketConfig</span><span class="p">,</span>
    <span class="n">MinimalBucketConfig</span><span class="p">,</span>
    <span class="n">ModelRewritingConfig</span><span class="p">,</span>
    <span class="n">PagedAttentionConfig</span><span class="p">,</span>
    <span class="n">ParallelConfig</span><span class="p">,</span>
    <span class="n">SchedulerConfig</span><span class="p">,</span>
<span class="p">)</span>
<span class="kn">from</span> <span class="nn">.models.metadata</span> <span class="kn">import</span> <span class="n">LLMConfig</span><span class="p">,</span> <span class="n">ModelMetadata</span><span class="p">,</span> <span class="n">get_model_cls_from_pretrained_id</span>
<span class="kn">from</span> <span class="nn">.optimum</span> <span class="kn">import</span> <span class="n">AttentionType</span><span class="p">,</span> <span class="n">OptimizationConfig</span><span class="p">,</span> <span class="n">QDtype</span><span class="p">,</span> <span class="n">QuantizationConfig</span>
<span class="kn">from</span> <span class="nn">.optimum.types</span> <span class="kn">import</span> <span class="n">get_kv_cache_dtype_from_qformat</span>
<span class="kn">from</span> <span class="nn">.outputs</span> <span class="kn">import</span> <span class="n">CompletionOutput</span><span class="p">,</span> <span class="n">RequestOutput</span>
<span class="kn">from</span> <span class="nn">.parallelize.compiler_config</span> <span class="kn">import</span> <span class="n">CompilerConfigContext</span>
<span class="kn">from</span> <span class="nn">.parallelize.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="kn">from</span> <span class="nn">.sampling_params</span> <span class="kn">import</span> <span class="n">SamplingParams</span>
<span class="kn">from</span> <span class="nn">.tokenizer</span> <span class="kn">import</span> <span class="n">encode_auto</span><span class="p">,</span> <span class="n">get_tokenizer</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">get_logger_with_tz</span>

<span class="n">logger</span> <span class="o">=</span> <span class="n">get_logger_with_tz</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="vm">__name__</span><span class="p">))</span>

<span class="c1"># Default position id for padding</span>
<span class="n">_POSITION_ID_PAD</span> <span class="o">=</span> <span class="mi">1</span>

<span class="c1"># Default param file name</span>
<span class="n">_PARAM_FILE_NAME</span> <span class="o">=</span> <span class="s2">&quot;params.safetensors&quot;</span>
<span class="n">_HF_CAUSAL_LM_CLASS_NAMES</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
    <span class="n">model_cls</span><span class="o">.</span><span class="vm">__name__</span> <span class="k">for</span> <span class="n">model_cls</span> <span class="ow">in</span> <span class="n">MODEL_FOR_CAUSAL_LM_MAPPING</span><span class="o">.</span><span class="n">values</span><span class="p">()</span>
<span class="p">)</span>

<span class="c1"># Default index of the padding block when paged attention model is used.</span>
<span class="n">DEFAULT_PAGED_ATTENTION_PADDING_BLOCK_IDX</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">CACHE_DIR</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;XDG_CACHE_HOME&quot;</span><span class="p">,</span> <span class="n">Path</span><span class="o">.</span><span class="n">home</span><span class="p">()</span> <span class="o">/</span> <span class="s2">&quot;.cache&quot;</span><span class="p">))</span> <span class="o">/</span> <span class="s2">&quot;furiosa&quot;</span> <span class="o">/</span> <span class="s2">&quot;llm&quot;</span>

<span class="n">TokenizerModeType</span> <span class="o">=</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="s2">&quot;slow&quot;</span><span class="p">]</span>

<span class="n">RAY_LOG_PREFIX</span> <span class="o">=</span> <span class="s2">&quot;[furiosa-llm]&quot;</span>

<span class="n">STREAMING_MAX_DECODE_TRIAL</span> <span class="o">=</span> <span class="mi">2</span>


<span class="c1"># FIXME: remove this after deprecating old generator config format.</span>
<span class="nd">@dataclass</span>
<span class="k">class</span> <span class="nc">DeprecatedGeneratorConfig</span><span class="p">:</span>
    <span class="n">position_id_pad</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">prefill_buckets</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">DeprecatedBucket</span><span class="p">]</span>
    <span class="n">decode_buckets</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">DeprecatedBucket</span><span class="p">]</span>
    <span class="n">model_qname</span><span class="p">:</span> <span class="nb">str</span>  <span class="c1"># qualified name of the model (module + class)</span>
    <span class="n">paged_attention_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PagedAttentionConfig</span><span class="p">]</span>
    <span class="n">packing_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;IDENTITY&quot;</span><span class="p">]</span>
    <span class="n">kv_cache_sharing_across_beams_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">KvCacheSharingAcrossBeamsConfig</span><span class="p">]</span>
    <span class="n">scheduler_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SchedulerConfig</span><span class="p">]</span>

    <span class="k">def</span> <span class="nf">export</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">RootModel</span><span class="p">[</span><span class="n">DeprecatedGeneratorConfig</span><span class="p">](</span><span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">model_dump_json</span><span class="p">(</span><span class="n">indent</span><span class="o">=</span><span class="mi">4</span><span class="p">))</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="s2">&quot;DeprecatedGeneratorConfig&quot;</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">o</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">DeprecatedGeneratorConfig</span><span class="p">(</span><span class="o">**</span><span class="n">o</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_available_devices</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Device</span><span class="p">]:</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="kn">from</span> <span class="nn">furiosa_smi_py</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[import-not-found, import-untyped]</span>
            <span class="n">CoreStatus</span><span class="p">,</span>
            <span class="n">list_devices</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">except</span> <span class="ne">ModuleNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ModuleNotFoundError</span><span class="p">(</span><span class="s2">&quot;Install furiosa_smi_py to get available devices automatically.&quot;</span><span class="p">)</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ImportError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to import furiosa_smi_py with error </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">try</span><span class="p">:</span>
        <span class="n">devices</span> <span class="o">=</span> <span class="n">list_devices</span><span class="p">()</span>
        <span class="n">available_devs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">:</span>
            <span class="c1"># e.g. /dev/rngd/npu1</span>
            <span class="n">name</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">device_info</span><span class="p">()</span><span class="o">.</span><span class="n">name</span><span class="p">()</span>
            <span class="n">npu_idx</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">name</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            <span class="n">core_status</span> <span class="o">=</span> <span class="n">device</span><span class="o">.</span><span class="n">core_status</span><span class="p">()</span>

            <span class="k">for</span> <span class="n">core_id</span><span class="p">,</span> <span class="n">status</span> <span class="ow">in</span> <span class="n">core_status</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># Cannot be compared by reference (using &quot;is&quot;) because `CoreStatus` is a Rust enum exposed with PyO3.</span>
                <span class="k">if</span> <span class="n">status</span> <span class="o">==</span> <span class="n">CoreStatus</span><span class="o">.</span><span class="n">Available</span><span class="p">:</span>
                    <span class="n">available_devs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">Device</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;npu:</span><span class="si">{</span><span class="n">npu_idx</span><span class="si">}</span><span class="s2">:</span><span class="si">{</span><span class="n">core_id</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">available_devs</span>
    <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Failed to get available devices with errror </span><span class="si">{</span><span class="n">e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_get_bucket_from_pipeline_name</span><span class="p">(</span><span class="n">pipeline_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Bucket</span><span class="p">:</span>
    <span class="c1"># Returns: tuple of (is_prefill, bucket)</span>
    <span class="c1"># Possible pipeline name formats:</span>
    <span class="c1"># * f&quot;{model_name}-{mode}-b{bucket.batch_size}-attn{bucket.attention_size} (will be deprecated)</span>
    <span class="c1"># * f&quot;{model_name}-kv{bucket.kv_cache_size}-b{bucket.batch_size}-attn{bucket.attention_size}</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">mode_or_kv_cache_size</span><span class="p">,</span> <span class="n">b_batch_size</span><span class="p">,</span> <span class="n">attn_attn_size</span> <span class="o">=</span> <span class="n">pipeline_name</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="p">)</span>

    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">b_batch_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
    <span class="n">attn_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attn_attn_size</span><span class="p">[</span><span class="mi">4</span><span class="p">:])</span>

    <span class="k">if</span> <span class="n">mode_or_kv_cache_size</span> <span class="o">==</span> <span class="s2">&quot;prefill&quot;</span><span class="p">:</span>
        <span class="n">kv_size</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">elif</span> <span class="n">mode_or_kv_cache_size</span> <span class="o">==</span> <span class="s2">&quot;decode&quot;</span><span class="p">:</span>
        <span class="n">kv_size</span> <span class="o">=</span> <span class="n">attn_size</span> <span class="o">-</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">mode_or_kv_cache_size</span><span class="o">.</span><span class="n">startswith</span><span class="p">(</span><span class="s2">&quot;kv&quot;</span><span class="p">)</span>
        <span class="n">kv_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">mode_or_kv_cache_size</span><span class="p">[</span><span class="mi">2</span><span class="p">:])</span>

    <span class="k">return</span> <span class="n">Bucket</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn_size</span><span class="p">,</span> <span class="n">kv_size</span><span class="p">)</span>


<div class="viewcode-block" id="LLM">
<a class="viewcode-back" href="../../furiosa_llm/references/llm.html#furiosa_llm.LLM">[docs]</a>
<span class="k">class</span> <span class="nc">LLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;An LLM for generating texts from given prompts and sampling parameters.</span>

<span class="sd">    Args:</span>
<span class="sd">        pretrained_id: The name of the pretrained model. This corresponds to</span>
<span class="sd">            pretrained_model_name_or_path in HuggingFace Transformers.</span>
<span class="sd">        task_type: The type of the task. This corresponds to task in HuggingFace Transformers.</span>
<span class="sd">            See https://huggingface.co/docs/transformers/main/en/quicktour#pipeline for more</span>
<span class="sd">            details.</span>
<span class="sd">        llm_config: The configuration for the LLM. This includes quantization and optimization</span>
<span class="sd">            configurations.</span>
<span class="sd">        qformat_path: The path to the quantization format file.</span>
<span class="sd">        qparam_path: The path to the quantization parameter file.</span>
<span class="sd">        quant_ckpt_file_path: The path to the quantized parameters checkpoint file.</span>
<span class="sd">        config: Additional HuggingFace Transformers model configuration. This is a dictionary</span>
<span class="sd">            that includes the configuration for the model.</span>
<span class="sd">        bucket_config: Config for bucket generating policy. If not given, the model will use single one batch, `max_seq_len_to_capture` attention size bucket per</span>
<span class="sd">            each phase.</span>
<span class="sd">        speculative_model: Specualtive model for speculative decoding.</span>
<span class="sd">        speculative_model_llm_config: The configuration for the specualtive model. This includes quantization and optimization</span>
<span class="sd">            configurations.</span>
<span class="sd">        speculative_model_qformat_path: The path to the quantization format file for the specualtive model.</span>
<span class="sd">        speculative_model_qparam_path: The path to the quantization parameter file for the specualtive model.</span>
<span class="sd">        speculative_model_quant_ckpt_file_path: The path to the quantized parameters checkpoint file for the specualtive model.</span>
<span class="sd">        speculative_model_config: Additional HuggingFace Transformers model configuration for the specualtive model. This is a dictionary</span>
<span class="sd">            that includes the configuration for the model.</span>
<span class="sd">        speculative_model_bucket_config: Config for bucket generating policy. If not given, the model will use single one batch, `max_seq_len_to_capture` attention size bucket per</span>
<span class="sd">            each phase.</span>
<span class="sd">        speculative_model_paged_attention_num_blocks: The maximum number of blocks that each k/v storage per layer can store for the specualtive model. This argument must be given</span>
<span class="sd">            if the specualtive model uses paged attention.</span>
<span class="sd">        num_speculative_tokens: The number of tokens that specualtive model will generate speculatively during each iteration of the decoding process</span>
<span class="sd">        max_seq_len_to_capture: Maximum sequence length covered by LLM engine. Sequence with larger context than this will not be covered.</span>
<span class="sd">            The default is 2048.</span>
<span class="sd">        tensor_parallel_size: The number of PEs for each tensor parallelism group. The default is 4.</span>
<span class="sd">        pipeline_parallel_size: The number of pipeline stages for pipeline parallelism. The default is 1,</span>
<span class="sd">            which means no pipeline parallelism.</span>
<span class="sd">        data_parallel_size: The size of the data parallelism group. If not given, it will be inferred from</span>
<span class="sd">            total avaialble PEs and other parallelism degrees.</span>
<span class="sd">        tokenizer: The name or path of a HuggingFace Transformers tokenizer.</span>
<span class="sd">        tokenizer_mode: The tokenizer mode. &quot;auto&quot; will use the fast tokenizer</span>
<span class="sd">            if available, and &quot;slow&quot; will always use the slow tokenizer.</span>
<span class="sd">        seed: The seed to initialize the random number generator for sampling.</span>
<span class="sd">        devices: The devices to run the model. It can be a single device or a list of devices.</span>
<span class="sd">            Each device can be either &quot;npu:X&quot; or &quot;npu:X:*&quot; where X is a specific device index.</span>
<span class="sd">            If not given, available devices will be used.</span>
<span class="sd">        param_file_path: The path to the parameter file to use for pipeline generation.</span>
<span class="sd">            If not specified, the parameters will be saved in a temporary file which will be</span>
<span class="sd">            deleted when ``LLM`` is destroyed.</span>
<span class="sd">        param_saved_format: The format of the parameter file. Only possible value is &quot;safetensors&quot; now.</span>
<span class="sd">            The default is &quot;safetensors&quot;.</span>
<span class="sd">        do_decompositions_for_model_rewrite: Whether to decompose some ops to describe various parallelism strategies</span>
<span class="sd">            with mppp config. When the value is True, mppp config that matches with the decomposed FX graph should be given.</span>
<span class="sd">        comp_supertask_kind: The format that pipeline&#39;s supertasks will be represented as.</span>
<span class="sd">            Possible values are &quot;fx&quot;,&quot;dfg&quot;, and &quot;edf&quot;, and the default is &quot;edf&quot;.</span>
<span class="sd">        cache_dir: The cache directory for all generated files for this LLM instance.</span>
<span class="sd">            When its value is ``None``, caching is disabled. The default is &quot;$HOME/.cache/furiosa/llm&quot;.</span>
<span class="sd">        backend: The backend implementation to run forward() of a model for the LLM.</span>
<span class="sd">            If not specified, the backend will be chosen based on the device kind.</span>
<span class="sd">        use_blockwise_compile: If True, each task will be compiled in the unit of transformer block,</span>
<span class="sd">            and compilation result for transformer block is generated once and reused. The default is ``True``.</span>
<span class="sd">        num_blocks_per_supertask: The number of transformer blocks that will be merged into one supertask. This option is valid</span>
<span class="sd">            only when `use_blockwise_compile=True`. The default is 1.</span>
<span class="sd">        num_blocks_per_pp_stage: The number of transformers blocks per each pipeline parallelism stage. If not given, transformer blocks will be</span>
<span class="sd">            distributed equally.</span>
<span class="sd">        embed_all_constants_into_graph: Whether to embed constant tensors into graph or make them as input of the graph and save them as separate files.</span>
<span class="sd">            The default is False.</span>
<span class="sd">        paged_attention_num_blocks: The maximum number of blocks that each k/v storage per layer can store. This argument must be given</span>
<span class="sd">            if model uses paged attention.</span>
<span class="sd">        paged_attention_block_size: The maximum number of tokens that can be stored in a single paged attention block. This argument must be given</span>
<span class="sd">            if model uses paged attention.</span>
<span class="sd">        kv_cache_sharing_across_beams_config: Configuration for sharing kv cache across beams. This argument must be given if and only if</span>
<span class="sd">            the model is optimized to share kv cache across beams. If this argument is given, decode phase buckets with batch size of</span>
<span class="sd">            ``batch_size`` * ``kv_cache_sharing_across_beams_config.beam_width`` will be created.</span>
<span class="sd">        scheduler_config: Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples</span>
<span class="sd">            that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler.</span>
<span class="sd">        packing_type: Packing algorithm. Possible values are &quot;IDENTITY&quot; only for now</span>
<span class="sd">        compiler_config_overrides: Overrides for the compiler config. This is a dictionary that includes the configuration for the compiler.</span>
<span class="sd">        use_random_weight: If True, the model will be initialized with random weights.</span>
<span class="sd">        num_pipeline_builder_workers: number of workers used for building pipelines (except for compilation). The default is 1 (no parallelism).</span>
<span class="sd">            Setting this value larger than 1 reduces pipeline building time, especially for large models, but requires much more memory.</span>
<span class="sd">        num_compile_workers: number of workers used for compilation. The default is 1 (no parallelism).</span>
<span class="sd">        skip_engine: If True, the native runtime engine will not be initialized. This is useful when you need</span>
<span class="sd">            the pipelines for other purposes than running them with the engine.</span>
<span class="sd">        artifacts_export_path: The path to export the artifacts. With artifacts, you can create ``LLM`` without quantizing or compiling the model again.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">max_seq_len_to_capture</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">task_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">llm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LLMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">qformat_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># FIXME: move to quantization_config</span>
        <span class="n">qparam_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># FIXME: move to quantization_config</span>
        <span class="n">quant_ckpt_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">hf_overrides</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>  <span class="c1"># aka hf_config</span>
        <span class="n">bucket_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BucketConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="s2">&quot;LLM&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_llm_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LLMConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_qformat_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_qparam_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_quant_ckpt_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{},</span>
        <span class="n">speculative_model_bucket_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BucketConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_paged_attention_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">max_seq_len_to_capture</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">2048</span><span class="p">,</span>
        <span class="n">tensor_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
        <span class="n">pipeline_parallel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">data_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_mode</span><span class="p">:</span> <span class="n">TokenizerModeType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># TODO: change devices default value to None and get devices from furiosa-smi.</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">param_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">param_saved_format</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;safetensors&quot;</span><span class="p">,</span> <span class="s2">&quot;pt&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;safetensors&quot;</span><span class="p">,</span>
        <span class="n">do_decompositions_for_model_rewrite</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># FIXME: move to compiler_config</span>
        <span class="n">comp_supertask_kind</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;edf&quot;</span><span class="p">,</span> <span class="s2">&quot;dfg&quot;</span><span class="p">,</span> <span class="s2">&quot;fx&quot;</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="n">CACHE_DIR</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LLMBackend</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_blockwise_compile</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>  <span class="c1"># FIXME: move to compiler_config</span>
        <span class="n">num_blocks_per_supertask</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># FIXME: move to compiler_config</span>
        <span class="n">num_blocks_per_pp_stage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">embed_all_constants_into_graph</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">paged_attention_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># FIXME: move to compiler_config</span>
        <span class="n">paged_attention_block_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>  <span class="c1"># FIXME: move to compiler_config</span>
        <span class="n">kv_cache_sharing_across_beams_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span>
            <span class="n">KvCacheSharingAcrossBeamsConfig</span>
        <span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>  <span class="c1"># FIXME: move to compiler_config / leave this in LLM attr ??</span>
        <span class="n">scheduler_config</span><span class="p">:</span> <span class="n">SchedulerConfig</span> <span class="o">=</span> <span class="n">SchedulerConfig</span><span class="p">(),</span>
        <span class="n">packing_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;IDENTITY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;IDENTITY&quot;</span><span class="p">,</span>
        <span class="n">compiler_config_overrides</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Mapping</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_random_weight</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">num_pipeline_builder_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_compile_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">skip_engine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">artifacts_export_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">_cleanup</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">_pipelines</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Pipeline</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">_custom_buckets</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Bucket</span><span class="p">]</span> <span class="o">=</span> <span class="p">[],</span>
        <span class="n">_add_prefill_last_block_slice</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="n">optimize_paged_attention_block_loading</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span>
            <span class="s2">&quot;optimize_paged_attention_block_loading&quot;</span><span class="p">,</span> <span class="kc">True</span>
        <span class="p">)</span>
        <span class="n">sparse_select_version</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;sparse_select_version&quot;</span><span class="p">,</span> <span class="s2">&quot;v1.5&quot;</span><span class="p">)</span>
        <span class="n">one_supertask_per_device</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;one_supertask_per_device&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">config</span> <span class="o">:=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;config&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;`config` is deprecated. use `hf_overrides` instead.&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">hf_overrides</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`config` and `hf_overrides` are given at the same time.&quot;</span><span class="p">)</span>
            <span class="n">hf_overrides</span> <span class="o">=</span> <span class="n">config</span>

        <span class="c1"># Set seed in order to guarantee the reproducibility with the same seed number</span>
        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">LLM</span><span class="o">.</span><span class="n">__verify_tokenizer_mode</span><span class="p">(</span><span class="n">tokenizer_mode</span><span class="p">)</span>

        <span class="c1"># Set logging options for ray.</span>
        <span class="k">if</span> <span class="s2">&quot;RAY_COLOR_PREFIX&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RAY_COLOR_PREFIX&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;1&quot;</span>
        <span class="k">if</span> <span class="s2">&quot;RAY_DEDUP_LOGS_ALLOW_REGEX&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">:</span>
            <span class="c1"># For not to dedup our info logs.</span>
            <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;RAY_DEDUP_LOGS_ALLOW_REGEX&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;INFO:*</span><span class="si">{</span><span class="n">RAY_LOG_PREFIX</span><span class="si">}</span><span class="s2">*&quot;</span>

        <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">_get_available_devices</span><span class="p">()</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Device is not given, using available device: </span><span class="si">{</span><span class="n">devices</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">devices</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>

        <span class="c1"># Normalize the devices</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">devices</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">parse_devices_str</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>
        <span class="n">LLM</span><span class="o">.</span><span class="n">__verify_devices</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">num_pipeline_builder_workers</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`num_pipeline_builder_workers` must be larger than 0&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">llm_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">opt_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_default_opt_config_from_pretrained_id</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
            <span class="n">quant_config</span> <span class="o">=</span> <span class="n">QuantizationConfig</span><span class="o">.</span><span class="n">from_qformat</span><span class="p">(</span><span class="n">qformat_path</span><span class="p">)</span> <span class="k">if</span> <span class="n">qformat_path</span> <span class="k">else</span> <span class="kc">None</span>
            <span class="n">llm_config</span> <span class="o">=</span> <span class="n">LLMConfig</span><span class="p">(</span><span class="n">opt_config</span><span class="p">,</span> <span class="n">quant_config</span><span class="p">)</span>

        <span class="c1"># To use speculative decoding, special model optimized for speculative decoding is needed.</span>
        <span class="k">if</span> <span class="n">num_speculative_tokens</span><span class="p">:</span>
            <span class="n">llm_config</span> <span class="o">=</span> <span class="n">LLMConfig</span><span class="p">(</span>
                <span class="n">llm_config</span><span class="o">.</span><span class="n">optimization_config</span><span class="o">.</span><span class="n">with_optimizations</span><span class="p">(</span>
                    <span class="p">{</span><span class="s2">&quot;optimized_for_speculative_decoding&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">}</span>
                <span class="p">),</span>
                <span class="n">llm_config</span><span class="o">.</span><span class="n">quantization_config</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span> <span class="o">=</span> <span class="n">ModelMetadata</span><span class="p">(</span>
            <span class="n">pretrained_id</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
            <span class="n">task_type</span><span class="o">=</span><span class="n">task_type</span><span class="p">,</span>
            <span class="n">llm_config</span><span class="o">=</span><span class="n">llm_config</span><span class="p">,</span>
            <span class="n">hf_configs</span><span class="o">=</span><span class="n">hf_overrides</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">config</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_generative_model</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">is_generative_model</span>
        <span class="n">kv_cache_dtype</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">kv_cache_dtype</span>

        <span class="k">if</span> <span class="n">max_seq_len_to_capture</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_max_seq_len</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`max_seq_len_to_capture` is larger than the model&#39;s max number of positions.&quot;</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="n">bucket_config</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># TODO: alaways set max_seq_len to model&#39;s max_position_embeddings once compiler supports it.</span>
            <span class="n">bucket_config</span> <span class="o">=</span> <span class="n">MinimalBucketConfig</span><span class="p">(</span><span class="n">max_seq_len</span><span class="o">=</span><span class="n">max_seq_len_to_capture</span><span class="p">)</span>

        <span class="n">buckets_for_prefill</span><span class="p">,</span> <span class="n">buckets_for_decode</span><span class="p">,</span> <span class="n">other_buckets</span> <span class="o">=</span> <span class="n">get_buckets</span><span class="p">(</span>
            <span class="n">bucket_config</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_generative_model</span><span class="p">,</span>
            <span class="n">max_seq_len_to_capture</span><span class="p">,</span>
            <span class="n">num_speculative_tokens</span><span class="p">,</span>
            <span class="n">prefill_chunk_size</span><span class="o">=</span><span class="n">scheduler_config</span><span class="o">.</span><span class="n">prefill_chunk_size</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># NOTE: Allow no prefill or decode bucket case with skip_engine=True for artifacts building and internal tests.</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_engine</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">buckets_for_prefill</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Prefill buckets must be given.&quot;</span><span class="p">)</span>

        <span class="c1"># Find the max attention_size of prefill/decode_buckets from artifacts and set them as fields.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">prompt_max_seq_len</span> <span class="o">=</span> <span class="p">(</span>
            <span class="nb">max</span><span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">buckets_for_prefill</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">buckets_for_prefill</span>
            <span class="k">else</span> <span class="mi">0</span>
        <span class="p">)</span>
        <span class="k">if</span> <span class="n">buckets_for_decode</span><span class="p">:</span>
            <span class="n">bucket_max_seq_len_to_capture</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">buckets_for_decode</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">bucket_max_seq_len_to_capture</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                <span class="p">(</span>
                    <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span>
                    <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">chain</span><span class="p">(</span><span class="n">buckets_for_prefill</span><span class="p">,</span> <span class="n">_custom_buckets</span><span class="p">,</span> <span class="n">other_buckets</span><span class="p">)</span>
                <span class="p">),</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">bucket_max_seq_len_to_capture</span> <span class="o">&lt;</span> <span class="n">max_seq_len_to_capture</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;There&#39;s no bucket to handle `max_seq_len_to_capture` length of sequence. Add bucket of size `max_seq_len_to_capture` or decrease `max_seq_len_to_capture`.&quot;</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">bucket_max_seq_len_to_capture</span> <span class="o">&gt;</span> <span class="n">max_seq_len_to_capture</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;Buckets with larger sequence length than `max_seq_len_to_capture` will be created. This will cause unnecessary overhead.&quot;</span>
            <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="n">bucket_max_seq_len_to_capture</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Prefill buckets: </span><span class="si">{</span><span class="n">buckets_for_prefill</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Decode buckets: </span><span class="si">{</span><span class="n">buckets_for_decode</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">LLM</span><span class="o">.</span><span class="n">__verify_buckets</span><span class="p">(</span>
            <span class="n">buckets_for_prefill</span><span class="p">,</span> <span class="n">buckets_for_decode</span><span class="p">,</span> <span class="n">kv_cache_sharing_across_beams_config</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">optimize_options</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams</span>
            <span class="ow">and</span> <span class="n">kv_cache_sharing_across_beams_config</span> <span class="ow">is</span> <span class="kc">None</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`kv_cache_sharing_across_beams_config` must be given if the model is optimized to share kv cache across beams.&quot;</span>
            <span class="p">)</span>

        <span class="n">padding_block_idx</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">DEFAULT_PAGED_ATTENTION_PADDING_BLOCK_IDX</span>
            <span class="k">if</span> <span class="n">optimize_paged_attention_block_loading</span>
            <span class="k">else</span> <span class="kc">None</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">attention_type</span> <span class="ow">is</span> <span class="n">AttentionType</span><span class="o">.</span><span class="n">PAGED_ATTENTION</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">paged_attention_num_blocks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="c1"># TODO: if `paged_attention_num_blocks` is not given, always calculate maximum possible num blocks and use that.</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;`paged_attention_num_blocks` must be given for paged attention models now.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">paged_attention_block_size</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;Currently, only paged attention with block_size=1 is supported.&quot;</span>
                <span class="p">)</span>
            <span class="k">assert</span> <span class="n">paged_attention_num_blocks</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">paged_attention_config</span> <span class="o">=</span> <span class="n">PagedAttentionConfig</span><span class="p">(</span>
                <span class="n">paged_attention_num_blocks</span><span class="p">,</span> <span class="n">paged_attention_block_size</span><span class="p">,</span> <span class="n">padding_block_idx</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">paged_attention_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">original_model_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">get_optimized_cls</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">_add_prefill_last_block_slice</span><span class="p">:</span>
            <span class="c1"># TODO: do we need to update `self.model_metadata` as well? But there might be no optimized class</span>
            <span class="c1"># for original model with slice optimization. Just save this information in model rewriting config now.</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">optimize_options</span><span class="o">.</span><span class="n">calculate_logit_only_for_last_token</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;You are using model with last block slice optimization. `_add_prefill_last_block_slice` option is ignored.&quot;</span>
                <span class="p">)</span>
            <span class="c1"># This is to notify generator that this model&#39;s last block has been sliced.</span>
            <span class="c1"># FIXME: pass this information with concrete metadata other than using model type.</span>
            <span class="n">original_model_type</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">with_optimizations</span><span class="p">(</span>
                <span class="p">{</span>
                    <span class="s2">&quot;calculate_logit_only_for_last_token&quot;</span><span class="p">:</span> <span class="kc">True</span><span class="p">,</span>
                <span class="p">}</span>
            <span class="p">)</span><span class="o">.</span><span class="n">get_optimized_cls</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span> <span class="o">=</span> <span class="n">GeneratorConfig</span><span class="p">(</span>
            <span class="n">_POSITION_ID_PAD</span><span class="p">,</span>
            <span class="nb">list</span><span class="p">(</span>
                <span class="nb">set</span><span class="p">(</span>  <span class="c1"># To remove duplicated buckets if any.</span>
                    <span class="p">(</span><span class="o">*</span><span class="n">buckets_for_prefill</span><span class="p">,</span> <span class="o">*</span><span class="n">buckets_for_decode</span><span class="p">,</span> <span class="o">*</span><span class="n">other_buckets</span><span class="p">,</span> <span class="o">*</span><span class="n">_custom_buckets</span><span class="p">)</span>
                <span class="p">)</span>
            <span class="p">),</span>
            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">original_model_type</span><span class="o">.</span><span class="vm">__module__</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">original_model_type</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">paged_attention_config</span><span class="p">,</span>
            <span class="n">packing_type</span><span class="p">,</span>
            <span class="n">kv_cache_sharing_across_beams_config</span><span class="p">,</span>
            <span class="n">num_speculative_tokens</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">prefill_buckets</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">DeprecatedBucket</span><span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span>
            <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_prefill</span>
        <span class="p">]</span>
        <span class="n">decode_buckets</span> <span class="o">=</span> <span class="p">[</span>
            <span class="n">DeprecatedBucket</span><span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span>
            <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_decode</span> <span class="ow">and</span> <span class="n">bucket</span><span class="o">.</span><span class="n">input_ids_size</span> <span class="o">==</span> <span class="mi">1</span>
        <span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_config</span> <span class="o">=</span> <span class="n">scheduler_config</span>

        <span class="c1"># FIXME: remove this after deprecating old generator config format.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">generator_config_in_deprecated_format</span> <span class="o">=</span> <span class="n">DeprecatedGeneratorConfig</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">position_id_pad</span><span class="p">,</span>
            <span class="n">prefill_buckets</span><span class="p">,</span>
            <span class="n">decode_buckets</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">model_qname</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">packing_type</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_config</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="c1"># FIXME: this is a temporary workaround to test decode buckets with more than one input_ids.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">custom_buckets</span> <span class="o">=</span> <span class="n">_custom_buckets</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">model_rewriting_config</span> <span class="o">=</span> <span class="n">ModelRewritingConfig</span><span class="p">(</span>
            <span class="n">do_decompositions_for_model_rewrite</span><span class="o">=</span><span class="n">do_decompositions_for_model_rewrite</span><span class="p">,</span>
            <span class="n">use_blockwise_compile</span><span class="o">=</span><span class="n">use_blockwise_compile</span><span class="p">,</span>
            <span class="n">num_blocks_per_supertask</span><span class="o">=</span><span class="n">num_blocks_per_supertask</span><span class="p">,</span>
            <span class="n">embed_all_constants_into_graph</span><span class="o">=</span><span class="n">embed_all_constants_into_graph</span><span class="p">,</span>
            <span class="n">add_prefill_last_block_slice</span><span class="o">=</span><span class="n">_add_prefill_last_block_slice</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">device_sets_for_actual_use</span> <span class="o">:=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;device_sets_for_actual_use&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;`device_sets_for_actual_use` is deprecated. Use `{tensor|pipeline|data}_parallel` options instead.&quot;</span>
            <span class="p">)</span>
            <span class="n">normalized_dev_mesh</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">parse_devices_str</span><span class="p">(</span><span class="n">device_set</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device_set</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span> <span class="k">else</span> <span class="n">device_set</span>
                <span class="k">for</span> <span class="n">device_set</span> <span class="ow">in</span> <span class="n">device_sets_for_actual_use</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dev_mesh</span> <span class="o">=</span> <span class="n">get_device_mesh</span><span class="p">(</span>
                <span class="n">devices</span><span class="p">,</span> <span class="n">tensor_parallel_size</span><span class="p">,</span> <span class="n">pipeline_parallel_size</span><span class="p">,</span> <span class="n">data_parallel_size</span>
            <span class="p">)</span>
            <span class="c1"># Flatten pp_tp_groups to build pipeline. This is 2d-matrix whose elements are dp subgroups.</span>
            <span class="n">normalized_dev_mesh</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">[</span><span class="n">dev</span> <span class="k">for</span> <span class="n">tp_group</span> <span class="ow">in</span> <span class="n">pp_tp_group</span> <span class="k">for</span> <span class="n">dev</span> <span class="ow">in</span> <span class="n">tp_group</span><span class="p">]</span> <span class="k">for</span> <span class="n">pp_tp_group</span> <span class="ow">in</span> <span class="n">dev_mesh</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="p">(</span>
            <span class="n">num_blocks_per_pp_stage</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">num_blocks_per_pp_stage</span><span class="p">)</span> <span class="o">!=</span> <span class="n">pipeline_parallel_size</span>
        <span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;`num_blocks_per_pp_stage` should have length of `pipeline_parallel_size`&quot;</span>
            <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span> <span class="o">=</span> <span class="n">ParallelConfig</span><span class="p">(</span>
            <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tensor_parallel_size</span><span class="p">,</span> <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="n">pipeline_parallel_size</span>
        <span class="p">)</span>

        <span class="n">data_parallel_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">normalized_dev_mesh</span><span class="p">)</span>

        <span class="c1"># Build pipelines for first pp_tp_group and replicate them for other pp_tp_groups later.</span>
        <span class="n">first_dp_subgroup_devices</span> <span class="o">=</span> <span class="n">normalized_dev_mesh</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">backend</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">dev_kind</span> <span class="o">=</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">kind</span>
            <span class="k">if</span> <span class="n">dev_kind</span> <span class="o">==</span> <span class="s2">&quot;npu&quot;</span><span class="p">:</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="n">LLMBackend</span><span class="o">.</span><span class="n">FURIOSA_RT_V2</span>
            <span class="k">elif</span> <span class="n">dev_kind</span> <span class="o">==</span> <span class="s2">&quot;cpu&quot;</span><span class="p">:</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="n">LLMBackend</span><span class="o">.</span><span class="n">MOCK_BACKEND_V2</span>
            <span class="k">elif</span> <span class="n">dev_kind</span> <span class="o">==</span> <span class="s2">&quot;cuda&quot;</span><span class="p">:</span>
                <span class="n">backend</span> <span class="o">=</span> <span class="n">LLMBackend</span><span class="o">.</span><span class="n">FURIOSA_RT_CUDA</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Invalid device kind: </span><span class="si">{</span><span class="n">dev_kind</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">comp_supertask_kind</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">backend</span> <span class="ow">in</span> <span class="p">(</span><span class="n">LLMBackend</span><span class="o">.</span><span class="n">FURIOSA_RT_NPU</span><span class="p">,</span> <span class="n">LLMBackend</span><span class="o">.</span><span class="n">FURIOSA_RT_V2</span><span class="p">):</span>
                <span class="n">comp_supertask_kind</span> <span class="o">=</span> <span class="s2">&quot;edf&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">comp_supertask_kind</span> <span class="o">=</span> <span class="s2">&quot;fx&quot;</span>
        <span class="k">if</span> <span class="n">comp_supertask_kind</span> <span class="o">==</span> <span class="s2">&quot;dfg&quot;</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Using dfg as comp_supertask_kind&quot;</span><span class="p">)</span>
        <span class="n">LLM</span><span class="o">.</span><span class="n">__verify_comp_supertask_kind</span><span class="p">(</span><span class="n">comp_supertask_kind</span><span class="p">)</span>

        <span class="n">beam_size_or_none</span> <span class="o">=</span> <span class="p">(</span>
            <span class="kc">None</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span> <span class="ow">is</span> <span class="kc">None</span>
            <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="o">.</span><span class="n">beam_width</span>
        <span class="p">)</span>

        <span class="c1"># Get Tokenizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">tokenizer_pretrained_id</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">tokenizer_mode</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>

        <span class="c1"># Please refer to an example at https://huggingface.co/docs/transformers/en/main_classes/text_generation#transformers.GenerationMixin.greedy_search.example</span>
        <span class="c1"># Some models like GPT-2 may not have pad_token_id. BTW, when we run a batch of sequence generations,</span>
        <span class="c1"># We must need pad_token_id to fill the batch with pad. With Hugging Face Transformers,</span>
        <span class="c1"># users should handle this issue. Our goal is to provide a better useability for users.</span>
        <span class="c1"># We handle this issue within LLM class.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">pad_token_id</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">eos_token_id</span>

        <span class="k">if</span> <span class="n">speculative_model</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># mimic vllm&#39;s behavior</span>
            <span class="n">num_speculative_tokens</span> <span class="o">=</span> <span class="n">num_speculative_tokens</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span> <span class="s2">&quot;n_predict&quot;</span>
            <span class="p">)</span>

        <span class="n">compiler_config_context</span> <span class="o">=</span> <span class="n">CompilerConfigContext</span><span class="p">(</span>
            <span class="n">model_metadata</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="p">,</span>
            <span class="n">beam_size</span><span class="o">=</span><span class="n">beam_size_or_none</span><span class="p">,</span>
            <span class="n">compiler_config_overrides</span><span class="o">=</span><span class="n">compiler_config_overrides</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">_pipelines</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="c1"># FIXME: This pass exists only for supporting `LLM.from_artifacts` API.</span>

            <span class="c1"># Only pick pipelines for given buckets.</span>
            <span class="n">buckets_to_include</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span><span class="p">)</span>

            <span class="n">pipelines_with_bucket_info</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">_get_bucket_from_pipeline_name</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">name</span><span class="p">),</span> <span class="n">pipeline</span><span class="p">)</span> <span class="k">for</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="n">_pipelines</span>
            <span class="p">]</span>

            <span class="n">pipelines</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Pipeline</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">pipeline</span>
                <span class="k">for</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="n">pipelines_with_bucket_info</span>
                <span class="k">if</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">buckets_to_include</span>
            <span class="p">]</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pipelines</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">buckets_to_include</span><span class="p">):</span>
                <span class="n">needed_buckets</span> <span class="o">=</span> <span class="n">buckets_to_include</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span>
                    <span class="n">bucket</span> <span class="k">for</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pipelines_with_bucket_info</span>
                <span class="p">)</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;Some needed buckets do not exist in the artifacts.</span><span class="se">\n</span><span class="si">{</span><span class="n">needed_buckets</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>

            <span class="c1"># replace devices in pipelines</span>
            <span class="k">for</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="n">pipelines</span><span class="p">:</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">devices</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">first_dp_subgroup_devices</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;The number of devices in the pipeline is different from the number of devices in the first dp subgroup.&quot;</span>
                    <span class="p">)</span>

                <span class="n">pipeline</span><span class="o">.</span><span class="n">devices</span> <span class="o">=</span> <span class="p">{</span>
                    <span class="n">DeviceId</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">i</span><span class="p">)):</span> <span class="n">dev</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">dev</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">first_dp_subgroup_devices</span><span class="p">)</span>
                <span class="p">}</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span> <span class="o">=</span> <span class="n">pipelines</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">need_quant_artifacts</span> <span class="ow">or</span> <span class="n">qparam_path</span> <span class="ow">or</span> <span class="n">qformat_path</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">qparam_path</span> <span class="ow">and</span> <span class="n">qformat_path</span><span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;To use quantized model, `qparam_path` and `qformat_path` should be given.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">qparam_path</span><span class="p">)</span> <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">qformat_path</span><span class="p">)):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;`qparam_path` or `qformat_path` is invalid. The file does not exist.&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_generative_model</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">kv_cache_dtype</span>
                    <span class="c1"># Check model&#39;s kv cache dtype complies with description of qformat file.</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">__verify_kv_cache_dtype_with_qformat</span><span class="p">(</span>
                        <span class="n">kv_cache_dtype</span><span class="p">,</span> <span class="n">qformat_path</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span>
                    <span class="p">)</span>

            <span class="n">model_</span> <span class="o">=</span> <span class="n">ModelCreationInfo</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="p">,</span>
                <span class="n">use_random_weight</span><span class="p">,</span>
                <span class="n">seed</span><span class="p">,</span>
                <span class="n">qformat_path</span><span class="o">=</span><span class="n">qformat_path</span><span class="p">,</span>
                <span class="n">qparam_path</span><span class="o">=</span><span class="n">qparam_path</span><span class="p">,</span>
                <span class="n">quant_ckpt_file_path</span><span class="o">=</span><span class="n">quant_ckpt_file_path</span><span class="p">,</span>
            <span class="p">)</span>

            <span class="k">if</span> <span class="n">artifacts_export_path</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;FURIOSA_COMPILE_DUMP_PATH&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">artifacts_export_path</span><span class="p">)</span>

            <span class="k">try</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build_all_pipelines</span><span class="p">(</span>
                    <span class="n">model_</span><span class="p">,</span>
                    <span class="n">first_dp_subgroup_devices</span><span class="p">,</span>
                    <span class="n">backend</span><span class="p">,</span>
                    <span class="n">comp_supertask_kind</span><span class="p">,</span>
                    <span class="n">use_random_weight</span><span class="p">,</span>
                    <span class="n">qformat_path</span><span class="p">,</span>
                    <span class="n">qparam_path</span><span class="p">,</span>
                    <span class="n">quant_ckpt_file_path</span><span class="p">,</span>
                    <span class="n">one_supertask_per_device</span><span class="p">,</span>
                    <span class="n">use_blockwise_compile</span><span class="p">,</span>
                    <span class="n">do_decompositions_for_model_rewrite</span><span class="p">,</span>
                    <span class="n">kv_cache_dtype</span><span class="o">.</span><span class="n">to_torch_dtype</span><span class="p">()</span> <span class="k">if</span> <span class="n">kv_cache_dtype</span> <span class="k">else</span> <span class="kc">None</span><span class="p">,</span>
                    <span class="n">sparse_select_version</span><span class="p">,</span>
                    <span class="n">num_pipeline_builder_workers</span><span class="p">,</span>
                    <span class="n">num_compile_workers</span><span class="p">,</span>
                    <span class="n">embed_all_constants_into_graph</span><span class="p">,</span>
                    <span class="n">num_blocks_per_supertask</span><span class="p">,</span>
                    <span class="n">num_blocks_per_pp_stage</span><span class="p">,</span>
                    <span class="n">_add_prefill_last_block_slice</span><span class="p">,</span>
                    <span class="n">param_file_path</span><span class="p">,</span>
                    <span class="n">param_saved_format</span><span class="p">,</span>
                    <span class="n">compiler_config_context</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">_cleanup</span><span class="p">,</span>
                <span class="p">)</span>

                <span class="c1"># Save artifacts berfoe copying pipelines according to `data_parallelism_size`.</span>
                <span class="k">if</span> <span class="n">artifacts_export_path</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">_save_engine_artifacts</span><span class="p">(</span>
                        <span class="n">artifacts_export_path</span><span class="p">,</span>
                        <span class="n">comp_supertask_kind</span><span class="p">,</span>
                        <span class="n">devices</span><span class="p">,</span>
                    <span class="p">)</span>
            <span class="k">finally</span><span class="p">:</span>
                <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;FURIOSA_COMPILE_DUMP_PATH&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>

        <span class="c1"># If data parallelism is used, replicate pipelines for each entity data parallelism subgroup.</span>
        <span class="k">if</span> <span class="n">data_parallel_size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Pipeline</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>  <span class="c1"># type: ignore[no-redef]</span>
                <span class="n">pipeline</span><span class="o">.</span><span class="n">shallow_copy_with_replaced_devices</span><span class="p">(</span>
                    <span class="nb">dict</span><span class="p">(</span><span class="n">zip_equal</span><span class="p">(</span><span class="n">first_dp_subgroup_devices</span><span class="p">,</span> <span class="n">flattened_pp_tp_group</span><span class="p">))</span>  <span class="c1"># type: ignore[arg-type]</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">pipeline</span><span class="p">,</span> <span class="n">flattened_pp_tp_group</span> <span class="ow">in</span> <span class="n">product</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">,</span> <span class="n">normalized_dev_mesh</span><span class="p">)</span>
            <span class="p">]</span>

        <span class="k">if</span> <span class="n">speculative_model</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">kv_cache_sharing_across_beams_config</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;Speculative decoding with beam search is not supported yet.&quot;</span>
                <span class="p">)</span>

            <span class="k">if</span> <span class="n">artifacts_export_path</span><span class="p">:</span>
                <span class="n">speculative_model_artifacts_export_path</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
                    <span class="n">artifacts_export_path</span><span class="p">,</span> <span class="s2">&quot;speculative_model&quot;</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">speculative_model_artifacts_export_path</span> <span class="o">=</span> <span class="kc">None</span>

            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">speculative_model</span><span class="p">,</span> <span class="n">LLM</span><span class="p">):</span>
                <span class="n">draft_llm</span> <span class="o">=</span> <span class="n">speculative_model</span>
                <span class="c1"># TODO: do we need to check given arguments match?</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">speculative_model</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
                <span class="n">draft_llm</span> <span class="o">=</span> <span class="n">LLM</span><span class="p">(</span>
                    <span class="n">speculative_model</span><span class="p">,</span>
                    <span class="n">task_type</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">task_type</span><span class="p">,</span>
                    <span class="n">llm_config</span><span class="o">=</span><span class="n">speculative_model_llm_config</span><span class="p">,</span>
                    <span class="n">qformat_path</span><span class="o">=</span><span class="n">speculative_model_qformat_path</span><span class="p">,</span>
                    <span class="n">qparam_path</span><span class="o">=</span><span class="n">speculative_model_qparam_path</span><span class="p">,</span>
                    <span class="n">quant_ckpt_file_path</span><span class="o">=</span><span class="n">speculative_model_quant_ckpt_file_path</span><span class="p">,</span>
                    <span class="n">hf_overrides</span><span class="o">=</span><span class="n">speculative_model_config</span><span class="p">,</span>
                    <span class="n">bucket_config</span><span class="o">=</span><span class="n">speculative_model_bucket_config</span><span class="p">,</span>
                    <span class="n">max_seq_len_to_capture</span><span class="o">=</span><span class="n">max_seq_len_to_capture</span><span class="p">,</span>
                    <span class="c1"># TODO:Expose parallel config for specualtive model later?</span>
                    <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
                    <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="n">pipeline_parallel_size</span><span class="p">,</span>
                    <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">data_parallel_size</span><span class="p">,</span>
                    <span class="c1"># NOTE: tokenizer is shared between main model and specualtive model.</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                    <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
                    <span class="c1"># Use same configs of big model.</span>
                    <span class="c1"># TODO: do we need to expose param_file_path for the specualtive model?</span>
                    <span class="n">do_decompositions_for_model_rewrite</span><span class="o">=</span><span class="n">do_decompositions_for_model_rewrite</span><span class="p">,</span>
                    <span class="n">comp_supertask_kind</span><span class="o">=</span><span class="n">comp_supertask_kind</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
                    <span class="n">use_blockwise_compile</span><span class="o">=</span><span class="n">use_blockwise_compile</span><span class="p">,</span>
                    <span class="n">num_blocks_per_supertask</span><span class="o">=</span><span class="n">num_blocks_per_supertask</span><span class="p">,</span>
                    <span class="n">embed_all_constants_into_graph</span><span class="o">=</span><span class="n">embed_all_constants_into_graph</span><span class="p">,</span>
                    <span class="n">paged_attention_num_blocks</span><span class="o">=</span><span class="n">speculative_model_paged_attention_num_blocks</span><span class="p">,</span>
                    <span class="n">paged_attention_block_size</span><span class="o">=</span><span class="n">paged_attention_block_size</span><span class="p">,</span>
                    <span class="n">kv_cache_sharing_across_beams_config</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                    <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
                    <span class="n">packing_type</span><span class="o">=</span><span class="n">packing_type</span><span class="p">,</span>
                    <span class="c1"># TODO: Should we expose this?</span>
                    <span class="n">compiler_config_overrides</span><span class="o">=</span><span class="n">compiler_config_overrides</span><span class="p">,</span>
                    <span class="c1"># TODO: Should we expose this?</span>
                    <span class="n">use_random_weight</span><span class="o">=</span><span class="n">use_random_weight</span><span class="p">,</span>
                    <span class="n">num_pipeline_builder_workers</span><span class="o">=</span><span class="n">num_pipeline_builder_workers</span><span class="p">,</span>
                    <span class="n">num_compile_workers</span><span class="o">=</span><span class="n">num_compile_workers</span><span class="p">,</span>
                    <span class="n">skip_engine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                    <span class="n">artifacts_export_path</span><span class="o">=</span><span class="n">speculative_model_artifacts_export_path</span><span class="p">,</span>
                    <span class="n">_add_prefill_last_block_slice</span><span class="o">=</span><span class="n">_add_prefill_last_block_slice</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`speculative_model` must be either a pretraiend model id or an instance of LLM.&quot;</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">draft_pipelines</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="n">Pipeline</span><span class="p">]]</span> <span class="o">=</span> <span class="n">draft_llm</span><span class="o">.</span><span class="n">pipelines</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">draft_generator_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">GeneratorConfig</span><span class="p">]</span> <span class="o">=</span> <span class="n">draft_llm</span><span class="o">.</span><span class="n">generator_config</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">speculative_model_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">PretrainedConfig</span><span class="p">]</span> <span class="o">=</span> <span class="n">draft_llm</span><span class="o">.</span><span class="n">model_config</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">draft_pipelines</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">draft_generator_config</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">speculative_model_config</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="c1"># for e2e testing purpose, it allows to skip to initialize the engine</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">skip_engine</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="kn">from</span> <span class="nn">furiosa.native_runtime.llm</span> <span class="kn">import</span> <span class="n">NativeLLMEngine</span>  <span class="c1"># type: ignore</span>
            <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span>
                    <span class="s2">&quot;NativeLLMEngine is not available. Please make sure that the furiosa-native-runtime is installed.</span><span class="se">\n</span><span class="s2">&quot;</span>
                    <span class="s1">&#39;You can install furiosa-native-runtime by running `pip install furiosa-llm`.</span><span class="se">\n</span><span class="s1">&#39;</span>
                    <span class="s2">&quot;If you want to use the LLM without the native runtime, you can set `skip_engine=True` in the constructor.&quot;</span>
                <span class="p">)</span>
                <span class="k">raise</span>

            <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                <span class="n">bucket</span><span class="o">.</span><span class="n">kv_cache_size</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">bucket</span><span class="o">.</span><span class="n">input_ids_size</span> <span class="o">&gt;</span> <span class="mi">1</span>
                <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                    <span class="s2">&quot;Currently NativeLLMEngine does not support speculative decoding.&quot;</span>
                <span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">engine</span> <span class="o">=</span> <span class="n">NativeLLMEngine</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">draft_pipelines</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">generator_config_in_deprecated_format</span><span class="p">,</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">draft_generator_config</span><span class="p">,</span>
                <span class="n">scheduler_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">scheduler_config</span><span class="p">,</span>
                <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>
                <span class="n">hf_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
                <span class="n">draft_hf_config</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">speculative_model_config</span><span class="p">,</span>
            <span class="p">)</span>

<div class="viewcode-block" id="LLM.load_artifacts">
<a class="viewcode-back" href="../../furiosa_llm/references/llm.html#furiosa_llm.LLM.load_artifacts">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">load_artifacts</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="c1"># Runtime Configuration</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">data_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">bucket_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BucketConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scheduler_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SchedulerConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">paged_attention_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="c1"># Other Configuration</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_mode</span><span class="p">:</span> <span class="n">TokenizerModeType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span> <span class="o">=</span> <span class="n">CACHE_DIR</span><span class="p">,</span>
        <span class="n">skip_engine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LLM&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Instantiate LLM from saved artifacts without quantization and compilation.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: A path to artifacts to load.</span>
<span class="sd">            devices: The devices to run the model. It can be a single device or a list of devices.</span>
<span class="sd">                Each device can be either &quot;npu:X&quot; or &quot;npu:X:*&quot; where X is a specific device index.</span>
<span class="sd">                If not given, devices saved in the artifacts will be used.</span>
<span class="sd">            data_parallel_size: The size of the data parallelism group. If not given, it will be inferred from</span>
<span class="sd">                total avaialble PEs and other parallelism degrees.</span>
<span class="sd">            bucket_config: Config for bucket generating policy. If not given, all buckets in the artifacts will be used.</span>
<span class="sd">            scheduler_config: Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples</span>
<span class="sd">                that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. If this is not given, scheduler config</span>
<span class="sd">                saved in the artifacts will be used.</span>
<span class="sd">            paged_attention_num_blocks: The maximum number of blocks that each k/v storage per layer can store. This argument must be given</span>
<span class="sd">                if model uses paged attention.</span>
<span class="sd">            tokenizer: The name or path of a HuggingFace Transformers tokenizer.</span>
<span class="sd">            tokenizer_mode: The tokenizer mode. &quot;auto&quot; will use the fast tokenizer</span>
<span class="sd">                if available, and &quot;slow&quot; will always use the slow tokenizer.</span>
<span class="sd">            seed: The seed to initialize the random number generator for sampling.</span>
<span class="sd">            cache_dir: The cache directory for all generated files for this LLM instance.</span>
<span class="sd">                When its value is ``None``, caching is disabled. The default is &quot;$HOME/.cache/furiosa/llm&quot;.</span>
<span class="sd">            skip_engine: If True, the native runtime engine will not be initialized. This is useful when you need</span>
<span class="sd">                the pipelines for other purposes than running them with the engine.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># check path is valid</span>
        <span class="n">artifact_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/artifact.json&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">artifact_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This artifacts is not valid.&quot;</span><span class="p">)</span>

        <span class="n">runtime_config_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/runtime_config.json&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">runtime_config_path</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Runtime configuration file is not valid.&quot;</span><span class="p">)</span>

        <span class="c1"># TODO: validation artifacts, pipelines, edf, safetensors</span>
        <span class="n">artifact</span> <span class="o">=</span> <span class="n">Artifact</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">artifact_path</span><span class="p">)</span>
        <span class="n">runtime_config</span> <span class="o">=</span> <span class="n">RuntimeConfig</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">runtime_config_path</span><span class="p">)</span>

        <span class="n">generator_config</span> <span class="o">=</span> <span class="n">artifact</span><span class="o">.</span><span class="n">generator_config</span>
        <span class="n">model_metadata</span> <span class="o">=</span> <span class="n">artifact</span><span class="o">.</span><span class="n">model_metadata</span>
        <span class="n">model_rewriting_config</span> <span class="o">=</span> <span class="n">artifact</span><span class="o">.</span><span class="n">model_rewriting_config</span>
        <span class="n">parallel_config</span> <span class="o">=</span> <span class="n">artifact</span><span class="o">.</span><span class="n">parallel_config</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">devices</span><span class="p">:</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">_get_available_devices</span><span class="p">()</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">bucket_config</span><span class="p">:</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">runtime_config</span><span class="o">.</span><span class="n">prefill_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                <span class="ow">and</span> <span class="n">runtime_config</span><span class="o">.</span><span class="n">decode_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="p">):</span>
                <span class="n">bucket_config</span> <span class="o">=</span> <span class="n">ManualBucketConfig</span><span class="p">(</span>
                    <span class="n">prefill_buckets</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">prefill_buckets</span><span class="p">,</span>
                    <span class="n">decode_buckets</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">decode_buckets</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">prefill_buckets</span> <span class="o">=</span> <span class="p">[]</span>
                <span class="n">decode_buckets</span> <span class="o">=</span> <span class="p">[]</span>

                <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_prefill</span><span class="p">:</span>
                        <span class="n">prefill_buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
                    <span class="k">elif</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_decode</span> <span class="ow">and</span> <span class="n">bucket</span><span class="o">.</span><span class="n">input_ids_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">decode_buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="c1"># Bucket for speculative decoding.</span>
                        <span class="c1"># speculative_decoding_buckets.append(bucket)</span>
                        <span class="k">pass</span>

                <span class="n">bucket_config</span> <span class="o">=</span> <span class="n">ManualBucketConfig</span><span class="p">(</span>
                    <span class="n">prefill_buckets</span><span class="o">=</span><span class="p">[</span>
                        <span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">prefill_buckets</span>
                    <span class="p">],</span>
                    <span class="n">decode_buckets</span><span class="o">=</span><span class="p">[</span>
                        <span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">decode_buckets</span>
                    <span class="p">],</span>
                <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">scheduler_config</span><span class="p">:</span>
            <span class="n">scheduler_config</span> <span class="o">=</span> <span class="n">SchedulerConfig</span><span class="p">(</span>
                <span class="n">npu_queue_limit</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">npu_queue_limit</span><span class="p">,</span>
                <span class="n">max_processing_samples</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">max_processing_samples</span><span class="p">,</span>
                <span class="n">spare_blocks_ratio</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">spare_blocks_ratio</span><span class="p">,</span>
                <span class="n">is_offline</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">is_offline</span><span class="p">,</span>
                <span class="n">prefill_chunk_size</span><span class="o">=</span><span class="n">runtime_config</span><span class="o">.</span><span class="n">prefill_chunk_size</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">paged_attention_num_blocks</span><span class="p">:</span>
            <span class="n">paged_attention_num_blocks</span> <span class="o">=</span> <span class="n">runtime_config</span><span class="o">.</span><span class="n">paged_attention_num_blocks</span>

        <span class="c1"># Find the max attention_size from the pipelines&#39; buckets</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bucket_config</span><span class="p">,</span> <span class="n">ManualBucketConfig</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">decode_buckets</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">model_metadata</span><span class="o">.</span><span class="n">is_generative_model</span>
                <span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">bucket</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">decode_buckets</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">bucket</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">prefill_buckets</span><span class="p">]</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bucket_config</span><span class="p">,</span> <span class="n">MinimalBucketConfig</span><span class="p">):</span>
            <span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">max_seq_len</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`bucket_config` must be an instance of BucketConfig.&quot;</span><span class="p">)</span>

        <span class="c1"># Load all saved pipelines</span>
        <span class="n">pipelines</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">__load_pipelines</span><span class="p">(</span>
            <span class="n">path</span><span class="p">,</span>
            <span class="n">try_from_lir</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">try_from_dfg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">artifact</span><span class="o">=</span><span class="n">artifact</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">model_metadata</span><span class="o">.</span><span class="n">pretrained_id</span><span class="p">,</span>
            <span class="n">task_type</span><span class="o">=</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">task_type</span><span class="p">,</span>
            <span class="n">llm_config</span><span class="o">=</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">llm_config</span><span class="p">,</span>
            <span class="n">hf_overrides</span><span class="o">=</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">hf_configs</span><span class="p">,</span>
            <span class="n">bucket_config</span><span class="o">=</span><span class="n">bucket_config</span><span class="p">,</span>
            <span class="n">speculative_model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">num_speculative_tokens</span><span class="o">=</span><span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span><span class="p">,</span>
            <span class="n">max_seq_len_to_capture</span><span class="o">=</span><span class="n">max_seq_len_to_capture</span><span class="p">,</span>
            <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
            <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_parallel_size</span><span class="p">,</span>
            <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">data_parallel_size</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">tokenizer_mode</span><span class="o">=</span><span class="n">tokenizer_mode</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
            <span class="n">do_decompositions_for_model_rewrite</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">do_decompositions_for_model_rewrite</span><span class="p">,</span>
            <span class="n">comp_supertask_kind</span><span class="o">=</span><span class="s2">&quot;edf&quot;</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">LLMBackend</span><span class="o">.</span><span class="n">FURIOSA_RT_V2</span><span class="p">,</span>
            <span class="n">use_blockwise_compile</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">use_blockwise_compile</span><span class="p">,</span>
            <span class="n">num_blocks_per_supertask</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">num_blocks_per_supertask</span><span class="p">,</span>
            <span class="n">embed_all_constants_into_graph</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">embed_all_constants_into_graph</span><span class="p">,</span>
            <span class="n">paged_attention_num_blocks</span><span class="o">=</span><span class="n">paged_attention_num_blocks</span><span class="p">,</span>
            <span class="n">kv_cache_sharing_across_beams_config</span><span class="o">=</span><span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="p">,</span>
            <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
            <span class="n">packing_type</span><span class="o">=</span><span class="n">generator_config</span><span class="o">.</span><span class="n">packing_type</span><span class="p">,</span>
            <span class="n">skip_engine</span><span class="o">=</span><span class="n">skip_engine</span><span class="p">,</span>
            <span class="n">_pipelines</span><span class="o">=</span><span class="n">pipelines</span><span class="p">,</span>
            <span class="n">_add_prefill_last_block_slice</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">add_prefill_last_block_slice</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


<div class="viewcode-block" id="LLM.from_artifacts">
<a class="viewcode-back" href="../../furiosa_llm/references/llm.html#furiosa_llm.LLM.from_artifacts">[docs]</a>
    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">from_artifacts</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">bucket_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BucketConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="s2">&quot;LLM&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">num_speculative_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_bucket_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BucketConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">use_speculative_decoding_if_possible</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
        <span class="n">data_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_mode</span><span class="p">:</span> <span class="n">TokenizerModeType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
        <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span> <span class="o">=</span> <span class="n">CACHE_DIR</span><span class="p">,</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">LLMBackend</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">paged_attention_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">scheduler_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SchedulerConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">speculative_model_paged_attention_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">packing_type</span><span class="p">:</span> <span class="n">Literal</span><span class="p">[</span><span class="s2">&quot;IDENTITY&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;IDENTITY&quot;</span><span class="p">,</span>
        <span class="n">skip_engine</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="o">*</span><span class="p">,</span>
        <span class="n">_cleanup</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="s2">&quot;LLM&quot;</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Instantiate LLM from saved artifacts without quantization and compilation.</span>

<span class="sd">        Args:</span>
<span class="sd">            path: A path to artifacts to load.</span>
<span class="sd">            bucket_config: Config for bucket generating policy. If not given, all buckets in the artifacts will be used.</span>
<span class="sd">            speculative_model: Draft model for speculative decoding.</span>
<span class="sd">            num_speculative_tokens: The number of tokens that specualtive model will generate speculatively during each iteration of the decoding process.</span>
<span class="sd">            speculative_model_bucket_config: Bucket generating config for speculative_model. If not given,</span>
<span class="sd">                all specualtive model buckets (if exists) in the artifacts will be used.</span>
<span class="sd">            use_speculative_decoding_if_possible: If True, speculative decoding will be used if possible</span>
<span class="sd">                (`speculative_model` is given or there&#39;s artifacts for specualtive model in the artifacts.).</span>
<span class="sd">                Otherwise, speculative decoding will not be used.</span>
<span class="sd">            data_parallel_size: The size of the data parallelism group. If not given, it will be inferred from</span>
<span class="sd">                total avaialble PEs and other parallelism degrees.</span>
<span class="sd">            tokenizer: The name or path of a HuggingFace Transformers tokenizer.</span>
<span class="sd">            tokenizer_mode: The tokenizer mode. &quot;auto&quot; will use the fast tokenizer</span>
<span class="sd">                if available, and &quot;slow&quot; will always use the slow tokenizer.</span>
<span class="sd">            seed: The seed to initialize the random number generator for sampling.</span>
<span class="sd">            devices: The devices to run the model. It can be a single device or a list of devices.</span>
<span class="sd">                Each device can be either &quot;npu:X&quot; or &quot;npu:X:*&quot; where X is a specific device index.</span>
<span class="sd">                If not given, devices saved in the artifacts will be used.</span>
<span class="sd">            cache_dir: The cache directory for all generated files for this LLM instance.</span>
<span class="sd">                When its value is ``None``, caching is disabled. The default is &quot;$HOME/.cache/furiosa/llm&quot;.</span>
<span class="sd">            backend: The backend implementation to run forward() of a model for the LLM.</span>
<span class="sd">                The default is LLMBackend.TORCH_PIPELINE_RUNNER.</span>
<span class="sd">            paged_attention_num_blocks: The maximum number of blocks that each k/v storage per layer can store. This argument must be given</span>
<span class="sd">                if model uses paged attention.</span>
<span class="sd">            scheduler_config: Configuration for the scheduler, allowing to maximum number of tasks which can be queued to HW, maximum number of samples</span>
<span class="sd">                that can be processed by the scheduler, and ratio of spare blocks that are reserved by scheduler. If this is not given, scheduler config</span>
<span class="sd">                saved in the artifacts will be used.</span>
<span class="sd">            speculative_model_paged_attention_num_blocks: The maximum number of blocks that each k/v storage per layer can store for the specualtive model. This argument must be given</span>
<span class="sd">                if the specualtive model uses paged attention.</span>
<span class="sd">            packing_type: Packing algorithm. Possible values are &quot;IDENTITY&quot; only for now</span>
<span class="sd">            skip_engine: If True, the native runtime engine will not be initialized. This is useful when you need</span>
<span class="sd">                the pipelines for other purposes than running them with the engine.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/ready&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;This artifacts is not valid.&quot;</span><span class="p">)</span>

        <span class="c1"># Load configs</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">generator_config</span> <span class="o">=</span> <span class="n">GeneratorConfig</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/generator_config.json&quot;</span><span class="p">)</span>
            <span class="n">scheduler_config_</span> <span class="o">=</span> <span class="n">SchedulerConfig</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/scheduler_config.json&quot;</span><span class="p">)</span>
        <span class="k">except</span> <span class="n">pydantic</span><span class="o">.</span><span class="n">ValidationError</span><span class="p">:</span>
            <span class="c1"># Generator config is in deprecated format.</span>
            <span class="n">generator_config_</span> <span class="o">=</span> <span class="n">DeprecatedGeneratorConfig</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/generator_config.json&quot;</span><span class="p">)</span>

            <span class="n">new_buckets</span> <span class="o">=</span> <span class="p">[</span>
                <span class="o">*</span><span class="p">(</span>
                    <span class="n">Bucket</span><span class="o">.</span><span class="n">prefill</span><span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">generator_config_</span><span class="o">.</span><span class="n">prefill_buckets</span>
                <span class="p">),</span>
                <span class="o">*</span><span class="p">(</span>
                    <span class="n">Bucket</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">generator_config_</span><span class="o">.</span><span class="n">decode_buckets</span>
                <span class="p">),</span>
            <span class="p">]</span>

            <span class="c1"># Artifacts&#39; generator config was saved with previous version.</span>
            <span class="c1"># TODO: remove this after deprecating previous version of generator config type.</span>
            <span class="n">generator_config</span> <span class="o">=</span> <span class="n">GeneratorConfig</span><span class="p">(</span>
                <span class="n">generator_config_</span><span class="o">.</span><span class="n">position_id_pad</span><span class="p">,</span>
                <span class="n">new_buckets</span><span class="p">,</span>
                <span class="n">generator_config_</span><span class="o">.</span><span class="n">model_qname</span><span class="p">,</span>
                <span class="n">generator_config_</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="p">,</span>
                <span class="n">generator_config_</span><span class="o">.</span><span class="n">packing_type</span><span class="p">,</span>
                <span class="n">generator_config_</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="p">,</span>
                <span class="kc">None</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">assert</span> <span class="n">generator_config_</span><span class="o">.</span><span class="n">scheduler_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
            <span class="n">scheduler_config_</span> <span class="o">=</span> <span class="n">generator_config_</span><span class="o">.</span><span class="n">scheduler_config</span>

        <span class="c1"># Use saved scheduler config if not given.</span>
        <span class="n">scheduler_config</span> <span class="o">=</span> <span class="n">scheduler_config</span> <span class="ow">or</span> <span class="n">scheduler_config_</span>

        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/model_metadata.json&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">model_metadata</span> <span class="o">=</span> <span class="n">ModelMetadata</span><span class="o">.</span><span class="n">model_validate_json</span><span class="p">(</span><span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/model_rewriting_config.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">model_rewriting_config</span> <span class="o">=</span> <span class="n">ModelRewritingConfig</span><span class="o">.</span><span class="n">model_validate_json</span><span class="p">(</span><span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/parallel_config.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">parallel_config</span> <span class="o">=</span> <span class="n">ParallelConfig</span><span class="o">.</span><span class="n">model_validate_json</span><span class="p">(</span><span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/other_config.json&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">other_configs</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span>

        <span class="n">try_from_lir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LLM_ENGINE_ARTIFACTS_TRY_FROM_LIR&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span>
        <span class="n">try_from_dfg</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;LLM_ENGINE_ARTIFACTS_TRY_FROM_DFG&quot;</span><span class="p">,</span> <span class="s2">&quot;0&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;1&quot;</span>

        <span class="c1"># Load all saved pipelines</span>
        <span class="n">pipelines</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">__load_pipelines</span><span class="p">(</span>
            <span class="n">path</span><span class="p">,</span>
            <span class="n">try_from_lir</span><span class="o">=</span><span class="n">try_from_lir</span><span class="p">,</span>
            <span class="n">try_from_dfg</span><span class="o">=</span><span class="n">try_from_dfg</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="n">paged_attention_num_blocks</span> <span class="o">=</span> <span class="n">paged_attention_num_blocks</span> <span class="ow">or</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="n">generator_config</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="p">,</span> <span class="s2">&quot;num_blocks&quot;</span><span class="p">,</span> <span class="kc">None</span>
        <span class="p">)</span>
        <span class="n">paged_attention_block_size</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span>
            <span class="n">generator_config</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="p">,</span> <span class="s2">&quot;block_size&quot;</span><span class="p">,</span> <span class="mi">1</span>
        <span class="p">)</span>
        <span class="n">comp_supertask_kind</span> <span class="o">=</span> <span class="n">other_configs</span><span class="p">[</span><span class="s2">&quot;comp_supertask_kind&quot;</span><span class="p">]</span>

        <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">devices</span> <span class="o">=</span> <span class="n">other_configs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;devices&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">devices</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="o">.</span><span class="n">get</span><span class="p">(</span>
                <span class="s2">&quot;DISABLE_LLM_ENGINE_ARTIFACTS_STRICT_LOAD&quot;</span><span class="p">,</span> <span class="kc">False</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Saved devices info is not found in the artifacts. Please provide devices explicitly or set `DISABLE_LLM_ENGINE_ARTIFACTS_STRICT_LOAD=1` to use all available devices.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">devices</span><span class="p">:</span>
                <span class="n">devices</span> <span class="o">=</span> <span class="p">[</span><span class="n">Device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">]</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Device is not given, fall back to devices specified in artifact: </span><span class="si">{</span><span class="n">devices</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="n">use_speculative_decoding_if_possible</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">speculative_model</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">speculative_model</span><span class="p">,</span> <span class="n">LLM</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">speculative_model_paged_attention_num_blocks</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="n">speculative_model</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                            <span class="s2">&quot;Given specualtive model does not support paged attention but `speculative_model_paged_attention_num_blocks` is given.&quot;</span>
                        <span class="p">)</span>
                    <span class="n">speculative_model</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="o">.</span><span class="n">num_blocks</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">speculative_model_paged_attention_num_blocks</span>
                    <span class="p">)</span>

                <span class="k">if</span> <span class="n">speculative_model_bucket_config</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
                        <span class="s2">&quot;`speculative_model_bucket_config` is not supported when &quot;</span>
                    <span class="p">)</span>
            <span class="k">elif</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/speculative_model&quot;</span><span class="p">):</span>
                <span class="c1"># specualtive model exists.</span>
                <span class="n">speculative_model</span> <span class="o">=</span> <span class="bp">cls</span><span class="o">.</span><span class="n">from_artifacts</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/speculative_model&quot;</span><span class="p">,</span>
                    <span class="n">bucket_config</span><span class="o">=</span><span class="n">speculative_model_bucket_config</span><span class="p">,</span>
                    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
                    <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
                    <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
                    <span class="n">paged_attention_num_blocks</span><span class="o">=</span><span class="n">speculative_model_paged_attention_num_blocks</span><span class="p">,</span>
                    <span class="n">packing_type</span><span class="o">=</span><span class="n">packing_type</span><span class="p">,</span>
                    <span class="n">skip_engine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">num_speculative_tokens</span><span class="p">:</span>
                <span class="k">if</span> <span class="p">(</span>
                    <span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span>
                    <span class="ow">and</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span> <span class="o">!=</span> <span class="n">num_speculative_tokens</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;`num_speculative_tokens` must be same as saved `num_speculative_tokens` value in artifacts: </span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">speculative_model</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="s2">&quot;Specualtive model given, but original model artifacts is not for speculative decoding.&quot;</span>
                    <span class="p">)</span>
                <span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span> <span class="o">=</span> <span class="n">num_speculative_tokens</span>

            <span class="k">if</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">speculative_model</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;You want to use speculative decoding but speculative model is not given and cannot be loaded.&quot;</span>
                <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Don&#39;t use speculative decoding.</span>
            <span class="k">if</span> <span class="n">speculative_model</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Ignore `speculative_model` because `use_speculative_decoding_if_possible` is False.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="n">num_speculative_tokens</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                    <span class="s2">&quot;Ignore `num_speculative_tokens` because `use_speculative_decoding_if_possible` is False.&quot;</span>
                <span class="p">)</span>
            <span class="n">num_speculative_tokens</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="n">other_buckets</span> <span class="o">=</span> <span class="p">[]</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">bucket_config</span><span class="p">:</span>
            <span class="c1"># If bucket config is not given, load available buckets from the artifacts.</span>
            <span class="n">prefill_buckets_</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">decode_buckets_</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_prefill</span><span class="p">:</span>
                    <span class="n">prefill_buckets_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">bucket</span><span class="o">.</span><span class="n">is_decode</span> <span class="ow">and</span> <span class="n">bucket</span><span class="o">.</span><span class="n">input_ids_size</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                    <span class="n">decode_buckets_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="c1"># Bucket for speculative decoding.</span>
                    <span class="n">other_buckets</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">bucket</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">num_speculative_tokens</span><span class="p">:</span>
                <span class="c1"># Check all needed verification buckets exist.</span>
                <span class="n">verification_buckets</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span>
                    <span class="n">bucket</span>
                    <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">other_buckets</span>
                    <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">input_ids_size</span> <span class="o">==</span> <span class="n">num_speculative_tokens</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="p">)</span>

                <span class="c1"># There should be verification bucket with same attention size for each decode bucket.</span>
                <span class="k">if</span> <span class="nb">any</span><span class="p">(</span>
                    <span class="n">Bucket</span><span class="p">(</span>
                        <span class="n">decode_bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span>
                        <span class="n">decode_bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">,</span>
                        <span class="n">decode_bucket</span><span class="o">.</span><span class="n">attention_size</span> <span class="o">-</span> <span class="n">num_speculative_tokens</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="ow">not</span> <span class="ow">in</span> <span class="n">verification_buckets</span>
                    <span class="k">for</span> <span class="n">decode_bucket</span> <span class="ow">in</span> <span class="n">decode_buckets_</span>
                <span class="p">):</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;There is missing verfication buckets in the artifacts. Buckets with input_ids_length </span><span class="si">{</span><span class="n">num_speculative_tokens</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> should exists.&quot;</span>
                    <span class="p">)</span>

            <span class="n">prefill_buckets</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">prefill_buckets_</span>
            <span class="p">]</span>
            <span class="n">decode_buckets</span> <span class="o">=</span> <span class="p">[</span>
                <span class="p">(</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">decode_buckets_</span>
            <span class="p">]</span>
            <span class="n">bucket_config</span> <span class="o">=</span> <span class="n">ManualBucketConfig</span><span class="p">(</span><span class="n">prefill_buckets</span><span class="p">,</span> <span class="n">decode_buckets</span><span class="p">)</span>

        <span class="c1"># Find the max attention_size from the pipelines&#39; buckets</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bucket_config</span><span class="p">,</span> <span class="n">ManualBucketConfig</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">decode_buckets</span><span class="p">:</span>
                <span class="k">assert</span> <span class="n">model_metadata</span><span class="o">.</span><span class="n">is_generative_model</span>
                <span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">bucket</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">decode_buckets</span><span class="p">])</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">bucket</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">prefill_buckets</span><span class="p">]</span>
                <span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">bucket_config</span><span class="p">,</span> <span class="n">MinimalBucketConfig</span><span class="p">):</span>
            <span class="n">max_seq_len_to_capture</span> <span class="o">=</span> <span class="n">bucket_config</span><span class="o">.</span><span class="n">max_seq_len</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`bucket_config` must be an instance of BucketConfig.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">cls</span><span class="p">(</span>
            <span class="n">model_metadata</span><span class="o">.</span><span class="n">pretrained_id</span><span class="p">,</span>
            <span class="n">task_type</span><span class="o">=</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">task_type</span><span class="p">,</span>
            <span class="n">llm_config</span><span class="o">=</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">llm_config</span><span class="p">,</span>
            <span class="n">hf_overrides</span><span class="o">=</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">hf_configs</span><span class="p">,</span>
            <span class="n">bucket_config</span><span class="o">=</span><span class="n">bucket_config</span><span class="p">,</span>
            <span class="n">speculative_model</span><span class="o">=</span><span class="n">speculative_model</span><span class="p">,</span>
            <span class="n">num_speculative_tokens</span><span class="o">=</span><span class="n">generator_config</span><span class="o">.</span><span class="n">num_speculative_tokens</span><span class="p">,</span>
            <span class="n">max_seq_len_to_capture</span><span class="o">=</span><span class="n">max_seq_len_to_capture</span><span class="p">,</span>
            <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">tensor_parallel_size</span><span class="p">,</span>
            <span class="n">pipeline_parallel_size</span><span class="o">=</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">pipeline_parallel_size</span><span class="p">,</span>
            <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">data_parallel_size</span><span class="p">,</span>
            <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
            <span class="n">tokenizer_mode</span><span class="o">=</span><span class="n">tokenizer_mode</span><span class="p">,</span>
            <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
            <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
            <span class="n">do_decompositions_for_model_rewrite</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">do_decompositions_for_model_rewrite</span><span class="p">,</span>
            <span class="n">comp_supertask_kind</span><span class="o">=</span><span class="n">comp_supertask_kind</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">backend</span><span class="o">=</span><span class="n">backend</span><span class="p">,</span>
            <span class="n">use_blockwise_compile</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">use_blockwise_compile</span><span class="p">,</span>
            <span class="n">num_blocks_per_supertask</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">num_blocks_per_supertask</span><span class="p">,</span>
            <span class="n">embed_all_constants_into_graph</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">embed_all_constants_into_graph</span><span class="p">,</span>
            <span class="n">paged_attention_num_blocks</span><span class="o">=</span><span class="n">paged_attention_num_blocks</span><span class="p">,</span>
            <span class="n">paged_attention_block_size</span><span class="o">=</span><span class="n">paged_attention_block_size</span><span class="p">,</span>
            <span class="n">kv_cache_sharing_across_beams_config</span><span class="o">=</span><span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="p">,</span>
            <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
            <span class="n">packing_type</span><span class="o">=</span><span class="n">packing_type</span><span class="p">,</span>
            <span class="n">skip_engine</span><span class="o">=</span><span class="n">skip_engine</span><span class="p">,</span>
            <span class="n">_pipelines</span><span class="o">=</span><span class="n">pipelines</span><span class="p">,</span>
            <span class="n">_custom_buckets</span><span class="o">=</span><span class="n">other_buckets</span><span class="p">,</span>
            <span class="n">_add_prefill_last_block_slice</span><span class="o">=</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">add_prefill_last_block_slice</span><span class="p">,</span>
            <span class="n">_cleanup</span><span class="o">=</span><span class="n">_cleanup</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span></div>


    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__load_pipelines</span><span class="p">(</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">try_from_lir</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">try_from_dfg</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">]</span> <span class="o">=</span> <span class="n">CACHE_DIR</span><span class="p">,</span>
        <span class="n">artifact</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Artifact</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Pipeline</span><span class="p">]:</span>
        <span class="n">pipelines</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="c1"># FIXME: unused &#39;bucket&#39;</span>
        <span class="n">bucket_to_pipeline</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Bucket</span><span class="p">,</span> <span class="n">Pipeline</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">if</span> <span class="n">artifact</span> <span class="ow">and</span> <span class="n">artifact</span><span class="o">.</span><span class="n">pipelines</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">pipeline_dict</span> <span class="ow">in</span> <span class="n">artifact</span><span class="o">.</span><span class="n">pipelines</span><span class="p">:</span>
                <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="o">.</span><span class="n">from_dict</span><span class="p">(</span><span class="n">pipeline_dict</span><span class="p">)</span>
                <span class="n">bucket</span> <span class="o">=</span> <span class="n">_get_bucket_from_pipeline_name</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">bucket_to_pipeline</span><span class="p">[</span><span class="n">bucket</span><span class="p">]</span> <span class="o">=</span> <span class="n">pipeline</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">glob</span><span class="o">.</span><span class="n">glob</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/pipeline.*.json&quot;</span><span class="p">))):</span>
                <span class="n">pipeline</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/pipeline.</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">.json&quot;</span><span class="p">)</span>
                <span class="n">bucket</span> <span class="o">=</span> <span class="n">_get_bucket_from_pipeline_name</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
                <span class="n">bucket_to_pipeline</span><span class="p">[</span><span class="n">bucket</span><span class="p">]</span> <span class="o">=</span> <span class="n">pipeline</span>

        <span class="k">for</span> <span class="n">bucket</span><span class="p">,</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="n">bucket_to_pipeline</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="n">blob_to_device</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">DataBlobId</span><span class="p">,</span> <span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">task</span> <span class="ow">in</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">supertasks</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">CompSuperTask</span><span class="p">)</span> <span class="ow">and</span> <span class="n">task</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">EDF</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">task</span><span class="o">.</span><span class="n">data_blob</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>
                    <span class="n">blob_to_device</span><span class="p">[</span><span class="n">task</span><span class="o">.</span><span class="n">data_blob</span><span class="p">]</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">devices</span><span class="p">[</span><span class="n">task</span><span class="o">.</span><span class="n">device</span><span class="p">]</span>

            <span class="n">blob_kind</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">get_blob_kind</span><span class="p">()</span>
            <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">kind</span> <span class="o">=</span> <span class="n">blob_kind</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">FX</span><span class="p">:</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.fx&quot;</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                        <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
                <span class="k">elif</span> <span class="n">kind</span> <span class="o">==</span> <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">EDF</span><span class="p">:</span>
                    <span class="k">try</span><span class="p">:</span>
                        <span class="kn">from</span> <span class="nn">furiosa.native_compiler</span> <span class="kn">import</span> <span class="p">(</span>  <span class="c1"># type: ignore[import]</span>
                            <span class="n">CompiledGraph</span><span class="p">,</span>
                            <span class="n">compile_from_path</span><span class="p">,</span>
                        <span class="p">)</span>
                    <span class="k">except</span> <span class="ne">ImportError</span><span class="p">:</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">error</span><span class="p">(</span><span class="s2">&quot;furiosa-native-compiler is required to load EDF format&quot;</span><span class="p">)</span>
                        <span class="k">raise</span>

                    <span class="n">compiler_config_yaml</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.config.yaml&quot;</span>
                    <span class="n">device</span> <span class="o">=</span> <span class="n">blob_to_device</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span>
                    <span class="n">target_npu</span> <span class="o">=</span> <span class="n">GraphModuleConverter</span><span class="o">.</span><span class="n">get_target_npu_from_device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="c1"># check if:</span>
                    <span class="c1">#   - edf file does not exist,</span>
                    <span class="c1">#   - try_from_lir is enabled,</span>
                    <span class="c1">#   - and lir exists</span>
                    <span class="c1"># then, compile lir to edf</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.edf&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="n">try_from_lir</span>
                        <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.lir&quot;</span><span class="p">)</span>
                    <span class="p">):</span>
                        <span class="k">if</span> <span class="n">try_from_dfg</span><span class="p">:</span>
                            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                                <span class="s2">&quot;Both TRY_FROM_LIR and TRY_FROM_DFG are enabled. In this case, TRY_FROM_LIR is prioritized.&quot;</span>
                            <span class="p">)</span>
                        <span class="n">compiler_config</span> <span class="o">=</span> <span class="n">try_compiler_config_from_yaml</span><span class="p">(</span><span class="n">compiler_config_yaml</span><span class="p">)</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Compiling LIR to EDF for </span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2"> with compiler config </span><span class="si">{</span><span class="n">compiler_config</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">compile_from_path</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.lir&quot;</span><span class="p">,</span>
                            <span class="n">target_npu</span><span class="p">,</span>
                            <span class="n">target_ir</span><span class="o">=</span><span class="s2">&quot;edf&quot;</span><span class="p">,</span>
                            <span class="n">config</span><span class="o">=</span><span class="n">compiler_config</span><span class="p">,</span>
                            <span class="n">dump_tag</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span>
                            <span class="n">dump_path</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">),</span>
                        <span class="p">)</span>
                        <span class="n">contents</span> <span class="o">=</span> <span class="n">CompiledGraph</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
                        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.edf&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>  <span class="c1"># type: ignore[assignment]</span>
                            <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

                    <span class="c1"># check if:</span>
                    <span class="c1">#   - edf file does not exist,</span>
                    <span class="c1">#   - try_from_dfg is enabled,</span>
                    <span class="c1">#   - and dfg exists</span>
                    <span class="c1"># then, compile dfg to edf</span>
                    <span class="k">if</span> <span class="p">(</span>
                        <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.edf&quot;</span><span class="p">)</span>
                        <span class="ow">and</span> <span class="n">try_from_dfg</span>
                        <span class="ow">and</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.dfg&quot;</span><span class="p">)</span>
                    <span class="p">):</span>
                        <span class="n">compiler_config</span> <span class="o">=</span> <span class="n">try_compiler_config_from_yaml</span><span class="p">(</span><span class="n">compiler_config_yaml</span><span class="p">)</span>
                        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;Compiling DFG to EDF for </span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2"> with compiler config </span><span class="si">{</span><span class="n">compiler_config</span><span class="si">}</span><span class="s2">&quot;</span>
                        <span class="p">)</span>
                        <span class="n">out</span> <span class="o">=</span> <span class="n">compile_from_path</span><span class="p">(</span>
                            <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.dfg&quot;</span><span class="p">,</span>
                            <span class="n">target_npu</span><span class="p">,</span>
                            <span class="n">target_ir</span><span class="o">=</span><span class="s2">&quot;edf&quot;</span><span class="p">,</span>
                            <span class="n">config</span><span class="o">=</span><span class="n">compiler_config</span><span class="p">,</span>
                            <span class="n">dump_tag</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span>
                            <span class="n">dump_path</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">path</span><span class="p">),</span>
                            <span class="n">dump_lir</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                        <span class="p">)</span>
                        <span class="n">contents</span> <span class="o">=</span> <span class="n">CompiledGraph</span><span class="o">.</span><span class="n">serialize</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
                        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.edf&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>  <span class="c1"># type: ignore[assignment]</span>
                            <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">contents</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type]</span>

                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.edf&quot;</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                        <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">CompiledGraph</span><span class="o">.</span><span class="n">deserialize</span><span class="p">(</span><span class="n">fp</span><span class="o">.</span><span class="n">read</span><span class="p">(),</span> <span class="n">tag</span><span class="o">=</span><span class="nb">id</span><span class="p">)</span>  <span class="c1"># type: ignore[arg-type, assignment]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SuperTask [</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">] is not supported to load&quot;</span><span class="p">)</span>

            <span class="c1"># Support both cases:</span>
            <span class="c1"># 1. param file is located in the artifacts directory</span>
            <span class="c1"># 2. param file is located in the global cache directory</span>
            <span class="k">for</span> <span class="n">param_idx</span><span class="p">,</span> <span class="n">param_file</span> <span class="ow">in</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">param_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="c1"># NOTE: param_file.path is already `os.path.basename`d</span>
                <span class="n">path_candidates</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">param_file</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">),</span>
                    <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cache_dir</span><span class="si">}</span><span class="s2">/param_files/</span><span class="si">{</span><span class="n">param_file</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">),</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">candidate</span> <span class="ow">in</span> <span class="n">path_candidates</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">candidate</span><span class="p">):</span>
                        <span class="n">param_file</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">candidate</span>
                        <span class="k">break</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Param file </span><span class="si">{</span><span class="n">param_file</span><span class="o">.</span><span class="n">path</span><span class="si">}</span><span class="s2"> is not found in neither artifacts path nor cache directory.&quot;</span>
                    <span class="p">)</span>
            <span class="n">pipelines</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">pipeline</span><span class="p">)</span>

        <span class="k">del</span> <span class="n">bucket_to_pipeline</span>

        <span class="k">return</span> <span class="n">pipelines</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">compute_prefill_buckets</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">preferred_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">prefill_buckets_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">prefill_buckets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]],</span>
        <span class="n">max_position_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Bucket</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">prefill_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">Bucket</span><span class="o">.</span><span class="n">prefill</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">attn_size</span> <span class="ow">in</span> <span class="n">prefill_buckets</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Generate the buckets automatically</span>
            <span class="n">percentage</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">prefill_buckets_num</span>
            <span class="n">interval</span> <span class="o">=</span> <span class="n">max_position_embeddings</span> <span class="o">*</span> <span class="n">percentage</span>
            <span class="n">atten_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">interval</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">prefill_buckets_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">Bucket</span><span class="o">.</span><span class="n">prefill</span><span class="p">(</span><span class="n">preferred_batch_size</span><span class="p">,</span> <span class="n">attn_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">attn_size</span> <span class="ow">in</span> <span class="n">atten_sizes</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">compute_decode_buckets</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">preferred_batch_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">decode_buckets_num</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">decode_buckets</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]],</span>
        <span class="n">max_position_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">Bucket</span><span class="p">]:</span>
        <span class="k">if</span> <span class="n">decode_buckets</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">Bucket</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">attn_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">attn_size</span> <span class="ow">in</span> <span class="n">decode_buckets</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Generate the buckets automatically</span>
            <span class="n">percentage</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="n">decode_buckets_num</span>
            <span class="n">interval</span> <span class="o">=</span> <span class="n">max_position_embeddings</span> <span class="o">*</span> <span class="n">percentage</span>
            <span class="n">attn_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">interval</span> <span class="o">*</span> <span class="n">i</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">decode_buckets_num</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)]</span>
            <span class="k">return</span> <span class="p">[</span><span class="n">Bucket</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">preferred_batch_size</span><span class="p">,</span> <span class="n">attn_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">attn_size</span> <span class="ow">in</span> <span class="n">attn_sizes</span><span class="p">]</span>

    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">__verify_buckets</span><span class="p">(</span>
        <span class="bp">cls</span><span class="p">,</span>
        <span class="n">prefills</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Bucket</span><span class="p">],</span>
        <span class="n">decodes</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Bucket</span><span class="p">],</span>
        <span class="n">kv_cache_beam_sharing</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">KvCacheSharingAcrossBeamsConfig</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">kv_cache_beam_sharing</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">bucket</span> <span class="ow">in</span> <span class="n">decodes</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">%</span> <span class="n">kv_cache_beam_sharing</span><span class="o">.</span><span class="n">beam_width</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;decode batch size must be a multiple of beam width, but got </span><span class="si">{</span><span class="n">bucket</span><span class="o">.</span><span class="n">batch_size</span><span class="si">}</span><span class="s2"> % </span><span class="si">{</span><span class="n">kv_cache_beam_sharing</span><span class="o">.</span><span class="n">beam_width</span><span class="si">}</span><span class="s2"> != 0&quot;</span>
                    <span class="p">)</span>
                <span class="k">if</span> <span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span> <span class="o">&lt;=</span> <span class="n">kv_cache_beam_sharing</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;decode bucket&#39;s attention size must be greater than max_new_tokens, but got </span><span class="si">{</span><span class="n">bucket</span><span class="o">.</span><span class="n">attention_size</span><span class="si">}</span><span class="s2"> &lt; </span><span class="si">{</span><span class="n">kv_cache_beam_sharing</span><span class="o">.</span><span class="n">max_new_tokens</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__verify_comp_supertask_kind</span><span class="p">(</span><span class="n">kind</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">kind</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;fx&quot;</span><span class="p">,</span> <span class="s2">&quot;dfg&quot;</span><span class="p">,</span> <span class="s2">&quot;edf&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown comp_supertask_kind: </span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">. Must be either &#39;fx&#39;, &#39;dfg&#39;, or &#39;edf&#39;.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__verify_tokenizer_mode</span><span class="p">(</span><span class="n">tokenizer_mode</span><span class="p">:</span> <span class="n">TokenizerModeType</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">tokenizer_mode_lowered</span> <span class="o">=</span> <span class="n">tokenizer_mode</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">tokenizer_mode_lowered</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">get_args</span><span class="p">(</span><span class="n">TokenizerModeType</span><span class="p">):</span>
            <span class="n">valid_options</span> <span class="o">=</span> <span class="s2">&quot;,&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">get_args</span><span class="p">(</span><span class="n">TokenizerModeType</span><span class="p">))</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Unknown tokenizer mode: </span><span class="si">{</span><span class="n">tokenizer_mode</span><span class="si">}</span><span class="s2">. Must be one of &#39;</span><span class="si">{</span><span class="n">valid_options</span><span class="si">}</span><span class="s2">&#39;.&quot;</span>
            <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__verify_devices</span><span class="p">(</span><span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">devices</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No devices are given&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="n">dev</span><span class="o">.</span><span class="n">kind</span> <span class="o">==</span> <span class="n">devices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">kind</span> <span class="k">for</span> <span class="n">dev</span> <span class="ow">in</span> <span class="n">devices</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;All devices must be the same kind.&quot;</span><span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__is_generative_model</span><span class="p">(</span><span class="n">model_type</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">PreTrainedModel</span><span class="p">]])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Check if the model is a generative model.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">model_type</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">model_type</span> <span class="ow">in</span> <span class="n">_HF_CAUSAL_LM_CLASS_NAMES</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">model_type</span><span class="o">.</span><span class="vm">__name__</span> <span class="ow">in</span> <span class="n">_HF_CAUSAL_LM_CLASS_NAMES</span>

    <span class="k">def</span> <span class="nf">_get_default_opt_config_from_pretrained_id</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pretrained_id</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">OptimizationConfig</span><span class="p">:</span>
        <span class="n">model_cls</span> <span class="o">=</span> <span class="n">get_model_cls_from_pretrained_id</span><span class="p">(</span><span class="n">pretrained_id</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">model_cls</span> <span class="ow">is</span> <span class="n">transformers</span><span class="o">.</span><span class="n">GPTJForCausalLM</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">OptimizationConfig</span><span class="p">(</span>
                <span class="n">attention_type</span><span class="o">=</span><span class="n">AttentionType</span><span class="o">.</span><span class="n">PAGED_ATTENTION</span><span class="p">,</span>
                <span class="n">optimize_rope</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">optimize_packed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">causal_mask_free_decoding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">model_cls</span> <span class="ow">is</span> <span class="n">transformers</span><span class="o">.</span><span class="n">BertForQuestionAnswering</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">OptimizationConfig</span><span class="p">(</span>
                <span class="n">use_unsplit_packed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">use_rngd_gelu</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">elif</span> <span class="n">model_cls</span> <span class="ow">is</span> <span class="n">transformers</span><span class="o">.</span><span class="n">LlamaForCausalLM</span><span class="p">:</span>
            <span class="c1"># Llama MLPerf slice model</span>
            <span class="k">return</span> <span class="n">OptimizationConfig</span><span class="p">(</span>
                <span class="n">attention_type</span><span class="o">=</span><span class="n">AttentionType</span><span class="o">.</span><span class="n">PAGED_ATTENTION</span><span class="p">,</span>
                <span class="n">optimize_rope</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">optimize_packed</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">causal_mask_free_decoding</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                <span class="n">calculate_logit_only_for_last_token</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Unsupported model architecture: </span><span class="si">{</span><span class="n">model_cls</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">build_all_pipelines</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">model</span><span class="p">:</span> <span class="n">ModelCreationInfo</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">],</span>
        <span class="n">backend</span><span class="p">:</span> <span class="n">LLMBackend</span><span class="p">,</span>
        <span class="n">comp_supertask_kind</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">use_random_weight</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">qformat_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">qparam_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">quant_ckpt_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">one_supertask_per_device</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">use_blockwise_compile</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">do_decompositions_for_model_rewrite</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">kv_cache_dtype</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">dtype</span><span class="p">],</span>
        <span class="n">sparse_select_version</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">num_pipeline_builder_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_compile_workers</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">embed_all_constants_into_graph</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">num_blocks_per_supertask</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
        <span class="n">num_blocks_per_pp_stage</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">[</span><span class="nb">int</span><span class="p">]],</span>
        <span class="n">add_prefill_last_block_slice</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="n">param_file_path</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">param_saved_format</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">compiler_config_context</span><span class="p">:</span> <span class="n">CompilerConfigContext</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">cleanup</span><span class="p">:</span> <span class="nb">bool</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="c1"># If backend using Pipeline is used, create directory for temporary files.</span>
        <span class="n">tmp_dir_path</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_parallelism_supported</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">cleanup</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tmp_dir</span> <span class="o">=</span> <span class="n">tempfile</span><span class="o">.</span><span class="n">TemporaryDirectory</span><span class="p">()</span>
                <span class="n">tmp_dir_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tmp_dir</span><span class="o">.</span><span class="n">name</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">tmp_dir_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">tempfile</span><span class="o">.</span><span class="n">mkdtemp</span><span class="p">())</span>

        <span class="c1"># Save model parameters when param file path is not given</span>
        <span class="c1"># and pipeline should be constructed.</span>
        <span class="k">if</span> <span class="n">param_file_path</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">backend</span><span class="o">.</span><span class="n">is_parallelism_supported</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">cache_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">model</span><span class="o">.</span><span class="n">is_hashable</span><span class="p">():</span>
                <span class="n">param_file_cache_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">)</span> <span class="o">/</span> <span class="s2">&quot;param_files&quot;</span>
                <span class="n">param_file_path</span> <span class="o">=</span> <span class="n">get_param_file_with_cache</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">param_file_cache_dir</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tmp_dir_path</span><span class="p">,</span> <span class="n">Path</span><span class="p">)</span>
                <span class="n">param_file_path</span> <span class="o">=</span> <span class="n">tmp_dir_path</span> <span class="o">/</span> <span class="n">_PARAM_FILE_NAME</span>
                <span class="n">instantiate_and_save_model</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="p">,</span>
                    <span class="n">use_random_weight</span><span class="p">,</span>
                    <span class="n">qformat_path</span><span class="p">,</span>
                    <span class="n">qparam_path</span><span class="p">,</span>
                    <span class="n">param_file_path</span><span class="p">,</span>
                    <span class="n">quant_ckpt_file_path</span><span class="o">=</span><span class="n">quant_ckpt_file_path</span><span class="p">,</span>
                <span class="p">)</span>

        <span class="n">cache_dir</span> <span class="o">=</span> <span class="kc">None</span> <span class="k">if</span> <span class="n">cache_dir</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">Path</span><span class="p">(</span><span class="n">cache_dir</span><span class="p">)</span>

        <span class="c1"># For now, `PipelineParallelismMppp` supports all valid cases because only pipeline parallelism is needed to be expressed within one pipeline.</span>
        <span class="k">if</span> <span class="n">num_blocks_per_pp_stage</span> <span class="ow">and</span> <span class="s2">&quot;mppp&quot;</span> <span class="ow">in</span> <span class="n">kwargs</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;`num_blocks_per_pp_stage` and custom `mppp` is given at the same time.`num_blocks_per_pp_stage` is ignored.&quot;</span>
            <span class="p">)</span>
        <span class="n">mppp</span> <span class="o">=</span> <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;mppp&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">or</span> <span class="n">PipelineParallelismMppp</span><span class="p">(</span><span class="n">num_blocks_per_pp_stage</span><span class="p">)</span>

        <span class="c1"># Build Pipelines for first dp subgroup.</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span> <span class="o">=</span> <span class="n">build_pipelines</span><span class="p">(</span>
            <span class="n">model</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">buckets</span><span class="p">,</span>
            <span class="n">devices</span><span class="p">,</span>
            <span class="n">param_file_path</span><span class="p">,</span>
            <span class="n">cache_dir</span><span class="p">,</span>
            <span class="n">backend</span><span class="p">,</span>
            <span class="n">mppp</span><span class="p">,</span>
            <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">from_str</span><span class="p">(</span><span class="n">comp_supertask_kind</span><span class="p">),</span>
            <span class="n">one_supertask_per_device</span><span class="p">,</span>
            <span class="n">use_blockwise_compile</span><span class="p">,</span>
            <span class="n">do_decompositions_for_model_rewrite</span><span class="p">,</span>
            <span class="n">kv_cache_dtype</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">paged_attention_config</span><span class="p">,</span>
            <span class="n">sparse_select_version</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="p">,</span>
            <span class="n">tmp_dir_path</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="p">,</span>
            <span class="n">compiler_config_context</span><span class="p">,</span>
            <span class="n">num_pipeline_builder_workers</span><span class="p">,</span>
            <span class="n">num_compile_workers</span><span class="p">,</span>
            <span class="n">embed_all_constants_into_graph</span><span class="p">,</span>
            <span class="n">num_blocks_per_supertask</span><span class="p">,</span>
            <span class="n">add_prefill_last_block_slice</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_generative_model</span><span class="p">,</span>
            <span class="n">param_saved_format</span><span class="p">,</span>
            <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No pipeline is generated&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_save_engine_artifacts</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
        <span class="n">comp_supertask_kind</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">devices</span><span class="p">:</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">],</span>
    <span class="p">):</span>
        <span class="kn">import</span> <span class="nn">shutil</span>

        <span class="n">path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="n">path</span><span class="o">.</span><span class="n">mkdir</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">):</span>
            <span class="n">blobs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">param_files</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">param_files</span><span class="p">)</span>

            <span class="n">blob_kind</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">get_blob_kind</span><span class="p">()</span>
            <span class="k">for</span> <span class="nb">id</span><span class="p">,</span> <span class="n">blob</span> <span class="ow">in</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">blobs</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="n">blob</span>
                <span class="n">kind</span> <span class="o">=</span> <span class="n">blob_kind</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="nb">id</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">kind</span> <span class="o">==</span> <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">FX</span><span class="p">:</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.fx&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                        <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">blob</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">kind</span> <span class="o">==</span> <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">EDF</span><span class="p">:</span>
                    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="nb">id</span><span class="si">}</span><span class="s2">.edf&quot;</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
                        <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">blob</span><span class="o">.</span><span class="n">serialize</span><span class="p">())</span>  <span class="c1"># type: ignore[attr-defined]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;SuperTask [</span><span class="si">{</span><span class="n">kind</span><span class="si">}</span><span class="s2">] is not supported to save&quot;</span><span class="p">)</span>
                <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span><span class="p">[</span><span class="nb">id</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>  <span class="c1"># type: ignore[assignment]</span>

            <span class="k">for</span> <span class="n">param_idx</span><span class="p">,</span> <span class="n">param_file</span> <span class="ow">in</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">param_files</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">filename</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">basename</span><span class="p">(</span><span class="n">param_file</span><span class="o">.</span><span class="n">path</span><span class="p">)</span>
                <span class="n">new_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">new_path</span><span class="o">.</span><span class="n">exists</span><span class="p">():</span>
                    <span class="n">shutil</span><span class="o">.</span><span class="n">copyfile</span><span class="p">(</span><span class="n">param_file</span><span class="o">.</span><span class="n">path</span><span class="p">,</span> <span class="n">new_path</span><span class="p">)</span>
                <span class="n">pipeline</span><span class="o">.</span><span class="n">param_files</span><span class="p">[</span><span class="n">param_idx</span><span class="p">]</span><span class="o">.</span><span class="n">path</span> <span class="o">=</span> <span class="n">filename</span>

            <span class="n">pipeline</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/pipeline.</span><span class="si">{</span><span class="n">idx</span><span class="si">}</span><span class="s2">.json&quot;</span><span class="p">)</span>
            <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span> <span class="o">=</span> <span class="n">blobs</span>
            <span class="n">pipeline</span><span class="o">.</span><span class="n">param_files</span> <span class="o">=</span> <span class="n">param_files</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/generator_config.json&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">scheduler_config</span><span class="o">.</span><span class="n">export</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/scheduler_config.json&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="o">.</span><span class="n">to_json_file</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/hf_config.json&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

        <span class="n">model_metadata_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/model_metadata.json&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_metadata_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_metadata</span><span class="o">.</span><span class="n">model_dump_json</span><span class="p">())</span>

        <span class="n">model_rewriting_config_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/model_rewriting_config.json&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_rewriting_config_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_rewriting_config</span><span class="o">.</span><span class="n">model_dump_json</span><span class="p">())</span>

        <span class="n">parallel_config_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/parallel_config.json&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">parallel_config_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">parallel_config</span><span class="o">.</span><span class="n">model_dump_json</span><span class="p">())</span>

        <span class="n">other_config_path</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/other_config.json&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">other_config_path</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fp</span><span class="p">:</span>
            <span class="n">fp</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                <span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span>
                    <span class="p">{</span>
                        <span class="s2">&quot;comp_supertask_kind&quot;</span><span class="p">:</span> <span class="n">comp_supertask_kind</span><span class="p">,</span>
                        <span class="s2">&quot;devices&quot;</span><span class="p">:</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">devices</span><span class="p">),</span>
                    <span class="p">}</span>
                <span class="p">)</span>
            <span class="p">)</span>

        <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">/ready&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">_verify_token_len_and_finalize_max_tokens</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span> <span class="nb">input</span><span class="p">:</span> <span class="n">BatchEncoding</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        If sampling_params.max_tokens is None, set it to the maximum allowed value.</span>
<span class="sd">        If not, verify that max_tokens + len(prompt) is not larger than the maximum allowed value.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">input_ids</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">input_ids</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;input_ids must be a list of integers.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
            <span class="n">max_input_seq_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ids</span><span class="p">)</span> <span class="k">for</span> <span class="n">ids</span> <span class="ow">in</span> <span class="n">input_ids</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_input_seq_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">max_input_seq_len</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">prompt_max_seq_len</span><span class="p">:</span>
            <span class="c1"># Similar format as OpenAI</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;This model&#39;s maximum input context length is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">prompt_max_seq_len</span><span class="si">}</span><span class="s2"> tokens, however you requested </span><span class="si">{</span><span class="n">max_input_seq_len</span><span class="si">}</span><span class="s2"> tokens. Please reduce your prompt.&quot;</span>
            <span class="p">)</span>

        <span class="k">assert</span> <span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_to_capture</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;Generative models must have max_seq_len_to_capture set.&quot;</span>
        <span class="k">if</span> <span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_to_capture</span> <span class="o">-</span> <span class="n">max_input_seq_len</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span> <span class="o">+</span> <span class="n">max_input_seq_len</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_to_capture</span><span class="p">:</span>
                <span class="c1"># Same error message as OpenAI&#39;s response</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;This model&#39;s maximum context length is </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">max_seq_len_to_capture</span><span class="si">}</span><span class="s2"> tokens, however you requested </span><span class="si">{</span><span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">max_input_seq_len</span><span class="si">}</span><span class="s2"> tokens (</span><span class="si">{</span><span class="n">max_input_seq_len</span><span class="si">}</span><span class="s2"> in your prompt; </span><span class="si">{</span><span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span><span class="si">}</span><span class="s2"> for the completion). Please reduce your prompt; or completion length.&quot;</span>
                <span class="p">)</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__verify_sampling_params_with_generator_config</span><span class="p">(</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span><span class="p">,</span>
        <span class="n">generator_config</span><span class="p">:</span> <span class="n">GeneratorConfig</span><span class="p">,</span>
    <span class="p">):</span>
        <span class="k">if</span> <span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`sampling_params.max_tokens` must be specified at this point.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">sampling_params</span><span class="o">.</span><span class="n">use_beam_search</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`sampling_params.use_beam_search` is not consistent with generator config. The model was configured to use beam search, but `sampling_params.use_beam_search` is False.&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">sampling_params</span><span class="o">.</span><span class="n">max_tokens</span>
                <span class="o">&gt;</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="o">.</span><span class="n">max_new_tokens</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`sampling_params.max_tokens` is larger than `generator_config.kv_cache_sharing_across_beams_config.max_new_tokens`&quot;</span>
                <span class="p">)</span>
            <span class="k">if</span> <span class="p">(</span>
                <span class="n">sampling_params</span><span class="o">.</span><span class="n">best_of</span>
                <span class="o">!=</span> <span class="n">generator_config</span><span class="o">.</span><span class="n">kv_cache_sharing_across_beams_config</span><span class="o">.</span><span class="n">beam_width</span>
            <span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;`sampling_params.best_of` is different from beam width specified in `generator_config.kv_cache_sharing_across_beams_config.beam_width`.&quot;</span>
                <span class="p">)</span>

<div class="viewcode-block" id="LLM.generate">
<a class="viewcode-back" href="../../furiosa_llm/references/llm.html#furiosa_llm.LLM.generate">[docs]</a>
    <span class="k">def</span> <span class="nf">generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(),</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BatchEncoding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate texts from given prompts and sampling parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompts: The prompts to generate texts.</span>
<span class="sd">            sampling_params: The sampling parameters for generating texts.</span>
<span class="sd">            prompt_token_ids: Pre-tokenized prompt input as a `BatchEncoding` object.</span>
<span class="sd">                If not provided, the prompt will be tokenized internally using the tokenizer.</span>
<span class="sd">            tokenizer_kwargs: Additional keyword arguments passed to the tokenizer&#39;s</span>
<span class="sd">                `encode` method, such as `{&quot;use_special_tokens&quot;: True}`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A list of `RequestOutput` objects containing the generated</span>
<span class="sd">            completions in the same order as the input prompts.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_generative_model</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;generate API can only be used for generative models.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tokenizer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">encode_auto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="o">**</span><span class="n">tokenizer_kwargs</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_token_len_and_finalize_max_tokens</span><span class="p">(</span><span class="n">sampling_params</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">)</span>
        <span class="n">LLM</span><span class="o">.</span><span class="n">__verify_sampling_params_with_generator_config</span><span class="p">(</span><span class="n">sampling_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="p">)</span>
        <span class="n">native_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_postprocess</span><span class="p">(</span><span class="n">native_outputs</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">)</span></div>


<div class="viewcode-block" id="LLM.stream_generate">
<a class="viewcode-back" href="../../furiosa_llm/references/llm.html#furiosa_llm.LLM.stream_generate">[docs]</a>
    <span class="k">async</span> <span class="k">def</span> <span class="nf">stream_generate</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
        <span class="n">sampling_params</span><span class="p">:</span> <span class="n">SamplingParams</span> <span class="o">=</span> <span class="n">SamplingParams</span><span class="p">(),</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BatchEncoding</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">tokenizer_kwargs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
        <span class="n">is_demo</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">AsyncGenerator</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Generate texts from given prompt and sampling parameters.</span>

<span class="sd">        Args:</span>
<span class="sd">            prompt: The prompt to generate texts. Note that unlike `generate`,</span>
<span class="sd">                this API supports only a single prompt.</span>
<span class="sd">            sampling_params: The sampling parameters for generating texts.</span>
<span class="sd">            prompt_token_ids: Pre-tokenized prompt input as a `BatchEncoding` object.</span>
<span class="sd">                If not provided, the prompt will be tokenized internally using the tokenizer.</span>
<span class="sd">            tokenizer_kwargs: Additional keyword arguments passed to the tokenizer&#39;s</span>
<span class="sd">                `encode` method, such as `{&quot;use_special_tokens&quot;: True}`.</span>

<span class="sd">        Returns:</span>
<span class="sd">            A stream of generated output tokens.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_generative_model</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;generate API can only be used for generative models.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;prompt must be a single string.&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">prompt_token_ids</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">tokenizer_kwargs</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="n">tokenizer_kwargs</span> <span class="o">=</span> <span class="p">{}</span>
            <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">encode_auto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompt</span><span class="p">,</span> <span class="o">**</span><span class="n">tokenizer_kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_verify_token_len_and_finalize_max_tokens</span><span class="p">(</span><span class="n">sampling_params</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">)</span>
        <span class="n">LLM</span><span class="o">.</span><span class="n">__verify_sampling_params_with_generator_config</span><span class="p">(</span><span class="n">sampling_params</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">generator_config</span><span class="p">)</span>

        <span class="c1"># FIXME: LLM.__init__() should take max_tokens to determine the maximum sequence length through bucket generations</span>
        <span class="c1"># and use the config value to raise an error.</span>
        <span class="k">if</span> <span class="n">is_demo</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="o">.</span><span class="n">input_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1024</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The length of the prompt is larger than 1024 tokens&quot;</span><span class="p">)</span>

        <span class="c1"># NOTE: type of engine.stream_generate() is AsyncGenerator[RequestOutput, None]</span>
        <span class="n">token_buffer</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">request_output</span><span class="p">:</span> <span class="n">RequestOutput</span>
        <span class="k">async</span> <span class="k">for</span> <span class="n">request_output</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">stream_generate</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">,</span> <span class="n">sampling_params</span><span class="p">):</span>
            <span class="n">num_decode_trials</span> <span class="o">=</span> <span class="n">STREAMING_MAX_DECODE_TRIAL</span>
            <span class="k">for</span> <span class="n">completion_output</span> <span class="ow">in</span> <span class="n">request_output</span><span class="o">.</span><span class="n">outputs</span><span class="p">:</span>
                <span class="n">token_buffer</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">completion_output</span><span class="o">.</span><span class="n">token_ids</span><span class="p">)</span>
                <span class="n">num_decode_trials</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">num_decode_trials</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">completion_output</span><span class="o">.</span><span class="n">token_ids</span><span class="p">))</span>

            <span class="k">if</span> <span class="n">num_decode_trials</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">for</span> <span class="n">tokens_to_discard</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_decode_trials</span><span class="p">):</span>
                <span class="n">end_offset</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">token_buffer</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">tokens_to_discard</span>
                <span class="n">new_text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
                    <span class="n">token_buffer</span><span class="p">[:</span> <span class="n">end_offset</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span>
                <span class="p">)</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="n">new_text</span><span class="o">.</span><span class="n">endswith</span><span class="p">(</span><span class="s2">&quot;�&quot;</span><span class="p">):</span>
                    <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">token_buffer</span> <span class="o">=</span> <span class="n">token_buffer</span><span class="p">[</span><span class="n">end_offset</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
            <span class="k">yield</span> <span class="n">new_text</span>

        <span class="k">if</span> <span class="n">token_buffer</span><span class="p">:</span>
            <span class="k">yield</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_buffer</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></div>


    <span class="k">def</span> <span class="nf">_generate_postprocess</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">native_outputs</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">prompt_token_ids</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]]:</span>
        <span class="n">skip_special_tokens</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>

        <span class="c1"># Convert one prompt and multiple generated sequences into a RequestOutput</span>
        <span class="k">def</span> <span class="nf">convert</span><span class="p">(</span><span class="n">prompt</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">_prompt_token_ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">request_output</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">[</span>
                <span class="n">CompletionOutput</span><span class="p">(</span>
                    <span class="n">o</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span>
                        <span class="n">o</span><span class="o">.</span><span class="n">token_ids</span><span class="p">,</span> <span class="n">skip_special_tokens</span><span class="p">,</span> <span class="n">clean_up_tokenization_spaces</span><span class="o">=</span><span class="kc">True</span>
                    <span class="p">),</span>
                    <span class="n">o</span><span class="o">.</span><span class="n">token_ids</span><span class="p">,</span>
                    <span class="n">o</span><span class="o">.</span><span class="n">finish_reason</span><span class="p">,</span>
                <span class="p">)</span>
                <span class="k">for</span> <span class="n">o</span> <span class="ow">in</span> <span class="n">request_output</span><span class="o">.</span><span class="n">outputs</span>
            <span class="p">]</span>

            <span class="k">return</span> <span class="n">RequestOutput</span><span class="p">(</span>
                <span class="n">request_id</span><span class="o">=</span><span class="n">uuid</span><span class="o">.</span><span class="n">uuid4</span><span class="p">()</span><span class="o">.</span><span class="fm">__str__</span><span class="p">(),</span>
                <span class="n">prompt</span><span class="o">=</span><span class="n">prompt</span><span class="p">,</span>
                <span class="n">prompt_token_ids</span><span class="o">=</span><span class="n">_prompt_token_ids</span><span class="p">,</span>
                <span class="n">outputs</span><span class="o">=</span><span class="n">outputs</span><span class="p">,</span>
                <span class="n">finished</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">native_outputs</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="nb">list</span><span class="p">)</span>
            <span class="k">return</span> <span class="p">[</span>
                <span class="n">convert</span><span class="p">(</span><span class="n">req</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">req</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">req</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">req</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">native_outputs</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="nb">str</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">convert</span><span class="p">(</span><span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="o">.</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">native_outputs</span><span class="p">)</span>  <span class="c1"># type: ignore</span>

    <span class="k">def</span> <span class="nf">bert_forward</span><span class="p">(</span>
        <span class="bp">self</span><span class="p">,</span>
        <span class="n">prompts</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
        <span class="n">contexts</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]],</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">RequestOutput</span><span class="p">]]:</span>
        <span class="n">prompt_token_ids</span> <span class="o">=</span> <span class="n">encode_auto</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">text_pair</span><span class="o">=</span><span class="n">contexts</span><span class="p">)</span>
        <span class="n">native_outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">engine</span><span class="o">.</span><span class="n">bert_forward</span><span class="p">(</span><span class="n">prompt_token_ids</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_generate_postprocess</span><span class="p">(</span><span class="n">native_outputs</span><span class="p">,</span> <span class="n">prompts</span><span class="p">,</span> <span class="n">prompt_token_ids</span><span class="p">)</span>

    <span class="k">def</span> <span class="fm">__del__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># Remove tmp directory if exists.</span>
        <span class="n">tmp_dir</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;tmp_dir&quot;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">tmp_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">tmp_dir</span><span class="o">.</span><span class="n">cleanup</span><span class="p">()</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__get_gms_for_pipeline</span><span class="p">(</span>
        <span class="n">pipeline</span><span class="p">:</span> <span class="n">Pipeline</span><span class="p">,</span>
        <span class="n">get_input_constants</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span>
        <span class="n">Tuple</span><span class="p">[</span><span class="n">GraphModule</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">...</span><span class="p">]],</span> <span class="o">...</span><span class="p">]</span>
    <span class="p">]:</span>
        <span class="n">ret</span><span class="p">:</span> <span class="n">List</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">gm_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">DataBlobId</span><span class="p">],</span> <span class="n">GraphModule</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="c1"># Sort supertasks by id to guarantee consistent order.</span>
        <span class="n">sorted_supertasks</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">supertask</span>
            <span class="k">for</span> <span class="n">_</span><span class="p">,</span> <span class="n">supertask</span> <span class="ow">in</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">pipeline</span><span class="o">.</span><span class="n">supertasks</span><span class="o">.</span><span class="n">items</span><span class="p">(),</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="nb">int</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">supertask</span> <span class="ow">in</span> <span class="n">sorted_supertasks</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">supertask</span><span class="p">,</span> <span class="n">CompSuperTask</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="n">supertask</span><span class="o">.</span><span class="n">kind</span> <span class="o">!=</span> <span class="n">SuperTaskKind</span><span class="o">.</span><span class="n">FX</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Supertask is not FX graph supertask.&quot;</span><span class="p">)</span>

            <span class="n">param_load_cache</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="p">{}</span>

            <span class="n">fake_mode</span> <span class="o">=</span> <span class="n">FakeTensorMode</span><span class="p">(</span><span class="n">allow_non_fake_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="k">with</span> <span class="n">fake_mode</span><span class="p">:</span>
                <span class="n">fake_example_inputs</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span>
                        <span class="n">pipeline</span><span class="o">.</span><span class="n">tensors</span><span class="p">[</span><span class="n">input_</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                        <span class="n">dtype</span><span class="o">=</span><span class="n">pipeline</span><span class="o">.</span><span class="n">tensors</span><span class="p">[</span><span class="n">input_</span><span class="p">]</span><span class="o">.</span><span class="n">dtype</span><span class="o">.</span><span class="n">to_torch_dtype</span><span class="p">(),</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">input_</span> <span class="ow">in</span> <span class="n">supertask</span><span class="o">.</span><span class="n">inputs</span>
                <span class="p">)</span>

            <span class="n">gm</span> <span class="o">=</span> <span class="n">gm_cache</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">supertask</span><span class="o">.</span><span class="n">data_blob</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">gm</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">supertask</span><span class="o">.</span><span class="n">data</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="n">supertask</span><span class="o">.</span><span class="n">data</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">assert</span> <span class="n">supertask</span><span class="o">.</span><span class="n">data_blob</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
                    <span class="n">data</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">blobs</span><span class="p">[</span><span class="n">supertask</span><span class="o">.</span><span class="n">data_blob</span><span class="p">]</span>

                <span class="n">gm</span> <span class="o">=</span> <span class="n">deserialize_gm</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
                <span class="c1"># NOTE: This Shape propagation is required because tensor meta infomration is lost during serialization. We need to regenerate this.</span>
                <span class="n">ShapeProp</span><span class="p">(</span><span class="n">gm</span><span class="p">)</span><span class="o">.</span><span class="n">propagate</span><span class="p">(</span><span class="o">*</span><span class="n">fake_example_inputs</span><span class="p">)</span>
                <span class="c1"># preprocess gms for it to be compiled immediately</span>
                <span class="n">gm</span> <span class="o">=</span> <span class="n">preprocess</span><span class="p">(</span><span class="n">gm</span><span class="p">,</span> <span class="n">fake_example_inputs</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">supertask</span><span class="o">.</span><span class="n">data_blob</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="n">gm_cache</span><span class="p">[</span><span class="n">supertask</span><span class="o">.</span><span class="n">data_blob</span><span class="p">]</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">gm</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">get_input_constants</span><span class="p">:</span>
                <span class="c1"># TODO: change this to share same tensor among slices.</span>
                <span class="k">def</span> <span class="nf">load_tensor</span><span class="p">(</span><span class="n">tensor_name</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
                    <span class="n">tensor_info</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">tensors</span><span class="p">[</span><span class="n">tensor_name</span><span class="p">]</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_info</span><span class="p">,</span> <span class="n">TensorInfo</span><span class="p">):</span>
                        <span class="c1"># If it&#39;s not an input constant tensor (i.e., input tensor not originated from constant tensor),</span>
                        <span class="c1"># just return None.</span>
                        <span class="k">return</span> <span class="kc">None</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="k">assert</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">tensor_info</span><span class="p">,</span> <span class="n">ParamInfo</span><span class="p">)</span>
                        <span class="n">param_value</span> <span class="o">=</span> <span class="n">tensor_info</span><span class="o">.</span><span class="n">value</span>
                        <span class="n">param_file_info</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">param_files</span><span class="p">[</span><span class="n">param_value</span><span class="o">.</span><span class="n">param_file</span><span class="p">]</span>

                        <span class="k">return</span> <span class="n">load_partial_param</span><span class="p">(</span>
                            <span class="n">param_file_info</span><span class="o">.</span><span class="n">path</span><span class="p">,</span>
                            <span class="n">param_value</span><span class="o">.</span><span class="n">name</span><span class="p">,</span>
                            <span class="n">param_value</span><span class="o">.</span><span class="n">placements</span><span class="p">,</span>
                            <span class="n">param_file_info</span><span class="o">.</span><span class="n">format</span><span class="p">,</span>
                            <span class="n">cache</span><span class="o">=</span><span class="n">param_load_cache</span><span class="p">,</span>
                        <span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

                <span class="n">example_input</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">load_tensor</span><span class="p">(</span><span class="n">input_name</span><span class="p">)</span> <span class="k">for</span> <span class="n">input_name</span> <span class="ow">in</span> <span class="n">supertask</span><span class="o">.</span><span class="n">inputs</span><span class="p">)</span>
                <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">((</span><span class="n">gm</span><span class="p">,</span> <span class="n">example_input</span><span class="p">))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">ret</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">gm</span><span class="p">)</span>

        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">ret</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">_get_splitted_gms</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">get_input_constants</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span>
        <span class="nb">str</span><span class="p">,</span>
        <span class="n">Union</span><span class="p">[</span>
            <span class="n">Tuple</span><span class="p">[</span><span class="n">GraphModule</span><span class="p">,</span> <span class="o">...</span><span class="p">],</span>
            <span class="n">Tuple</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">GraphModule</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="o">...</span><span class="p">]],</span> <span class="o">...</span><span class="p">],</span>
        <span class="p">],</span>
    <span class="p">]:</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Get sub GraphModules for each pipeline.</span>

<span class="sd">        Returns:</span>
<span class="sd">            Dict[str, Union[Tuple[GraphModule, ...], Tuple[Tuple[GraphModule, Tuple[Optional[torch.Tensor], ...]], ...],],]:</span>
<span class="sd">                Dictionary whose key is the pipeline name and value is the tuple containing ``GraphModule``s (computation supertasks) and some additional information if necessary.</span>
<span class="sd">                if ``get_input_constants==False``, each value is just a tuple of ``GraphModule``s in the pipeline.</span>
<span class="sd">                Otherwise, each value is a tuple whose element is ``GraphModule`` in the pipeline  and list of input constant tensors,</span>
<span class="sd">                which were originally constant tensors, but converted to input. The list of input constant tensors has same length as corresponding ``GraphModule``&#39;s number of inputs</span>
<span class="sd">                with each element exactly corresponding to the input of the ``GraphModule`` with same index, but elements with original input tensor indexes are ``None``.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">Pipeline</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;get_splitted_gms is only supported for parallel backends&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="p">{</span>
            <span class="n">pipeline</span><span class="o">.</span><span class="n">name</span><span class="p">:</span> <span class="n">LLM</span><span class="o">.</span><span class="n">__get_gms_for_pipeline</span><span class="p">(</span>
                <span class="n">pipeline</span><span class="p">,</span> <span class="n">get_input_constants</span><span class="o">=</span><span class="n">get_input_constants</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">pipeline</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">pipelines</span>
        <span class="p">}</span>

    <span class="nd">@staticmethod</span>
    <span class="k">def</span> <span class="nf">__verify_kv_cache_dtype_with_qformat</span><span class="p">(</span>
        <span class="n">kv_cache_dtype</span><span class="p">:</span> <span class="n">QDtype</span><span class="p">,</span> <span class="n">qformat_path</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">,</span> <span class="n">model_metadata</span><span class="p">:</span> <span class="n">ModelMetadata</span>
    <span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">kv_cache_dtype_from_qformat</span> <span class="o">=</span> <span class="n">get_kv_cache_dtype_from_qformat</span><span class="p">(</span><span class="n">qformat_path</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">kv_cache_dtype</span> <span class="o">!=</span> <span class="n">kv_cache_dtype_from_qformat</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;kv_cache_dtype != qformat&#39;s kv_cache dtype: </span><span class="si">{</span><span class="n">kv_cache_dtype</span><span class="si">}</span><span class="s2"> != </span><span class="si">{</span><span class="n">kv_cache_dtype_from_qformat</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="nd">@cached_property</span>
    <span class="k">def</span> <span class="nf">model_max_seq_len</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
        <span class="n">possible_keys</span> <span class="o">=</span> <span class="p">[</span>
            <span class="c1"># OPT, LLaMA, BERT</span>
            <span class="s2">&quot;max_position_embeddings&quot;</span><span class="p">,</span>
            <span class="c1"># GPT-2, GPT-J</span>
            <span class="s2">&quot;n_positions&quot;</span><span class="p">,</span>
            <span class="c1"># MPT</span>
            <span class="s2">&quot;max_seq_len&quot;</span><span class="p">,</span>
            <span class="c1"># ChatGLM2</span>
            <span class="s2">&quot;seq_length&quot;</span><span class="p">,</span>
            <span class="c1"># Command-R</span>
            <span class="s2">&quot;model_max_length&quot;</span><span class="p">,</span>
            <span class="c1"># Others</span>
            <span class="s2">&quot;max_sequence_length&quot;</span><span class="p">,</span>
            <span class="s2">&quot;max_seq_length&quot;</span><span class="p">,</span>
            <span class="s2">&quot;seq_len&quot;</span><span class="p">,</span>
        <span class="p">]</span>

        <span class="k">for</span> <span class="n">attr_name</span> <span class="ow">in</span> <span class="n">possible_keys</span><span class="p">:</span>
            <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">):</span>
                <span class="n">model_max_seq_len</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model_config</span><span class="p">,</span> <span class="n">attr_name</span><span class="p">)</span>
                <span class="k">break</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># If none of the keys were found in the config, use a default and</span>
            <span class="c1"># log a warning.</span>
            <span class="n">default_max_len</span> <span class="o">=</span> <span class="mi">2048</span>
            <span class="n">model_max_seq_len</span> <span class="o">=</span> <span class="n">default_max_len</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
                <span class="s2">&quot;The model&#39;s config.json does not contain any of the following &quot;</span>
                <span class="s2">&quot;keys to determine the original maximum length of the model: &quot;</span>
                <span class="s2">&quot;</span><span class="si">%s</span><span class="s2">. Assuming the model&#39;s maximum length is </span><span class="si">%d</span><span class="s2">.&quot;</span><span class="p">,</span>
                <span class="n">possible_keys</span><span class="p">,</span>
                <span class="n">default_max_len</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="k">return</span> <span class="n">model_max_seq_len</span></div>



<span class="k">def</span> <span class="nf">try_compiler_config_from_yaml</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Compiler config must be given at: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">yaml</span><span class="o">.</span><span class="n">safe_load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">load_artifacts</span><span class="p">(</span>
    <span class="n">path</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span><span class="p">],</span>
    <span class="o">*</span><span class="p">,</span>
    <span class="c1"># Runtime Configuration</span>
    <span class="n">devices</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Sequence</span><span class="p">[</span><span class="n">Device</span><span class="p">]]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">data_parallel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bucket_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">BucketConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">scheduler_config</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SchedulerConfig</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">paged_attention_num_blocks</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="c1"># Other Configuration</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">PreTrainedTokenizer</span><span class="p">,</span> <span class="n">PreTrainedTokenizerFast</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">tokenizer_mode</span><span class="p">:</span> <span class="n">TokenizerModeType</span> <span class="o">=</span> <span class="s2">&quot;auto&quot;</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">cache_dir</span><span class="p">:</span> <span class="n">os</span><span class="o">.</span><span class="n">PathLike</span> <span class="o">=</span> <span class="n">CACHE_DIR</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">LLM</span><span class="p">:</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Instantiate LLM from saved artifacts without quantization and compilation.</span>

<span class="sd">    Internally, this function calls :meth:`LLM.load_artifacts`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;backend&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">LLM</span><span class="o">.</span><span class="n">load_artifacts</span><span class="p">(</span>
        <span class="n">path</span><span class="p">,</span>
        <span class="n">devices</span><span class="o">=</span><span class="n">devices</span><span class="p">,</span>
        <span class="n">data_parallel_size</span><span class="o">=</span><span class="n">data_parallel_size</span><span class="p">,</span>
        <span class="n">bucket_config</span><span class="o">=</span><span class="n">bucket_config</span><span class="p">,</span>
        <span class="n">scheduler_config</span><span class="o">=</span><span class="n">scheduler_config</span><span class="p">,</span>
        <span class="n">paged_attention_num_blocks</span><span class="o">=</span><span class="n">paged_attention_num_blocks</span><span class="p">,</span>
        <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">,</span>
        <span class="n">tokenizer_mode</span><span class="o">=</span><span class="n">tokenizer_mode</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">cache_dir</span><span class="o">=</span><span class="n">cache_dir</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By FuriosaAI, Inc.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2024, FuriosaAI, Inc..
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>