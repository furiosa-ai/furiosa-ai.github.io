
<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta content="IE=edge,chrome=1" http-equiv="X-UA-Compatible">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
    <title>FuriosaAI Renegade SDK Installation Guide and API Reference</title>

    <style media="screen">
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight .gh {
  color: #999999;
}
.highlight .sr {
  color: #f6aa11;
}
.highlight .go {
  color: #888888;
}
.highlight .gp {
  color: #555555;
}
.highlight .gs {
}
.highlight .gu {
  color: #aaaaaa;
}
.highlight .nb {
  color: #f6aa11;
}
.highlight .cm {
  color: #75715e;
}
.highlight .cp {
  color: #75715e;
}
.highlight .c1 {
  color: #75715e;
}
.highlight .cs {
  color: #75715e;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cpf {
  color: #75715e;
}
.highlight .err {
  color: #960050;
}
.highlight .gr {
  color: #960050;
}
.highlight .gt {
  color: #960050;
}
.highlight .gd {
  color: #49483e;
}
.highlight .gi {
  color: #49483e;
}
.highlight .ge {
  color: #49483e;
}
.highlight .kc {
  color: #66d9ef;
}
.highlight .kd {
  color: #66d9ef;
}
.highlight .kr {
  color: #66d9ef;
}
.highlight .no {
  color: #66d9ef;
}
.highlight .kt {
  color: #66d9ef;
}
.highlight .mf {
  color: #ae81ff;
}
.highlight .mh {
  color: #ae81ff;
}
.highlight .il {
  color: #ae81ff;
}
.highlight .mi {
  color: #ae81ff;
}
.highlight .mo {
  color: #ae81ff;
}
.highlight .m, .highlight .mb, .highlight .mx {
  color: #ae81ff;
}
.highlight .sc {
  color: #ae81ff;
}
.highlight .se {
  color: #ae81ff;
}
.highlight .ss {
  color: #ae81ff;
}
.highlight .sd {
  color: #e6db74;
}
.highlight .s2 {
  color: #e6db74;
}
.highlight .sb {
  color: #e6db74;
}
.highlight .sh {
  color: #e6db74;
}
.highlight .si {
  color: #e6db74;
}
.highlight .sx {
  color: #e6db74;
}
.highlight .s1 {
  color: #e6db74;
}
.highlight .s, .highlight .sa, .highlight .dl {
  color: #e6db74;
}
.highlight .na {
  color: #a6e22e;
}
.highlight .nc {
  color: #a6e22e;
}
.highlight .nd {
  color: #a6e22e;
}
.highlight .ne {
  color: #a6e22e;
}
.highlight .nf, .highlight .fm {
  color: #a6e22e;
}
.highlight .vc {
  color: #ffffff;
}
.highlight .nn {
  color: #ffffff;
}
.highlight .ni {
  color: #ffffff;
}
.highlight .bp {
  color: #ffffff;
}
.highlight .vg {
  color: #ffffff;
}
.highlight .vi {
  color: #ffffff;
}
.highlight .nv, .highlight .vm {
  color: #ffffff;
}
.highlight .w {
  color: #ffffff;
}
.highlight {
  color: #ffffff;
}
.highlight .n, .highlight .py, .highlight .nx {
  color: #ffffff;
}
.highlight .nl {
  color: #f92672;
}
.highlight .ow {
  color: #f92672;
}
.highlight .nt {
  color: #f92672;
}
.highlight .k, .highlight .kv {
  color: #f92672;
}
.highlight .kn {
  color: #f92672;
}
.highlight .kp {
  color: #f92672;
}
.highlight .o {
  color: #f92672;
}
    </style>
    <style media="print">
      * {
        -webkit-transition:none!important;
        transition:none!important;
      }
      .highlight table td { padding: 5px; }
.highlight table pre { margin: 0; }
.highlight, .highlight .w {
  color: #586e75;
}
.highlight .err {
  color: #002b36;
  background-color: #dc322f;
}
.highlight .c, .highlight .ch, .highlight .cd, .highlight .cm, .highlight .cpf, .highlight .c1, .highlight .cs {
  color: #657b83;
}
.highlight .cp {
  color: #b58900;
}
.highlight .nt {
  color: #b58900;
}
.highlight .o, .highlight .ow {
  color: #93a1a1;
}
.highlight .p, .highlight .pi {
  color: #93a1a1;
}
.highlight .gi {
  color: #859900;
}
.highlight .gd {
  color: #dc322f;
}
.highlight .gh {
  color: #268bd2;
  background-color: #002b36;
  font-weight: bold;
}
.highlight .k, .highlight .kn, .highlight .kp, .highlight .kr, .highlight .kv {
  color: #6c71c4;
}
.highlight .kc {
  color: #cb4b16;
}
.highlight .kt {
  color: #cb4b16;
}
.highlight .kd {
  color: #cb4b16;
}
.highlight .s, .highlight .sa, .highlight .sb, .highlight .sc, .highlight .dl, .highlight .sd, .highlight .s2, .highlight .sh, .highlight .sx, .highlight .s1 {
  color: #859900;
}
.highlight .sr {
  color: #2aa198;
}
.highlight .si {
  color: #d33682;
}
.highlight .se {
  color: #d33682;
}
.highlight .nn {
  color: #b58900;
}
.highlight .nc {
  color: #b58900;
}
.highlight .no {
  color: #b58900;
}
.highlight .na {
  color: #268bd2;
}
.highlight .m, .highlight .mb, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .il, .highlight .mo, .highlight .mx {
  color: #859900;
}
.highlight .ss {
  color: #859900;
}
    </style>
    <link href="stylesheets/screen-373dae74.css" rel="stylesheet" media="screen" />
    <link href="stylesheets/print-953e3353.css" rel="stylesheet" media="print" />
      <script src="javascripts/all-e9bde216.js"></script>

    <script>
      $(function() { setupCodeCopy(); });
    </script>
  </head>

  <body class="index" data-languages="[]">
    <a href="#" id="nav-button">
      <span>
        NAV
        <img src="images/navbar-cad8cdcb.png" alt="" />
      </span>
    </a>
    <div class="toc-wrapper">
      <img src="images/logo-4b03aa02.svg" class="logo" alt="" />
        <div class="search">
          <input type="text" class="search" id="input-search" placeholder="Search">
        </div>
        <ul class="search-results"></ul>
      <ul id="toc" class="toc-list-h1">
          <li>
            <a href="#introduction" class="toc-h1 toc-link" data-title="Introduction">Introduction</a>
          </li>
          <li>
            <a href="#prerequisites" class="toc-h1 toc-link" data-title="Prerequisites">Prerequisites</a>
          </li>
          <li>
            <a href="#installing-furiosaai-sdk" class="toc-h1 toc-link" data-title="Installing FuriosaAI SDK">Installing FuriosaAI SDK</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#fpga-installation" class="toc-h2 toc-link" data-title="FPGA Installation">FPGA Installation</a>
                  </li>
                  <li>
                    <a href="#jupyter-notebook-examples" class="toc-h2 toc-link" data-title="Jupyter Notebook Examples">Jupyter Notebook Examples</a>
                  </li>
                  <li>
                    <a href="#furiosaai-cli" class="toc-h2 toc-link" data-title="FuriosaAI CLI">FuriosaAI CLI</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#getting-started" class="toc-h1 toc-link" data-title="Getting Started">Getting Started</a>
          </li>
          <li>
            <a href="#nux-quantizer" class="toc-h1 toc-link" data-title="Nux Quantizer">Nux Quantizer</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#quantization-getting-started" class="toc-h2 toc-link" data-title="Quantization Getting Started">Quantization Getting Started</a>
                  </li>
                  <li>
                    <a href="#exporting-pytorch-model-to-onnx-model-example" class="toc-h2 toc-link" data-title="Exporting Pytorch model to ONNX model example">Exporting Pytorch model to ONNX model example</a>
                  </li>
                  <li>
                    <a href="#nux-quantizer-in-detail" class="toc-h2 toc-link" data-title="Nux Quantizer in detail">Nux Quantizer in detail</a>
                  </li>
                  <li>
                    <a href="#graph-optimization" class="toc-h2 toc-link" data-title="Graph Optimization">Graph Optimization</a>
                  </li>
                  <li>
                    <a href="#calibration" class="toc-h2 toc-link" data-title="Calibration">Calibration</a>
                  </li>
                  <li>
                    <a href="#quantization" class="toc-h2 toc-link" data-title="Quantization">Quantization</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#nux-python-api" class="toc-h1 toc-link" data-title="Nux Python API">Nux Python API</a>
          </li>
          <li>
            <a href="#nux-c-c-api" class="toc-h1 toc-link" data-title="Nux C/C++ API">Nux C/C++ API</a>
              <ul class="toc-list-h2">
                  <li>
                    <a href="#create_nux" class="toc-h2 toc-link" data-title="create_nux()">create_nux()</a>
                  </li>
                  <li>
                    <a href="#destroy_nux" class="toc-h2 toc-link" data-title="destroy_nux()">destroy_nux()</a>
                  </li>
                  <li>
                    <a href="#nux_create_sync_model" class="toc-h2 toc-link" data-title="nux_create_sync_model()">nux_create_sync_model()</a>
                  </li>
                  <li>
                    <a href="#destroy_sync_model" class="toc-h2 toc-link" data-title="destroy_sync_model()">destroy_sync_model()</a>
                  </li>
                  <li>
                    <a href="#model_count_inputs" class="toc-h2 toc-link" data-title="model_count_inputs()">model_count_inputs()</a>
                  </li>
                  <li>
                    <a href="#model_count_outputs" class="toc-h2 toc-link" data-title="model_count_outputs()">model_count_outputs()</a>
                  </li>
                  <li>
                    <a href="#model_input_tensor" class="toc-h2 toc-link" data-title="model_input_tensor()">model_input_tensor()</a>
                  </li>
                  <li>
                    <a href="#model_output_tensor" class="toc-h2 toc-link" data-title="model_output_tensor()">model_output_tensor()</a>
                  </li>
                  <li>
                    <a href="#model_run" class="toc-h2 toc-link" data-title="model_run()">model_run()</a>
                  </li>
                  <li>
                    <a href="#tensor_set_buffer" class="toc-h2 toc-link" data-title="tensor_set_buffer()">tensor_set_buffer()</a>
                  </li>
                  <li>
                    <a href="#tensor_get_buffer" class="toc-h2 toc-link" data-title="tensor_get_buffer()">tensor_get_buffer()</a>
                  </li>
                  <li>
                    <a href="#nux_create_task_model" class="toc-h2 toc-link" data-title="nux_create_task_model()">nux_create_task_model()</a>
                  </li>
                  <li>
                    <a href="#task_model_get_task" class="toc-h2 toc-link" data-title="task_model_get_task()">task_model_get_task()</a>
                  </li>
                  <li>
                    <a href="#task_model_try_get_task" class="toc-h2 toc-link" data-title="task_model_try_get_task()">task_model_try_get_task()</a>
                  </li>
                  <li>
                    <a href="#task_input" class="toc-h2 toc-link" data-title="task_input()">task_input()</a>
                  </li>
                  <li>
                    <a href="#task_input_size" class="toc-h2 toc-link" data-title="task_input_size()">task_input_size()</a>
                  </li>
                  <li>
                    <a href="#task_execute" class="toc-h2 toc-link" data-title="task_execute()">task_execute()</a>
                  </li>
                  <li>
                    <a href="#destroy_task_model" class="toc-h2 toc-link" data-title="destroy_task_model()">destroy_task_model()</a>
                  </li>
                  <li>
                    <a href="#task_model_is_all_task_done" class="toc-h2 toc-link" data-title="task_model_is_all_task_done()">task_model_is_all_task_done()</a>
                  </li>
              </ul>
          </li>
          <li>
            <a href="#supported-operators" class="toc-h1 toc-link" data-title="Supported Operators">Supported Operators</a>
          </li>
      </ul>
        <ul class="toc-footer">
            <li>FuriosaAI SDK 0.1.0</li>
            <li><a href='./index.html'>Korean Version</a></li>
        </ul>
    </div>
    <div class="page-wrapper">
      <div class="dark-box"></div>
      <div class="content">
        <h1 id='introduction'>Introduction</h1>
<p>FuriosaAI NPU (Neural Processing Unit) has APIs available in C/C++ and Python for executing a DNN model.</p>

<p>This document includes FuriosaAI NPU FPGA installation guide, Jupyter Notebook examples, and how to use compiler CLI.</p>

<p>Nux Quantizer which helps users to quantize their models is introduced. </p>
<h1 id='prerequisites'>Prerequisites</h1>
<ul>
<li>Linux (Ubuntu 18.04 LTS or later)</li>
</ul>
<h1 id='installing-furiosaai-sdk'>Installing FuriosaAI SDK</h1><h2 id='fpga-installation'>FPGA Installation</h2>
<p><a href="https://github.com/furiosa-ai/furiosa-fpga-install">FuriosaAI FPGA Install</a></p>
<h2 id='jupyter-notebook-examples'>Jupyter Notebook Examples</h2>
<p><a href="https://github.com/furiosa-ai/nuxpy-examples">FuriosaAI Jupyter Notebook Examples</a></p>

<aside class="info">
FURIOSA_ACCESS_KEY_ID and FURIOSA_SECRET_ACCESS_KEY are required.
</aside>
<h2 id='furiosaai-cli'>FuriosaAI CLI</h2>
<p><a href="https://github.com/furiosa-ai/furiosa-cli">FuriosaAI CLI</a></p>

<aside class="info">
FURIOSA_ACCESS_KEY_ID and FURIOSA_SECRET_ACCESS_KEY are required.
</aside>
<h1 id='getting-started'>Getting Started</h1>
<p><img src="images/python_workflow-f626da73.png" alt="Python API Workflow" />
Renegade Python API workflow is like the above.</p>

<p><img src="images/python_workflow_detail-df5d2b0e.png" alt="Python API Workflow" />
Running models on Renegade NPU consists of three steps:</p>

<ol>
<li>Covert models on TensorFlow or PyTorch into TFLite and ONNX models with quantization 
<aside class="success">
TFLite or ONNX models do not require this step
</aside></li>
<li>Compile a TFLite or ONNX model and deploy the compiled model to virtual NPU</li>
<li>Run the compiled model on physical NPU </li>
</ol>
<h1 id='nux-quantizer'>Nux Quantizer</h1>
<p>Nux-quantizer supports post-training 8-bit quantization. </p>

<p>Nux-quantizer follows <a href="https://www.tensorflow.org/lite/performance/quantization_spec">Tensorflow Lite 8-bit quantization specification</a>.</p>

<p>Nux-quantizer now supports ONNX models, TensorFlow models will be supported later.</p>
<h2 id='quantization-getting-started'>Quantization Getting Started</h2><h3 id='nux-quantizer-post_training_quantization'>nux.quantizer.post_training_quantization()</h3>
<blockquote>
<p>One click quantization </p>
</blockquote>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">nux</span>


<span class="n">model</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">input_tensors</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">calibration_data</span> <span class="o">=</span> <span class="p">...</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">post_training_quantization</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">calibration_data</span><span class="p">)</span>

<span class="k">with</span> <span class="n">nux</span><span class="p">.</span><span class="n">session</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># ...
</span></code></pre></div>
<p>Input ONNX model can be quantized on one-click. This function applies the following quantization schemes: </p>

<ul>
<li>To minimize accuracy drop from quantization, <code>convolution</code> operators are <code>per-channel</code>-wise quantized and the other operators are <code>per-tensor</code>-wise quantized.</li>
<li><a href="https://pytorch.org/tutorials/advanced/static_quantization_tutorial.html#post-training-static-quantization"><code>static</code> quantization</a> which quantizes both weights and activations is used for applying <a href="https://arxiv.org/abs/1712.05877">integer-only-arithmetic</a> which enables faster inference than floating-point calculation.</li>
</ul>
<h3 id='parameters'>Parameters</h3>
<table><thead>
<tr>
<th>Name</th>
<th>Explanation</th>
</tr>
</thead><tbody>
<tr>
<td>model</td>
<td><code>onnx.ModelProto</code> ONNX modle (DNN model in ONNX format)</td>
</tr>
<tr>
<td>input_tensors</td>
<td><code>List[str]</code> input tensors of ONNX model</td>
</tr>
<tr>
<td>calibration_data</td>
<td><code>Union[Dict[str, np.ndarray], List[Dict[str, np.ndarray]]]</code> calibration data for collecting dynamic range of activations, randomly generated if not given.</td>
</tr>
</tbody></table>
<h3 id='return'>Return</h3>
<p><code>onnx.ModelProto</code> static per-channel int8 quantized model</p>
<h2 id='exporting-pytorch-model-to-onnx-model-example'>Exporting Pytorch model to ONNX model example</h2>
<p>Function <code>nux.quantizer.post_training_quantization</code> requires a ONNX model. Function <code>torch.onnx.export</code> exports Pytorch model into ONNX model.
Example code shows that converting Pytorch model <code>ExampleNet</code> in python script to an ONNX model and quantizing the ONNX model with dynamic range collected with randomly generated calibration data.</p>

<blockquote>
<p>Export Pytorch to ONNX </p>
</blockquote>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">io</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="n">F</span>
<span class="kn">import</span> <span class="nn">onnx</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="kn">import</span> <span class="nn">nux</span>


<span class="c1"># describe Example Pytorch model
</span><span class="k">class</span> <span class="nc">ExampleNet</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExampleNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">12</span><span class="p">)</span>
        <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">bn2</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="p">.</span><span class="n">conv3</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>


<span class="c1"># util function for exporting Pytorch model to ONNX
</span><span class="k">def</span> <span class="nf">export_example_net</span><span class="p">():</span>
    <span class="n">dummy_input</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
    <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"input"</span><span class="p">]</span>
    <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"output"</span><span class="p">]</span>
    <span class="n">model_io</span> <span class="o">=</span> <span class="n">io</span><span class="p">.</span><span class="n">BytesIO</span><span class="p">()</span>
    <span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="n">export</span><span class="p">(</span><span class="n">ExampleNet</span><span class="p">(),</span>
                      <span class="p">(</span><span class="n">dummy_input</span><span class="p">,),</span>
                      <span class="n">model_io</span><span class="p">,</span>
                      <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                      <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span>
                      <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">)</span>
    <span class="n">model_io</span><span class="p">.</span><span class="n">seek</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">onnx</span><span class="p">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">model_io</span><span class="p">)</span>

<span class="c1"># util function to get input tensor name
</span><span class="k">def</span> <span class="nf">example_net_input_tensors</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span><span class="s">"input"</span><span class="p">]</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">export_example_net</span><span class="p">()</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">post_training_quantization</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">example_net_input_tensors</span><span class="p">())</span>

<span class="k">with</span> <span class="n">nux</span><span class="p">.</span><span class="n">session</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># ...
</span></code></pre></div>
<p>ONNX model <code>ExampleNet</code> is visualized by <a href="https://github.com/lutzroeder/netron">Netron</a> as follows:</p>

<p><img src="images/nux-quantizer/simple_model-e3ffbcb9.png" alt="simple_model" /></p>
<h2 id='nux-quantizer-in-detail'>Nux Quantizer in detail</h2>
<blockquote>
<p>Nux Quantizer in detail </p>
</blockquote>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">nux</span>


<span class="n">IS_TEST</span> <span class="o">=</span> <span class="p">...</span>

<span class="n">input_tensors</span> <span class="o">=</span> <span class="n">example_net_input_tensors</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">export_example_net</span><span class="p">()</span>

<span class="c1"># graph optimization
</span><span class="n">optimized_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="c1"># calibration
</span><span class="n">calibration_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">build_calibration_model</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">)</span>
<span class="k">if</span> <span class="n">IS_TSET</span><span class="p">:</span>
    <span class="n">dynamic_ranges</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">calibrate_with_random_input</span><span class="p">(</span><span class="n">calibration_model</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">calibration_data</span> <span class="o">=</span> <span class="n">get_calibration_data</span><span class="p">(...)</span>
    <span class="n">dynamic_ranges</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">calibration_model</span><span class="p">,</span> <span class="n">calibration_data</span><span class="p">)</span>

<span class="c1"># quantization
</span><span class="n">quantized_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">dynamic_ranges</span><span class="p">)</span>

<span class="k">with</span> <span class="n">nux</span><span class="p">.</span><span class="n">session</span><span class="p">.</span><span class="n">create</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">)</span> <span class="k">as</span> <span class="n">sess</span><span class="p">:</span>
    <span class="c1"># ...
</span></code></pre></div>
<p>The workflow of nux-quantizer is like the following: 
An input ONNX model is processed by three steps; 1) graph optimization 2) calibration 3) quantization.
The output of these steps is quantized ONNX model.  </p>

<p><img src="images/nux-quantizer/nux-quantizer_quantization_pipepline-edd29681.png" alt="nux-quantizer pipeline" /></p>
<h2 id='graph-optimization'>Graph Optimization</h2>
<blockquote>
<p>Graph Optimization</p>
</blockquote>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">nux</span> 


<span class="n">model</span> <span class="o">=</span> <span class="p">...</span>

<span class="n">optimized_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</code></pre></div><h3 id='nux-quantizer-optimize'>nux.quantizer.optimize()</h3>
<p>This function optimizes ONNX model in graph level. 
The input ONNX model is optimized by graph optimization techniques such as shape inference and operator fusion. </p>
<h3 id='parameters-2'>Parameters</h3>
<table><thead>
<tr>
<th>Name</th>
<th>Explanation</th>
</tr>
</thead><tbody>
<tr>
<td>model</td>
<td><code>onnx.ModelProto</code> ONNX model</td>
</tr>
</tbody></table>
<h3 id='return-2'>Return</h3>
<p><code>onnx.ModelProto</code> Optimized ONNX model</p>
<h3 id='result'>Result</h3>
<p>The <code>optimized ExampleNet</code> is visualized like the following:</p>

<p><img src="images/nux-quantizer/optimized_simple_model-6cd0bd9a.png" alt="optimized_model" /></p>
<h2 id='calibration'>Calibration</h2><h3 id='nux-quantizer-build_calibration_model'>nux.quantizer.build_calibration_model()</h3>
<blockquote>
<p>Calibration</p>
</blockquote>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">nux</span> 


<span class="n">IS_TEST</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">optimized_model</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">input_tensors</span> <span class="o">=</span> <span class="p">...</span>

<span class="n">calibration_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">build_calibration_model</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">)</span>

<span class="k">if</span> <span class="ow">not</span> <span class="n">IS_TSET</span><span class="p">:</span>
    <span class="n">calibration_data</span> <span class="o">=</span> <span class="n">get_calibration_data</span><span class="p">(...)</span> <span class="c1"># user-written function
</span>    <span class="n">dynamic_ranges</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">calibration_model</span><span class="p">,</span> <span class="n">calibration_data</span><span class="p">)</span>    
<span class="k">else</span><span class="p">:</span>
    <span class="n">dynamic_ranges</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">calibrate_with_random_input</span><span class="p">(</span><span class="n">calibration_model</span><span class="p">)</span>    
</code></pre></div>
<p>This function adds <code>ReduceMin</code> and <code>ReduceMax</code> nodes to every output node. </p>
<h3 id='parameters-3'>Parameters</h3>
<table><thead>
<tr>
<th>Name</th>
<th>Explanation</th>
</tr>
</thead><tbody>
<tr>
<td>model</td>
<td><code>onnx.ModelProto</code> ONNX model</td>
</tr>
<tr>
<td>input_tensors</td>
<td><code>List[str]</code> Input tensors of ONNX model</td>
</tr>
</tbody></table>
<h3 id='return-3'>Return</h3>
<p><code>onnx.ModelProto</code> Calibration model</p>
<h3 id='result-2'>Result</h3>
<p>The <code>calibration ExampleNet</code> is visualized as follows:</p>

<p><img src="images/nux-quantizer/augmented_simple_model_for_calibration-d6a46c1a.png" alt="calibration_model" /></p>
<h3 id='nux-quantizer-calibrate'>nux.quantizer.calibrate()</h3>
<p>This function executes a calibration model and then collects the dynamic range of activations with preprocessed calibration data.  </p>
<h3 id='parameters-4'>Parameters</h3>
<table><thead>
<tr>
<th>Name</th>
<th>Explanation</th>
</tr>
</thead><tbody>
<tr>
<td>model</td>
<td><code>onnx.ModelProto</code> ONNX model</td>
</tr>
<tr>
<td>calibration_data</td>
<td><code>Union[Dict[str, np.ndarray], List[Dict[str, np.ndarray]]]</code> Preprocessed calibration data</td>
</tr>
</tbody></table>
<h3 id='return-4'>Return</h3>
<p><code>Dict[str, Tuple[float, float]]</code> Dynamic range of activations </p>
<h3 id='result-3'>Result</h3>
<p>The dynamic range of <code>calibratio ExampleNet</code> is collected by calibration as follows:</p>
<div class="highlight"><pre class="highlight plaintext"><code>{'input': (1.2729454283544328e-05, 0.9999984502792358), '18': (-0.811184287071228, 1.0816885232925415), '20': (-0.6744376420974731, 0.5646329522132874), '21': (-0.29423975944519043, 0.5646329522132874), 'output': (-0.24178576469421387, 0.20031100511550903)}
</code></pre></div><h3 id='nux-quantizer-calibrate_with_random_input'>nux.quantizer.calibrate_with_random_input()</h3>
<p>This function executes a calibration model and then collects the dynamic range of activations with randomly generated calibration data. </p>
<h3 id='parameters-5'>Parameters</h3>
<table><thead>
<tr>
<th>Name</th>
<th>Explanation</th>
</tr>
</thead><tbody>
<tr>
<td>model</td>
<td><code>onnx.ModelProto</code> ONNX model</td>
</tr>
</tbody></table>
<h3 id='return-5'>Return</h3>
<p><code>Dict[str, Tuple[float, float]]</code> Dynamic range of activations</p>
<h3 id='result-4'>Result</h3>
<p>The dynamic range of <code>calibration ExampleNet</code> is collected by random calibration as follows:
```text</p>

<p>{&#39;input&#39;: (2.170155084968428e-06, 0.9999873042106628), &#39;18&#39;: (-1.1680195331573486, 1.125024437904358), &#39;20&#39;: (-0.6748880743980408, 0.6123914122581482), &#39;21&#39;: (-0.44649842381477356, 0.6123914122581482), &#39;output&#39;: (-0.2456839233636856, 0.39172685146331787)}
```</p>
<h2 id='quantization'>Quantization</h2><h3 id='nux-quantizer-quantize'>nux.quantizer.quantize()</h3>
<blockquote>
<p>Quantization </p>
</blockquote>
<div class="highlight"><pre class="highlight python tab-python"><code><span class="kn">import</span> <span class="nn">nux</span>


<span class="n">optimized_model</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">input_tensors</span> <span class="o">=</span> <span class="p">...</span>
<span class="n">dynamic_ranges</span> <span class="o">=</span> <span class="p">...</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">nux</span><span class="p">.</span><span class="n">quantizer</span><span class="p">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">optimized_model</span><span class="p">,</span> <span class="n">input_tensors</span><span class="p">,</span> <span class="n">dynamic_ranges</span><span class="p">)</span>
</code></pre></div>
<p>This function quantizes ONNX model with the dynamic range collected by calibration. </p>
<h3 id='parameters-6'>Parameters</h3>
<table><thead>
<tr>
<th>Name</th>
<th>Explanation</th>
</tr>
</thead><tbody>
<tr>
<td>model</td>
<td><code>onnx.ModelProto</code> ONNX model</td>
</tr>
<tr>
<td>input_tensors</td>
<td><code>List[str]</code> Input tensors of ONNX model</td>
</tr>
<tr>
<td>dynamic_ragnes</td>
<td><code>Dict[str, Tuple[float, float]]</code> Dynamic range of activations</td>
</tr>
</tbody></table>
<h3 id='return-6'>Return</h3>
<p><code>onnx.ModelProto</code> <code>static</code> <code>per-channel</code> 8-bit quantized model</p>
<h3 id='result-5'>Result</h3>
<p>The <code>quantized ExampleNet</code> is visualized as follows: </p>

<p><img src="images/nux-quantizer/quantized_simple_model-174257e1.png" alt="quantized_model" /></p>
<h1 id='nux-python-api'>Nux Python API</h1>
<p><a href="nuxpy/">Python API</a></p>
<h1 id='nux-c-c-api'>Nux C/C++ API</h1><h2 id='create_nux'>create_nux()</h2>
<blockquote>
<p>Create a Nux handle</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_handle_t</span> <span class="n">nux</span><span class="p">;</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">create_nux</span><span class="p">(</span><span class="o">&amp;</span><span class="n">nux</span><span class="p">);</span>
</code></pre></div>
<p>This API creates a nux handle for subsequent activities.</p>
<h3 id='parameters-7'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>nux</td>
<td>Mutable pointer to receive a created Nux handle</td>
</tr>
</tbody></table>
<h3 id='return-7'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_nux_creation_failed</code>.</p>
<h2 id='destroy_nux'>destroy_nux()</h2>
<blockquote>
<p>Destroy a Nux handle</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">destroy_nux</span><span class="p">(</span><span class="n">nux</span><span class="p">);</span>
</code></pre></div>
<p>This API destroys a nux handle.</p>
<h3 id='parameters-8'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>nux</td>
<td>Nux handle to be destroyed. It must not be NULL.</td>
</tr>
</tbody></table>
<h2 id='nux_create_sync_model'>nux_create_sync_model()</h2>
<blockquote>
<p>Create a synchronous model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_sync_model_t</span> <span class="n">sync_model</span><span class="p">;</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">nux_create_sync_model</span><span class="p">(</span><span class="n">nux</span><span class="p">,</span>
                                       <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">buffer</span><span class="p">,</span>
                                       <span class="n">model_size</span><span class="p">,</span>
                                       <span class="o">&amp;</span><span class="n">sync_model</span><span class="p">);</span>
</code></pre></div>
<p>This creates a synchronous model handle.</p>
<h3 id='parameters-9'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>nux</td>
<td>Mutable pointer to specify Nux handle</td>
</tr>
<tr>
<td>buffer</td>
<td>Byte buffer containing ENF binary (i.e., model to be used in an inference task)</td>
</tr>
<tr>
<td>model_size</td>
<td>Byte length of <code>buffer</code></td>
</tr>
<tr>
<td>sync_model</td>
<td>Mutable pointer to receive the handle of a created synchronous model</td>
</tr>
</tbody></table>

<p>The corresponding Python API does not require the above parameters. Instead,
it directly takes an <code>enf</code> file path.</p>
<h3 id='return-8'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_nux_creation_failed</code>.</p>
<h2 id='destroy_sync_model'>destroy_sync_model()</h2>
<blockquote>
<p>Destroy a synchronous model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">destroy_sync_model</span><span class="p">(</span><span class="n">sync_model</span><span class="p">);</span>
</code></pre></div>
<p>This destroys a synchronous model handle, which is not going to be used any more.</p>
<h3 id='parameters-10'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>sync_model</td>
<td>Synchronous model to be destroyed, and it must not be NULL.</td>
</tr>
</tbody></table>
<h2 id='model_count_inputs'>model_count_inputs()</h2>
<blockquote>
<p>Get the number of input tensors</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_sync_model_t</span> <span class="n">sync_model</span><span class="p">;</span>
<span class="p">...</span>
<span class="kt">int</span> <span class="n">nInputs</span> <span class="o">=</span> <span class="n">model_count_inputs</span><span class="p">(</span><span class="n">sync_model</span><span class="p">);</span>
</code></pre></div>
<p>Return the number of input tensors of a given sync model.</p>
<h3 id='parameters-11'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>sync_model</td>
<td>Synchronous model handle.</td>
</tr>
</tbody></table>
<h3 id='return-9'>Return</h3>
<p>The number of input tensors for the given model.</p>
<h2 id='model_count_outputs'>model_count_outputs()</h2>
<blockquote>
<p>Get the number of output tensors</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_sync_model_t</span> <span class="n">sync_model</span><span class="p">;</span>
<span class="p">...</span>
<span class="kt">int</span> <span class="n">nOutputs</span> <span class="o">=</span> <span class="n">model_count_outputs</span><span class="p">(</span><span class="n">sync_model</span><span class="p">);</span>
</code></pre></div>
<p>Return the number of output tensors of a given sync model.</p>
<h3 id='parameters-12'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>sync_model</td>
<td>Synchronous model handle.</td>
</tr>
</tbody></table>
<h3 id='return-10'>Return</h3>
<p>The number of output tensors for the given model.</p>
<h2 id='model_input_tensor'>model_input_tensor()</h2>
<blockquote>
<p>Get an input tensor handle for a synchronous model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_sync_model_t</span> <span class="n">sync_model</span><span class="p">;</span>
<span class="n">nux_tensor_t</span> <span class="n">tensor</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">index</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">model_input_tensor</span><span class="p">(</span><span class="n">sync_model</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">tensor</span><span class="p">);</span>
</code></pre></div>
<p>Get the handle of a specified input tensor from a given sync model.</p>

<aside class="info">
An input tensor handle will be valid
until destroy_sync_model() with this sync_model is called.
</aside>
<h3 id='parameters-13'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>sync_model</td>
<td>Synchronous model handle.</td>
</tr>
<tr>
<td>index</td>
<td>Input tensor index.</td>
</tr>
<tr>
<td>tensor[out]</td>
<td>Mutable pointer to receive the handle of a specified input tensor.</td>
</tr>
</tbody></table>
<h3 id='return-11'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_nux_creation_failed</code>.</p>
<h2 id='model_output_tensor'>model_output_tensor()</h2>
<blockquote>
<p>Get an output tensor handle for a synchronous model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_sync_model_t</span> <span class="n">sync_model</span><span class="p">;</span>
<span class="n">nux_tensor_t</span> <span class="n">tensor</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">index</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">model_output_tensor</span><span class="p">(</span><span class="n">sync_model</span><span class="p">,</span> <span class="n">index</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">tensor</span><span class="p">);</span>
</code></pre></div>
<p>Get the handle of a specified output tensor from a given sync model.</p>

<aside class="info">
An output tensor handle will be valid
until destroy_sync_model() with this sync_model is called.
</aside>
<h3 id='parameters-14'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>sync_model</td>
<td>Synchronous model handle.</td>
</tr>
<tr>
<td>index</td>
<td>Output tensor index.</td>
</tr>
<tr>
<td>tensor[out]</td>
<td>Mutable pointer to receive the handle of a specified output tensor.</td>
</tr>
</tbody></table>
<h3 id='return-12'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_nux_creation_failed</code>.</p>
<h2 id='model_run'>model_run()</h2>
<blockquote>
<p>Execute a synchronous model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_sync_model_t</span> <span class="n">sync_model</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">model_run</span><span class="p">(</span><span class="n">sync_model</span><span class="p">);</span>
</code></pre></div>
<p>Run a single inference task</p>

<p>Before calling this function, you must fill input tensors with proper data.
Please refer to <code>model_input_tensor</code> and <code>tensor_set_buffer</code>
to learn how to fill input tensors with data.</p>
<h3 id='parameters-15'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>sync_model</td>
<td>Synchronous model handle.</td>
</tr>
</tbody></table>
<h3 id='return-13'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_nux_creation_failed</code>.</p>
<h2 id='tensor_set_buffer'>tensor_set_buffer()</h2>
<blockquote>
<p>Copy data to an input tensor</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_tensor_t</span> <span class="n">inputTensor</span><span class="p">;</span>
<span class="kt">void</span><span class="o">*</span> <span class="n">buffer</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">buf_size</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">tensor_set_buffer</span><span class="p">(</span><span class="n">inputTensor</span><span class="p">,</span> <span class="n">buffer</span><span class="p">,</span> <span class="n">buf_size</span><span class="p">);</span>
</code></pre></div>
<p>Copy data into the data buffer of a specified input tensor.</p>

<p>To execute <code>model_run</code>, you first need to fill input tensors with data.
This function copies the data into the data buffer of a specified input tensor.</p>
<h3 id='parameters-16'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>inputTensor</td>
<td>Tensor handle into which you want to copy input data.</td>
</tr>
<tr>
<td>buffer</td>
<td>Pointer to the data buffer.</td>
</tr>
<tr>
<td>buf_size</td>
<td>Byte length of <code>buffer</code></td>
</tr>
</tbody></table>
<h3 id='return-14'>Return</h3>
<p><code>nux_error_t_success</code> if successful,
or <code>nux_error_t_invalid_buffer</code> if <code>buffer</code> is invalid.</p>
<h2 id='tensor_get_buffer'>tensor_get_buffer()</h2>
<blockquote>
<p>Copy data from an output tensor</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_tensor_t</span> <span class="n">outputTensor</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_buffer_t</span> <span class="n">buffer</span><span class="p">;</span>
<span class="n">nux_buffer_len_t</span> <span class="n">buf_size</span><span class="p">;</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">tensor_get_buffer</span><span class="p">(</span><span class="n">outputTensor</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">buffer</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">buf_size</span><span class="p">);</span>
</code></pre></div>
<p>Get a pointer to the data buffer of a given tensor.</p>

<p>Once <code>model_run</code> is called, the inference result will be written into output tensors.
This function returns a pointer to the data buffer of a specified output tensor.</p>

<aside class="info">
the buffers of output tensors are valid until destroy_sync_model is called.
</aside>
<h3 id='parameters-17'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>outputTensor</td>
<td>Tensor from which you want to get a pointer of the data buffer</td>
</tr>
<tr>
<td>buffer</td>
<td>Mutable pointer to receive the pointer to the data buffer</td>
</tr>
<tr>
<td>buf_size[out]</td>
<td>Byte length of <code>buffer</code></td>
</tr>
</tbody></table>
<h3 id='return-15'>Return</h3>
<p><code>nux_error_t_success</code> if successful.</p>
<h2 id='nux_create_task_model'>nux_create_task_model()</h2>
<blockquote>
<p>Create a task model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="kt">void</span> <span class="o">*</span><span class="n">buffer</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">max_batch</span><span class="p">;</span>
<span class="kt">int</span> <span class="n">model_size</span><span class="p">;</span>
<span class="n">nux_handle_t</span> <span class="n">nux</span><span class="p">;</span>
<span class="n">nux_task_model_t</span> <span class="n">task_model</span><span class="p">;</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">nux_create_task_model</span><span class="p">(</span><span class="n">nux</span><span class="p">,</span>
                                 <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">char</span><span class="o">*</span><span class="p">)</span><span class="n">buffer</span><span class="p">,</span>
                                 <span class="n">model_size</span><span class="p">,</span>
                                 <span class="n">max_batch</span><span class="p">,</span>
                                 <span class="n">output_callback</span><span class="p">,</span>
                                 <span class="n">error_callback</span><span class="p">,</span>
                                 <span class="n">finish_callback</span><span class="p">,</span>
                                 <span class="o">&amp;</span><span class="n">task_model</span><span class="p">);</span>
</code></pre></div>
<blockquote>
<p>The signatures of the above callback functions should be:</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="kt">void</span> <span class="nf">output_cb</span><span class="p">(</span><span class="n">nux_request_id_t</span> <span class="n">id</span><span class="p">,</span>
               <span class="n">nux_output_index_t</span> <span class="n">out_id</span><span class="p">,</span>
               <span class="n">nux_buffer_t</span> <span class="n">buf</span><span class="p">,</span>
               <span class="n">nux_buffer_len_t</span> <span class="n">buf_len</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// fill your logic</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="nf">my_error_cb</span><span class="p">(</span><span class="n">nux_request_id_t</span> <span class="n">id</span><span class="p">,</span> <span class="n">nux_error_t</span> <span class="n">err</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// fill your logic</span>
<span class="p">}</span>
<span class="kt">void</span> <span class="nf">my_finish_cb</span><span class="p">(</span><span class="n">nux_request_id_t</span> <span class="n">id</span><span class="p">)</span> <span class="p">{</span>
    <span class="c1">// fill your logic</span>
<span class="p">}</span>
</code></pre></div>
<p>Create an instance of a task model.</p>

<p>This function allows users to run multiple inference tasks asynchronously and simultaneously.
When each task is completed or failed, corresponding callback functions will be called
with <code>nux_request_id_t</code>, an identifier of a task request.
See also <code>task_execute()</code> for more details.</p>
<h3 id='parameters-18'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>nux</td>
<td>Nux handle.</td>
</tr>
<tr>
<td>buffer</td>
<td>Byte buffer containing ENF binary.</td>
</tr>
<tr>
<td>model_size</td>
<td>Byte length of <code>buffer</code></td>
</tr>
<tr>
<td>max_batch</td>
<td>Number of concurrent running tasks. This can be limited according to internal configurations and HW capacity.</td>
</tr>
<tr>
<td>output_callback</td>
<td>Callback function invoked when a task is completed. It will be called per output tensor.</td>
</tr>
<tr>
<td>error_callback</td>
<td>Callback function invoked when a task is failed.</td>
</tr>
<tr>
<td>finish_callback</td>
<td>Callback function which will be called finally after the output_callback is called after all output tensors.</td>
</tr>
<tr>
<td>task_model</td>
<td>Mutable pointer to receive the handle of a created task model.</td>
</tr>
</tbody></table>
<h3 id='return-16'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_nux_creation_failed</code>.</p>
<h2 id='task_model_get_task'>task_model_get_task()</h2>
<blockquote>
<p>Get a task from a task model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_task_model_t</span> <span class="n">task_model</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_task_t</span> <span class="n">task</span><span class="p">;</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">task_model_get_task</span><span class="p">(</span><span class="n">task_model</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">task</span><span class="p">);</span>
</code></pre></div>
<p>Retrieve a task handle from a specified task model.</p>

<p>When there is no available task in a given task model,
it will block until new task is available.</p>
<h3 id='parameters-19'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task_model</td>
<td>Handle of a task model.</td>
</tr>
<tr>
<td>task</td>
<td>Mutable pointer to receive the handle of a created task.</td>
</tr>
</tbody></table>
<h3 id='return-17'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_model_execution_failed</code>.</p>
<h2 id='task_model_try_get_task'>task_model_try_get_task()</h2>
<blockquote>
<p>Get a task from a task model (Non-blocking)</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_task_model_t</span> <span class="n">task_model</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_task_t</span> <span class="n">task</span><span class="p">;</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">task_model_try_get_task</span><span class="p">(</span><span class="n">task_model</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">task</span><span class="p">);</span>
</code></pre></div>
<p>Get a task handle from the specified task model without blocking operations.</p>

<p>It&#39;s the non-blocking version of <code>task_model_get_task</code>.</p>
<h3 id='parameters-20'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task_model</td>
<td>Handle of a task model.</td>
</tr>
<tr>
<td>task</td>
<td>Mutable pointer to receive the handle of a created task.</td>
</tr>
</tbody></table>
<h3 id='return-18'>Return</h3>
<p>This function returns an available task, or
it will return immediately <code>nux_error_t_get_task_failed</code> if there&#39;s no available task in a given task model.</p>
<h2 id='task_input'>task_input()</h2>
<blockquote>
<p>Get a task input tensor handle</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_task_t</span> <span class="n">task</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_buffer_t</span> <span class="n">buffer</span> <span class="o">=</span> <span class="n">task_input</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">index</span><span class="p">);</span>
</code></pre></div>
<p>Return a mutable pointer to the buffer of the specified input tensor.</p>
<h3 id='parameters-21'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task</td>
<td>Task handle</td>
</tr>
<tr>
<td>index</td>
<td>Index of the input tensor.</td>
</tr>
</tbody></table>
<h3 id='return-19'>Return</h3>
<p>A mutable pointer to the data buffer of the given input tensor.</p>
<h2 id='task_input_size'>task_input_size()</h2>
<blockquote>
<p>Get the size of an input tensor</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_task_t</span> <span class="n">task</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_buffer_len_t</span> <span class="n">length</span> <span class="o">=</span> <span class="n">task_input_size</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="mi">0</span><span class="p">);</span>
</code></pre></div>
<p>Return the buffer length in bytes of the specified input tensor.</p>
<h3 id='parameters-22'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task</td>
<td>Task handle</td>
</tr>
<tr>
<td>index</td>
<td>Index of the input tensor.</td>
</tr>
</tbody></table>
<h3 id='return-20'>Return</h3>
<p>The size of the specified input tensor.</p>
<h2 id='task_execute'>task_execute()</h2>
<blockquote>
<p>Run a task model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_task_t</span> <span class="n">task</span><span class="p">;</span>
<span class="n">nux_request_id_t</span> <span class="n">request_id</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">nux_error_t</span> <span class="n">err</span> <span class="o">=</span> <span class="n">task_execute</span><span class="p">(</span><span class="n">task</span><span class="p">,</span> <span class="n">request_id</span><span class="p">);</span>
</code></pre></div>
<p>Request one asynchronous inference task.</p>

<p>This function submits a request for an inference task identified by <code>task</code>.
Once a task is completed, <code>output_callback</code> function passed to <code>nux_create_task_model</code>
will be called with a distinct output index per output tensor.
<code>finish_callback</code> function will be also called
after <code>output_callback</code> function is called for all output tensors.</p>

<aside class="warning">
Once you call task_execute with a task,
the task will get destroyed automatically. Please do not call destroy_task with
the task which is already passed to task_execute.
</aside>
<h3 id='parameters-23'>Parameters</h3>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task</td>
<td>Task handle obtained from calling <code>task_model_get_task</code> or <code>task_model_try_get_task</code>.</td>
</tr>
<tr>
<td>request_id</td>
<td>request_id An positive integer to distinguish task requests. The behavior of <code>task_execute</code> doesn&#39;t rely on a <code>request_id</code> value at all. <code>request_id</code> will be just passed to callback functions.</td>
</tr>
</tbody></table>
<h3 id='return-21'>Return</h3>
<p><code>nux_error_t_success</code> if successful, or
it will return <code>nux_error_t_model_execution_failed</code>.</p>
<h2 id='destroy_task_model'>destroy_task_model()</h2>
<blockquote>
<p>Destroy a task model</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">destroy_task_model</span><span class="p">(</span><span class="n">task_model</span><span class="p">);</span>
</code></pre></div>
<p>Destroy the task model and release its resources.</p>

<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task_model</td>
<td>Task model to be destroyed.</td>
</tr>
</tbody></table>
<h2 id='task_model_is_all_task_done'>task_model_is_all_task_done()</h2>
<blockquote>
<p>Check whether all tasks are done</p>
</blockquote>
<div class="highlight"><pre class="highlight c tab-c"><code><span class="cp">#include "nux.h"
</span>
<span class="n">nux_task_model_t</span> <span class="n">task_model</span><span class="p">;</span>
<span class="p">...</span>
<span class="n">bool</span> <span class="n">ck</span> <span class="o">=</span> <span class="n">task_model_is_all_task_done</span><span class="p">(</span><span class="n">task_model</span><span class="p">);</span>
</code></pre></div>
<table><thead>
<tr>
<th>Parameter</th>
<th>Description</th>
</tr>
</thead><tbody>
<tr>
<td>task_model</td>
<td>Task model handle.</td>
</tr>
</tbody></table>

<p>je</p>
<h3 id='return-22'>Return</h3>
<p><code>true</code> if there&#39;s no running tasks, or <code>false</code> if any task is still running.</p>
<h1 id='supported-operators'>Supported Operators</h1>
<ul>
<li>Add</li>
<li>AveragePool2d</li>
<li>Broadcast</li>
<li>Clip</li>
<li>Concatenation</li>
<li>Conv2d</li>
<li>DepthToSpace</li>
<li>DepthwiseConv2d</li>
<li>Exp</li>
<li>Expand</li>
<li>Flatten</li>
<li>FullyConnected</li>
<li>Gemm</li>
<li>LpNorm</li>
<li>Mask</li>
<li>MatMul</li>
<li>MaxPool2d</li>
<li>Mean</li>
<li>Mul</li>
<li>Pad</li>
<li>Pad</li>
<li>ReduceL2</li>
<li>ReduceSum</li>
<li>Relu</li>
<li>Requantize</li>
<li>Reshape</li>
<li>Resize</li>
<li>Sigmoid</li>
<li>Slice</li>
<li>Softmax</li>
<li>Softplus</li>
<li>Split</li>
<li>TableLookup</li>
<li>Transpose</li>
<li>TransposeConv</li>
<li>Unsqueeze</li>
</ul>

      </div>
      <div class="dark-box">
      </div>
    </div>
  </body>
</html>
