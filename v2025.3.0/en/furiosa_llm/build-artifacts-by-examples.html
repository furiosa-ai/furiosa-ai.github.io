
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HTTHGM3MD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-0HTTHGM3MD');
    </script>
    
    <title>Building Model Artifacts By Examples &#8212; FuriosaAI Developer Center 2025.3.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=37f7f57c" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=c5ef0929"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'furiosa_llm/build-artifacts-by-examples';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = 'https://raw.githubusercontent.com/furiosa-ai/furiosa-ai.github.io/refs/heads/main/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = '2025.3.0';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            true;
        </script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Parallelism" href="model-parallelism.html" />
    <link rel="prev" title="Model Preparation" href="model-preparation.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="2025.3.0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/doc-logo-dark.svg" class="logo__image only-light" alt=""/>
    <img src="../_static/doc-logo-light.svg" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">
            <div class='sidebar-title mr-auto'>
                Furiosa Docs
            </div>
        </p>
  
</a></div>
        <div class="sidebar-primary-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/rngd.html">FuriosaAI RNGD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/software_stack.html">FuriosaAI’s Software Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/supported_models.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whatsnew/index.html">What’s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/roadmap.html">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Get Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../get_started/prerequisites.html">Installing Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/furiosa_llm.html">Quick Start with Furiosa-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../get_started/upgrade_guide.html">Upgrading FuriosaAI’s Software</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Furiosa-LLM</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Furiosa-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="furiosa-llm-serve.html">OpenAI-Compatible Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-preparation.html">Model Preparation</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Building Model Artifacts By Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-parallelism.html">Model Parallelism</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="reference.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="reference/llm.html">LLM class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/sampling_params.html">SamplingParams class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/artifact_builder.html">ArtifactBuilder</a></li>


<li class="toctree-l2"><a class="reference internal" href="reference/llm_engine.html">LLMEngine class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/async_llm_engine.html">AsyncLLMEngine class</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="examples.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/llm_chat.html">Chat</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/llm_chat_with_tools.html">Chat with tools</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="k8s_deployment.html">Deploying Furiosa-LLM on Kubernetes</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cloud Native Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cloud_native_toolkit/intro.html">Cloud Native Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_native_toolkit/container.html">Container Support</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cloud_native_toolkit/kubernetes.html">Kubernetes Plugins</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/feature_discovery.html">Installing Furiosa Feature Discovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/device_plugin.html">Installing Furiosa Device Plugin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/metrics_exporter.html">Installing Furiosa Metrics Exporter</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../device_management/system_management_interface.html">Furiosa SMI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_cli.html">Furiosa SMI CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_lib.html">Furiosa SMI Library</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://forums.furiosa.ai">Forums</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">Customer Support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://furiosa.ai">FuriosaAI Homepage</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/latest/en/">Furiosa Gen 1 NPU SDK Doc</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item"><img id='furiosa_logo' width="100" /></div>
      <div class="sidebar-primary-item">

  <p class="copyright">
    
      © Copyright 2025 FuriosaAI Inc.
      <br/>
    
  </p>
</div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button></div>
      
        <div class="header-article-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="header-article-item"><button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Building Model Artifacts By Examples</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2>  </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line-tool-vs-artifactbuilder-api">Command-Line Tool vs. ArtifactBuilder API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-examples">Basic Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-context-length">Specifying Context Length</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-buckets">Specifying Buckets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-parallelism-for-large-models">Using Parallelism for Large Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enabling-trust-remote-code">Enabling <code class="docutils literal notranslate"><span class="pre">trust_remote_code</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-float16-float32-models">Building Float16, Float32 Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#long-context-length-and-chunked-prefill">Long-context Length and Chunked Prefill</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-quantized-models">Building Quantized Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-model-artifacts-in-parallel">Building Model Artifacts in Parallel</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="building-model-artifacts-by-examples">
<span id="buildingmodelartifactsbyexamples"></span><h1>Building Model Artifacts By Examples<a class="headerlink" href="#building-model-artifacts-by-examples" title="Link to this heading">#</a></h1>
<p>This document provides examples of how to build model artifacts using Furiosa-LLM.
If you’re unfamiliar with the concept of model artifacts or the overall workflow for preparing a model for Furiosa-LLM,
please refer to the <a class="reference internal" href="model-preparation.html#modelpreparation"><span class="std std-ref">Model Preparation</span></a> section for an introduction.</p>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<p>Before starting this section, please ensure you have the following prerequisites:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../get_started/prerequisites.html#aptsetup"><span class="std std-ref">Setting up APT</span></a> and <a class="reference internal" href="../get_started/prerequisites.html#installingprerequisites"><span class="std std-ref">Installing Prerequisites</span></a></p></li>
<li><p><a class="reference internal" href="../get_started/furiosa_llm.html#installingfuriosallm"><span class="std std-ref">Furiosa-LLM Installation</span></a></p></li>
<li><p><a class="reference internal" href="../get_started/furiosa_llm.html#authorizinghuggingfacehub"><span class="std std-ref">HuggingFace Access Token</span></a></p></li>
</ul>
</section>
<section id="command-line-tool-vs-artifactbuilder-api">
<h2>Command-Line Tool vs. ArtifactBuilder API<a class="headerlink" href="#command-line-tool-vs-artifactbuilder-api" title="Link to this heading">#</a></h2>
<p>There are two ways to build model artifacts using Furiosa-LLM:</p>
<ul class="simple">
<li><p>The <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API</p></li>
<li><p>The <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span></code> command-line tool</p></li>
</ul>
<p>The <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API is a Python interface that enables you to build model artifacts
with Furiosa-LLM, offering greater programmability and flexibility.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span></code> command is a command-line tool that provides the same functionality
as the <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API. It offers a simpler and more convenient way to build model artifacts
without writing any code. Its syntax is as follows:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>furiosa-llm<span class="w"> </span>build<span class="w"> </span>&lt;MODEL_ID_OR_PATH&gt;<span class="w"> </span>&lt;OUTPUT_PATH&gt;<span class="w"> </span><span class="o">[</span>OPTIONS<span class="o">]</span>
</pre></div>
</div>
<p>You can see more options by running <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span> <span class="pre">--help</span></code>.
Essentially, it is a wrapper around the <code class="docutils literal notranslate"><span class="pre">ArtifactBuilder</span></code> API,
so you can find details about the options in the reference for the <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API.</p>
</section>
<section id="examples">
<h2>Examples<a class="headerlink" href="#examples" title="Link to this heading">#</a></h2>
<p>The examples in this section are based on <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span></code> command and the Llama 3.1 8B model.
You can also implement the same examples using the <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API.</p>
<section id="basic-examples">
<h3>Basic Examples<a class="headerlink" href="#basic-examples" title="Link to this heading">#</a></h3>
<p>Below is a basic example of building a model artifact using the <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span></code> command.
The first argument can be a Hugging Face model ID or a local path.
A local path should start with <code class="docutils literal notranslate"><span class="pre">.</span></code> or <code class="docutils literal notranslate"><span class="pre">/</span></code> and should point to a directory
containing a Hugging Face Transformers model or a quantized model by Furiosa-LLM.
The second argument is the output path where the model artifact will be saved.</p>
<p>The following command doesn’t have any options, so the model artifact will be built with the default options.
You can find the default options by running <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span> <span class="pre">--help</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span>
</pre></div>
</div>
</section>
<section id="specifying-context-length">
<h3>Specifying Context Length<a class="headerlink" href="#specifying-context-length" title="Link to this heading">#</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">--max-seq-len-to-capture</span></code> option specifies the maximum sequence length supported by the Furiosa-LLM engine.
You can adjust this value according to your model and use case, as it affects how the model is optimized.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span> \
    <span class="o">--</span><span class="nb">max</span><span class="o">-</span><span class="n">seq</span><span class="o">-</span><span class="nb">len</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">capture</span> <span class="mi">4096</span>
</pre></div>
</div>
</section>
<section id="specifying-buckets">
<h3>Specifying Buckets<a class="headerlink" href="#specifying-buckets" title="Link to this heading">#</a></h3>
<p>Furiosa-LLM utilizes buckets to optimize models for varying sequence lengths.
The <code class="docutils literal notranslate"><span class="pre">--prefill-buckets</span> <span class="pre">BUCKET_SIZE</span></code> and <code class="docutils literal notranslate"><span class="pre">--decode-buckets</span> <span class="pre">BUCKET_SIZE</span></code>
options specify a comma-separated list of bucket sizes for prefill and decode phases, respectively.
<code class="docutils literal notranslate"><span class="pre">-pb</span></code> and <code class="docutils literal notranslate"><span class="pre">-db</span></code> are the short options for <code class="docutils literal notranslate"><span class="pre">--prefill-buckets</span></code> and <code class="docutils literal notranslate"><span class="pre">--decode-buckets</span></code>, respectively.
The prefill bucket size is used during the input prompt processing phase, while the decode bucket size is used during the text-generation phase.
Each bucket is defined as a tuple of batch size and sequence length, specified as <code class="docutils literal notranslate"><span class="pre">BATCH_SIZE,SEQ_LEN</span></code>.
You can specify multiple bucket sizes for the prefill and decode phases as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span> \
    <span class="o">-</span><span class="n">pb</span> <span class="mi">1</span><span class="p">,</span><span class="mi">512</span> \
    <span class="o">-</span><span class="n">pb</span> <span class="mi">1</span><span class="p">,</span><span class="mi">1024</span> \
    <span class="o">-</span><span class="n">db</span> <span class="mi">4</span><span class="p">,</span><span class="mi">1024</span> \
    <span class="o">-</span><span class="n">db</span> <span class="mi">4</span><span class="p">,</span><span class="mi">2048</span>
</pre></div>
</div>
<p>The above example specifies two prefill buckets: <code class="docutils literal notranslate"><span class="pre">1,512</span></code> and <code class="docutils literal notranslate"><span class="pre">1,1024</span></code>, and two decode buckets: <code class="docutils literal notranslate"><span class="pre">4,1024</span></code> and <code class="docutils literal notranslate"><span class="pre">4,2048</span></code>.
This means that the model can process up to 1024 tokens in input prompts in a single batch,
and generate up to 2048 tokens in 4 batches.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>We highly recommend using a batch size of 1 for prefill buckets. For decode buckets, use larger batch sizes to improve
utilization—preferably powers of two, such as 2, 4, or 8. This is because the prefill phase typically saturates compute
capacity even with small batches, whereas the decode phase suffers from low compute efficiency, as it generates
only one token per request.</p>
</div>
</section>
<section id="using-parallelism-for-large-models">
<h3>Using Parallelism for Large Models<a class="headerlink" href="#using-parallelism-for-large-models" title="Link to this heading">#</a></h3>
<p>To serve large models efficiently, model parallelism is required to distribute the model across multiple NPUs.
Furiosa-LLM supports 3D parallelism, which includes tensor parallelism, pipeline parallelism, and data parallelism.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span> <span class="pre">PE_NUM</span></code> option specifies the number of PEs (Processing Elements) to use for tensor parallelism.
Note that we use the number of PEs as the unit of tensor parallelism because a single RNGD contains 8 partitionable PEs.</p>
<p>The degree of tensor parallelism must be a power of two, starting from 1 (e.g., 1, 2, 4, 8, etc.).
We recommend using 1, 4, or 8 as the tensor parallelism size.
For example, a value of 1 or 4 is recommended for BERT models, while 4 or 8 is recommended for Llama models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span> \
    <span class="o">--</span><span class="n">tensor</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">8</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">--pipeline-parallel-size</span> <span class="pre">DEVICE_NUM</span></code> option specifies the number of devices across which the model should be partitioned for pipeline parallelism.
For example, if you have 4 devices and want to use all of them for pipeline parallelism,
you can specify <code class="docutils literal notranslate"><span class="pre">--pipeline-parallel-size</span> <span class="pre">4</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span> \
    <span class="o">--</span><span class="n">pipeline</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">4</span> \
    <span class="o">--</span><span class="n">tensor</span><span class="o">-</span><span class="n">parallel</span><span class="o">-</span><span class="n">size</span> <span class="mi">8</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">--pipeline-parallel-size</span></code> option can be overriden at model load time.
Therefore, the settings specified at build time serve only as a preferred configuration.
In contrast, the <code class="docutils literal notranslate"><span class="pre">--tensor-parallel-size</span></code> option is fixed at build time and cannot be changed at load time.</p>
</div>
<p>To learn more about model parallelism, please refer to the <a class="reference internal" href="model-parallelism.html#modelparallelism"><span class="std std-ref">Model Parallelism</span></a> section.</p>
</section>
<section id="enabling-trust-remote-code">
<span id="trustremotecode"></span><h3>Enabling <code class="docutils literal notranslate"><span class="pre">trust_remote_code</span></code><a class="headerlink" href="#enabling-trust-remote-code" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">trust_remote_code</span></code> is an option in Hugging Face Transformers that allows you to trust remote code
when loading a model from the Hugging Face Hub. You can use this option when building a model artifact
from remote code on the Hugging Face Hub as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">LGAI</span><span class="o">-</span><span class="n">EXAONE</span><span class="o">/</span><span class="n">EXAONE</span><span class="o">-</span><span class="mf">3.5</span><span class="o">-</span><span class="mf">7.8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span> \
    <span class="o">--</span><span class="n">auto</span><span class="o">-</span><span class="n">bfloat16</span><span class="o">-</span><span class="n">cast</span> \
    <span class="o">--</span><span class="n">trust</span><span class="o">-</span><span class="n">remote</span><span class="o">-</span><span class="n">code</span>
</pre></div>
</div>
</section>
<section id="building-float16-float32-models">
<span id="autobfloat16cast"></span><h3>Building Float16, Float32 Models<a class="headerlink" href="#building-float16-float32-models" title="Link to this heading">#</a></h3>
<p>Furiosa-LLM builds models in bfloat16 format by default. However, if you want to build a model
in float16 or float32 format, you must explicitly request casting to bfloat16
using the <code class="docutils literal notranslate"><span class="pre">--auto-bfloat16-cast</span></code> flag; otherwise, an error will occur.
The example in <a class="reference internal" href="#trustremotecode"><span class="std std-ref">Enabling trust_remote_code</span></a> uses <code class="docutils literal notranslate"><span class="pre">--auto-bfloat16-cast</span></code> option because the EXAONE model uses float16 as dtype.</p>
</section>
<section id="long-context-length-and-chunked-prefill">
<span id="chunkedprefill"></span><h3>Long-context Length and Chunked Prefill<a class="headerlink" href="#long-context-length-and-chunked-prefill" title="Link to this heading">#</a></h3>
<p>Chunked prefill is a technique that splits large prefills into small chunks.
This is an experimental feature and is still under development.
It does not yet support batching a single prefill with multiple decode requests.
However, it can still be useful for long-context length models.
To enable this feature, you need to specify the <code class="docutils literal notranslate"><span class="pre">--prefill-chunk-size</span> <span class="pre">CHUNK_SIZE</span></code> option as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">LGAI</span><span class="o">-</span><span class="n">EXAONE</span><span class="o">/</span><span class="n">EXAONE</span><span class="o">-</span><span class="mf">3.5</span><span class="o">-</span><span class="mf">7.8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">./</span><span class="n">Output</span><span class="o">-</span><span class="n">EXAONE</span><span class="o">-</span><span class="mf">3.5</span><span class="o">-</span><span class="mf">7.8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">-</span><span class="n">tp</span> <span class="mi">8</span> \
    <span class="o">--</span><span class="nb">max</span><span class="o">-</span><span class="n">seq</span><span class="o">-</span><span class="nb">len</span><span class="o">-</span><span class="n">to</span><span class="o">-</span><span class="n">capture</span> <span class="mi">32768</span> \
    <span class="o">--</span><span class="n">prefill</span><span class="o">-</span><span class="n">chunk</span><span class="o">-</span><span class="n">size</span> <span class="mi">8192</span> \
    <span class="o">-</span><span class="n">db</span> <span class="mi">4</span><span class="p">,</span><span class="mi">32768</span> \
    <span class="o">--</span><span class="n">auto</span><span class="o">-</span><span class="n">bfloat16</span><span class="o">-</span><span class="n">cast</span> \
    <span class="o">--</span><span class="n">trust</span><span class="o">-</span><span class="n">remote</span><span class="o">-</span><span class="n">code</span>
</pre></div>
</div>
</section>
<section id="building-quantized-models">
<h3>Building Quantized Models<a class="headerlink" href="#building-quantized-models" title="Link to this heading">#</a></h3>
<p>You can find an example of building a quantized model in the <a class="reference internal" href="model-preparation.html#modelquantization"><span class="std std-ref">Model Quantization (Optional)</span></a> section.</p>
</section>
<section id="building-model-artifacts-in-parallel">
<h3>Building Model Artifacts in Parallel<a class="headerlink" href="#building-model-artifacts-in-parallel" title="Link to this heading">#</a></h3>
<p>Building a large model artifact can take a long time.
To speed up the process, you can use multiple workers to build the model artifact.
The <code class="docutils literal notranslate"><span class="pre">--num-pipeline-builder-workers</span> <span class="pre">[NUM]</span></code> option specifies the number of
workers used to analyze and convert a model graph into multiple execution plans,
each of which serves as input to the Furiosa compiler.
This option is especially useful when building multiple buckets for a model.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">--num-pipeline-builder-workers</span> <span class="pre">NUM</span></code> option requires CPU memory
approximately equal to <cite>NUM</cite> times the total size of the model weights.
Ensure that you have sufficient CPU memory available before using this option.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">--num-compile-workers</span> <span class="pre">NUM</span></code> option specifies the number of workers to use for compiling a model artifact.
This option can speed up the compilation process by distributing the compilation tasks across multiple workers.</p>
<p>The following example shows how to build a model artifact using 4 pipeline builder workers and 4 compile workers.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">build</span> <span class="n">meta</span><span class="o">-</span><span class="n">llama</span><span class="o">/</span><span class="n">Llama</span><span class="o">-</span><span class="mf">3.1</span><span class="o">-</span><span class="mi">8</span><span class="n">B</span><span class="o">-</span><span class="n">Instruct</span> \
    <span class="o">/</span><span class="n">path_to</span><span class="o">/</span><span class="n">artifact</span> \
    <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="n">pipeline</span><span class="o">-</span><span class="n">builder</span><span class="o">-</span><span class="n">workers</span> <span class="mi">4</span> \
    <span class="o">--</span><span class="n">num</span><span class="o">-</span><span class="nb">compile</span><span class="o">-</span><span class="n">workers</span> <span class="mi">4</span>
</pre></div>
</div>
</section>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="model-preparation.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Model Preparation</p>
      </div>
    </a>
    <a class="right-next"
       href="model-parallelism.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Parallelism</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#command-line-tool-vs-artifactbuilder-api">Command-Line Tool vs. ArtifactBuilder API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#examples">Examples</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-examples">Basic Examples</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-context-length">Specifying Context Length</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#specifying-buckets">Specifying Buckets</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#using-parallelism-for-large-models">Using Parallelism for Large Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#enabling-trust-remote-code">Enabling <code class="docutils literal notranslate"><span class="pre">trust_remote_code</span></code></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-float16-float32-models">Building Float16, Float32 Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#long-context-length-and-chunked-prefill">Long-context Length and Chunked Prefill</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-quantized-models">Building Quantized Models</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#building-model-artifacts-in-parallel">Building Model Artifacts in Parallel</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By FuriosaAI, Inc.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025 FuriosaAI Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>