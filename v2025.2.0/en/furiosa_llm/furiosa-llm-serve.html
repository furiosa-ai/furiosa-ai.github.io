
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HTTHGM3MD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-0HTTHGM3MD');
    </script>
    
    <title>OpenAI-Compatible Server &#8212; FuriosaAI Developer Center 2025.2.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=24148a24" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=39353cc5"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'furiosa_llm/furiosa-llm-serve';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Model Preparation" href="model-preparation.html" />
    <link rel="prev" title="Furiosa-LLM" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="2025.2.0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">
<div>
📣 SDK <b>2025.2.0</b> has been released on Apr 25, 2025.
Please check out <a href="https://developer.furiosa.ai/v2025.2.0/en/whatsnew/index.html#furiosa-sdk-2025-2-0-beta2-2025-04-25">the SDK release announcement for version 2025.2.0</a>.
</div>
</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/doc-logo-dark.svg" class="logo__image only-light" alt=""/>
    <img src="../_static/doc-logo-light.svg" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">
            <div class='sidebar-title mr-auto'>
                Furiosa Docs
            </div>
        </p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/rngd.html">FuriosaAI RNGD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/software_stack.html">FuriosaAI’s Software Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/supported_models.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whatsnew/index.html">What’s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/roadmap.html">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../getting_started/prerequisites.html">Installing Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/furiosa_llm.html">Quick Start with Furiosa-LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/furiosa_mlperf.html">Running MLPerf™ Inference Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/upgrade_guide.html">Upgrading FuriosaAI’s Software</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Furiosa-LLM</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Furiosa-LLM</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">OpenAI-Compatible Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-preparation.html">Model Preparation</a></li>
<li class="toctree-l1"><a class="reference internal" href="build-artifacts-by-examples.html">Building Model Artifacts By Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-parallelism.html">Model Parallelism</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="reference.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="reference/llm.html">LLM class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/sampling_params.html">SamplingParams class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/artifact_builder.html">ArtifactBuilder</a></li>



<li class="toctree-l2"><a class="reference internal" href="reference/llm_engine.html">LLMEngine class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/async_llm_engine.html">AsyncLLMEngine class</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="examples.html">Examples</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="examples/llm_chat.html">Chat</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples/llm_chat_with_tools.html">Chat with tools</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cloud Native Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cloud_native_toolkit/intro.html">Cloud Native Toolkit</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cloud_native_toolkit/container.html">Container Support</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cloud_native_toolkit/kubernetes.html">Kubernetes Plugins</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/feature_discovery.html">Installing Furiosa Feature Discovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/device_plugin.html">Installing Furiosa Device Plugin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/metrics_exporter.html">Installing Furiosa Metrics Exporter</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../device_management/system_management_interface.html">Furiosa SMI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_cli.html">Furiosa SMI CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_lib.html">Furiosa SMI Library</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://forums.furiosa.ai">Forums</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">Customer Support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://furiosa.ai">FuriosaAI Homepage</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/latest/en/">Furiosa Gen 1 NPU SDK Doc</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item"><img id='furiosa_logo' width="100" /></div>
      <div class="sidebar-primary-item">

  <p class="copyright">
    
      © Copyright 2025 FuriosaAI Inc.
      <br/>
    
  </p>
</div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button></div>
      
        <div class="header-article-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="header-article-item"><button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>OpenAI-Compatible Server</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2>  </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-openai-api">Using the OpenAI API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-templates">Chat Templates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-calling-support">Tool Calling Support</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-support">Reasoning Support</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-openai-api-parameters">Supported OpenAI API Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-api-post-v1-chat-completions">Chat API (<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/v1/chat/completions</span></code>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#completions-api-post-v1-completions">Completions API (<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/v1/completions</span></code>)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-api-endpoints">Additional API Endpoints</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#models-endpoint">Models Endpoint</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#version-endpoint">Version Endpoint</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-endpoint">Metrics Endpoint</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monitoring-the-openai-compatible-server">Monitoring the OpenAI-Compatible Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#launching-the-openai-compatible-server-container">Launching the OpenAI-Compatible Server Container</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="openai-compatible-server">
<span id="openaiserver"></span><h1>OpenAI-Compatible Server<a class="headerlink" href="#openai-compatible-server" title="Link to this heading">#</a></h1>
<p>In addition to the Python API, Furiosa LLVM offers an OpenAI-compatible server
that hosts a single model and provides two OpenAI-compatible APIs:
<a class="reference external" href="https://platform.openai.com/docs/api-reference/completions">Completions API</a> and
<a class="reference external" href="https://platform.openai.com/docs/api-reference/chat">Chat API</a>.</p>
<p>To launch the server, use the <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">serve</span></code> command with the model
artifact path, as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">furiosa</span><span class="o">-</span><span class="n">llm</span> <span class="n">serve</span> <span class="p">[</span><span class="n">ARTIFACT_PATH</span><span class="p">]</span>
</pre></div>
</div>
<p>The following sections describe how to launch and configure the server
and interact with the server using OpenAI API clients.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This document is based on Furiosa SDK 2025.2.0 (beta0).
The features and APIs described herein are subject to change in the future.</p>
</div>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<p>To use the OpenAI-Compatible server, you need the following:</p>
<ul class="simple">
<li><p>A system with the prerequisites installed (see <a class="reference internal" href="../getting_started/prerequisites.html#installingprerequisites"><span class="std std-ref">Installing Prerequisites</span></a>)</p></li>
<li><p>An installation of <a class="reference internal" href="../getting_started/furiosa_llm.html#installingfuriosallm"><span class="std std-ref">Furiosa-LLM</span></a></p></li>
<li><p>A <a class="reference internal" href="../getting_started/furiosa_llm.html#authorizinghuggingfacehub"><span class="std std-ref">Hugging Face access token</span></a></p></li>
<li><p>A model artifact</p></li>
<li><p>Chat template for chat application (Optional)</p></li>
</ul>
</section>
<section id="using-the-openai-api">
<h2>Using the OpenAI API<a class="headerlink" href="#using-the-openai-api" title="Link to this heading">#</a></h2>
<p>Once the server is running, you can interact with it using an HTTP client,
as shown in the following example:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>curl<span class="w"> </span>http://localhost:8000/v1/chat/completions<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-H<span class="w"> </span><span class="s2">&quot;Content-Type: application/json&quot;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-d<span class="w"> </span><span class="s1">&#39;{</span>
<span class="s1">    &quot;model&quot;: &quot;EMPTY&quot;,</span>
<span class="s1">    &quot;messages&quot;: [{&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;What is the capital of France?&quot;}]</span>
<span class="s1">    }&#39;</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span><span class="p">|</span><span class="w"> </span>python<span class="w"> </span>-m<span class="w"> </span>json.tool
</pre></div>
</div>
<p>You can also use the OpenAI client to interact with the server.
To use the OpenAI client, you need to install the <code class="docutils literal notranslate"><span class="pre">openai</span></code> package first:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>openai
</pre></div>
</div>
<p>The OpenAI client provides two APIs: <code class="docutils literal notranslate"><span class="pre">client.chat.completions</span></code> and
<code class="docutils literal notranslate"><span class="pre">client.completions</span></code>.
To stream responses, you can use the <code class="docutils literal notranslate"><span class="pre">client.chat.completions</span></code>
API with <code class="docutils literal notranslate"><span class="pre">stream=True</span></code>, as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">asyncio</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">AsyncOpenAI</span>

<span class="c1"># Replace the following with your base URL</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8000/v1&quot;</span>
<span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;EMPTY&quot;</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">AsyncOpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">)</span>

<span class="k">async</span> <span class="k">def</span><span class="w"> </span><span class="nf">run</span><span class="p">():</span>
    <span class="n">stream_chat_completion</span> <span class="o">=</span> <span class="k">await</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="s2">&quot;EMPTY&quot;</span><span class="p">,</span>
        <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;Say this is a test&quot;</span><span class="p">}],</span>
        <span class="n">stream</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">async</span> <span class="k">for</span> <span class="n">chunk</span> <span class="ow">in</span> <span class="n">stream_chat_completion</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">chunk</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">delta</span><span class="o">.</span><span class="n">content</span> <span class="ow">or</span> <span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">,</span> <span class="n">flush</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="n">asyncio</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">run</span><span class="p">())</span>
</pre></div>
</div>
<p>By default, the Furiosa-LLM server binds to <code class="docutils literal notranslate"><span class="pre">localhost:8000</span></code>.
You can change the host and port using the <code class="docutils literal notranslate"><span class="pre">--host</span></code> and <code class="docutils literal notranslate"><span class="pre">--port</span></code> options.</p>
</section>
<section id="chat-templates">
<h2>Chat Templates<a class="headerlink" href="#chat-templates" title="Link to this heading">#</a></h2>
<p>To use a language model in a chat application, we need to prepare a structured
string to give as input.
This is essential because the model must understand the conversation’s context,
including the speaker’s role (e.g., “user” and “assistant”) and the
message content.
Just as different models require distinct tokenization methods, they also have
varying input formats for chat.
This is why a chat template is necessary.</p>
<p>Furiosa-LLM supports chat templates based on the Jinja2 template engine, similar
to Hugging Face Transformers.
If the model’s tokenizer includes a built-in chat template,
<code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">serve</span></code> will automatically use it.
However, if the tokenizer lacks a built-in template, or if you want to override
the default, you can specify one using the <code class="docutils literal notranslate"><span class="pre">--chat-template</span></code> parameter.</p>
<p>For reference, you can find a well-structured example of a chat template in the
<a class="reference external" href="https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_1/">Llama 3.1 Model Card</a>.</p>
<p>To launch the server with a custom chat template, use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>furiosa-llm<span class="w"> </span>serve<span class="w"> </span><span class="o">[</span>ARTIFACT_PATH<span class="o">]</span><span class="w"> </span>--chat-template<span class="w"> </span><span class="o">[</span>CHAT_TEMPLATE_PATH<span class="o">]</span>
</pre></div>
</div>
</section>
<section id="tool-calling-support">
<span id="toolcalling"></span><h2>Tool Calling Support<a class="headerlink" href="#tool-calling-support" title="Link to this heading">#</a></h2>
<p>Furiosa-LLM supports tool calling (also known as function calling) for models
trained with this capability.</p>
<p>Within the <code class="docutils literal notranslate"><span class="pre">tool_choice</span></code> options supported by the
<a class="reference external" href="https://platform.openai.com/docs/api-reference/chat/create#chat-create-tool_choice">OpenAI API</a>,
Furiosa-LLM supports <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> and <code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code>.
Future releases will support <code class="docutils literal notranslate"><span class="pre">&quot;required&quot;</span></code> and named function calling.</p>
<p>The system converts model outputs into the OpenAI response format through a
designated parser implementation.
At this time, only the <code class="docutils literal notranslate"><span class="pre">llama3_json</span></code> parser is available.
Additional parsers will be introduced in future releases.</p>
<p>The following command starts the server with tool calling enabled for Llama 3.1 models:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>furiosa-llm<span class="w"> </span>serve<span class="w"> </span><span class="o">[</span>ARTIFACT_PATH<span class="o">]</span><span class="w"> </span>--enable-auto-tool-choice<span class="w"> </span>--tool-call-parser<span class="w"> </span>llama3_json
</pre></div>
</div>
<p>To use the tool calling feature, specify the <code class="docutils literal notranslate"><span class="pre">tools</span></code> and <code class="docutils literal notranslate"><span class="pre">tool_choice</span></code>
parameters. Here’s an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">json</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">base_url</span><span class="o">=</span><span class="s2">&quot;http://localhost:8000/v1&quot;</span><span class="p">,</span> <span class="n">api_key</span><span class="o">=</span><span class="s2">&quot;test&quot;</span><span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">get_weather</span><span class="p">(</span><span class="n">location</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">unit</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;Getting the weather for </span><span class="si">{</span><span class="n">location</span><span class="si">}</span><span class="s2"> in </span><span class="si">{</span><span class="n">unit</span><span class="si">}</span><span class="s2">...&quot;</span>
<span class="n">tool_functions</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;get_weather&quot;</span><span class="p">:</span> <span class="n">get_weather</span><span class="p">}</span>

<span class="n">tools</span> <span class="o">=</span> <span class="p">[{</span>
    <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;function&quot;</span><span class="p">,</span>
    <span class="s2">&quot;function&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;name&quot;</span><span class="p">:</span> <span class="s2">&quot;get_weather&quot;</span><span class="p">,</span>
        <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;Get the current weather in a given location&quot;</span><span class="p">,</span>
        <span class="s2">&quot;parameters&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;object&quot;</span><span class="p">,</span>
            <span class="s2">&quot;properties&quot;</span><span class="p">:</span> <span class="p">{</span>
                <span class="s2">&quot;location&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;description&quot;</span><span class="p">:</span> <span class="s2">&quot;City and state, e.g., &#39;San Francisco, CA&#39;&quot;</span><span class="p">},</span>
                <span class="s2">&quot;unit&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;type&quot;</span><span class="p">:</span> <span class="s2">&quot;string&quot;</span><span class="p">,</span> <span class="s2">&quot;enum&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;celsius&quot;</span><span class="p">,</span> <span class="s2">&quot;fahrenheit&quot;</span><span class="p">]}</span>
            <span class="p">},</span>
            <span class="s2">&quot;required&quot;</span><span class="p">:</span> <span class="p">[</span><span class="s2">&quot;location&quot;</span><span class="p">,</span> <span class="s2">&quot;unit&quot;</span><span class="p">]</span>
        <span class="p">}</span>
    <span class="p">}</span>
<span class="p">}]</span>

<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;meta-llama/Llama-3.1-70B-Instruct&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;What&#39;s the weather like in San Francisco?&quot;</span><span class="p">}],</span>
    <span class="n">tools</span><span class="o">=</span><span class="n">tools</span><span class="p">,</span>
    <span class="n">tool_choice</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span> <span class="c1"># None is also equivalent to &quot;auto&quot;</span>
<span class="p">)</span>

<span class="n">tool_call</span> <span class="o">=</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">tool_calls</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">function</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Function called: </span><span class="si">{</span><span class="n">tool_call</span><span class="o">.</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Arguments: </span><span class="si">{</span><span class="n">tool_call</span><span class="o">.</span><span class="n">arguments</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Result: </span><span class="si">{</span><span class="n">get_weather</span><span class="p">(</span><span class="o">**</span><span class="n">json</span><span class="o">.</span><span class="n">loads</span><span class="p">(</span><span class="n">tool_call</span><span class="o">.</span><span class="n">arguments</span><span class="p">))</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The expected output is as follows.</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Function called: get_weather
Arguments: {&quot;location&quot;: &quot;San Francisco, CA&quot;, &quot;unit&quot;: &quot;F&quot;}
Result: The temperature in San Francisco, CA is 70 °F
</pre></div>
</div>
</section>
<section id="reasoning-support">
<span id="reasoning"></span><h2>Reasoning Support<a class="headerlink" href="#reasoning-support" title="Link to this heading">#</a></h2>
<p>Furiosa-LLM provides support for models with reasoning capabilities, such as Deepseek R1 series. These models follow a structured approach by first conducting reasoning steps and then providing a final answer.</p>
<p>The reasoning process follows this sequence:</p>
<ul class="simple">
<li><p>The model-specific start-of-reasoning token is appended to the input prompt through the chat template.</p></li>
<li><p>The model generates its reasoning.</p></li>
<li><p>Once reasoning is complete, the model outputs an end-of-reasoning token followed by the final answer.</p></li>
</ul>
<p>Since start-of-reasoning and end-of-reasoning tokens are model-specific, we support different reasoning parsers for different models.
Currently, <code class="docutils literal notranslate"><span class="pre">deepseek_r1</span></code> parser is available. This parser expects <code class="docutils literal notranslate"><span class="pre">&lt;token&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;/token&gt;</span></code> as the start-of-reasoning and end-of-reasoning tokens respectively.
Any models that follow the same token scheme (such as Qwen QWQ) can use this parser.</p>
<p>To launch a server with reasoning capabilities for Deepseek R1 series, use the following command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>furiosa-llm<span class="w"> </span>serve<span class="w"> </span><span class="o">[</span>ARTIFACT_PATH<span class="o">]</span><span class="w"> </span>--enable-reasoning<span class="w"> </span>--reasoning-parser<span class="w"> </span>deepseek_r1
</pre></div>
</div>
<p>You can access the reasoning content through these response fields:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">response.choices[].message.reasoning_content</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">response.choices[].delta.reasoning_content</span></code></p></li>
</ul>
<p>Here’s an example that demonstrates how to access the reasoning content:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">openai</span><span class="w"> </span><span class="kn">import</span> <span class="n">OpenAI</span>

<span class="c1"># Replace the following with your base URL</span>
<span class="n">base_url</span> <span class="o">=</span> <span class="s2">&quot;http://localhost:8000/v1&quot;</span>
<span class="n">api_key</span> <span class="o">=</span> <span class="s2">&quot;EMPTY&quot;</span>

<span class="n">client</span> <span class="o">=</span> <span class="n">OpenAI</span><span class="p">(</span><span class="n">api_key</span><span class="o">=</span><span class="n">api_key</span><span class="p">,</span> <span class="n">base_url</span><span class="o">=</span><span class="n">base_url</span><span class="p">)</span>


<span class="n">messages</span> <span class="o">=</span> <span class="p">[{</span><span class="s2">&quot;role&quot;</span><span class="p">:</span> <span class="s2">&quot;user&quot;</span><span class="p">,</span> <span class="s2">&quot;content&quot;</span><span class="p">:</span> <span class="s2">&quot;9.11 and 9.8, which is greater?&quot;</span><span class="p">}]</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">chat</span><span class="o">.</span><span class="n">completions</span><span class="o">.</span><span class="n">create</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;EMPTY&quot;</span><span class="p">,</span>
    <span class="n">messages</span><span class="o">=</span><span class="n">messages</span>
<span class="p">)</span>

<span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="p">,</span> <span class="s2">&quot;reasoning_content&quot;</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Reasoning:&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">reasoning_content</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Answer:&quot;</span><span class="p">,</span> <span class="n">response</span><span class="o">.</span><span class="n">choices</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">message</span><span class="o">.</span><span class="n">content</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="docutils literal notranslate"><span class="pre">reasoning_content</span></code> field is a Furiosa-LLM-specific extension and is not part of the standard OpenAI API.
This field will appear only in responses that contain reasoning content, and
attempting to access this field in responses without reasoning content will raise an <code class="docutils literal notranslate"><span class="pre">AttributeError</span></code>.</p>
</div>
</section>
<section id="supported-openai-api-parameters">
<h2>Supported OpenAI API Parameters<a class="headerlink" href="#supported-openai-api-parameters" title="Link to this heading">#</a></h2>
<p>The following table outlines the supported parameters for Chat and Completions APIs.
Any parameters not listed in the table are unsupported and will be ignored by the server.</p>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Please note that using <code class="docutils literal notranslate"><span class="pre">use_beam_search</span></code> together with <code class="docutils literal notranslate"><span class="pre">stream</span></code> is not
allowed because beam search requires the whole sequence to produce the
output tokens.</p>
</div>
<section id="chat-api-post-v1-chat-completions">
<h3>Chat API (<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/v1/chat/completions</span></code>)<a class="headerlink" href="#chat-api-post-v1-chat-completions" title="Link to this heading">#</a></h3>
<p>Parameters without descriptions inherit their behavior and functionality from the corresponding parameters in <a class="reference external" href="https://platform.openai.com/docs/api-reference/chat">OpenAI Chat API</a>.</p>
<div class="pst-scrollable-table-container"><table class="table table-center">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>model</p></td>
<td><p>string</p></td>
<td></td>
<td><p>Required by the client, but the value is ignored on the server.</p></td>
</tr>
<tr class="row-odd"><td><p>messages</p></td>
<td><p>array</p></td>
<td></td>
<td></td>
</tr>
<tr class="row-even"><td><p>stream</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>stream_options</p></td>
<td><p>object</p></td>
<td><p>null</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>n</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>Currently limited to 1.</p></td>
</tr>
<tr class="row-odd"><td><p>temperature</p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>top_p</p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>best_of</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>use_beam_search</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>top_k</p></td>
<td><p>integer</p></td>
<td><p>-1</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>min_p</p></td>
<td><p>float</p></td>
<td><p>0.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>length_penalty</p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>early_stopping</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>min_tokens</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>max_tokens</p></td>
<td><p>integer</p></td>
<td><p>null</p></td>
<td><p>Legacy parameter superseded by <code class="docutils literal notranslate"><span class="pre">max_completion_tokens</span></code></p></td>
</tr>
<tr class="row-odd"><td><p>max_completion_tokens</p></td>
<td><p>integer</p></td>
<td><p>null</p></td>
<td><p>If null, the server will use the maximum possible length considering the prompt.
The sum of this value and the prompt length must not exceed the model’s maximum context length.</p></td>
</tr>
<tr class="row-even"><td><p>tools</p></td>
<td><p>array</p></td>
<td><p>null</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>tool_choice</p></td>
<td><p>string or object</p></td>
<td><p>null</p></td>
<td><p>Supports named function calling, <code class="docutils literal notranslate"><span class="pre">&quot;none&quot;</span></code>, and <code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code>.
<code class="docutils literal notranslate"><span class="pre">&quot;auto&quot;</span></code> is available only when <code class="docutils literal notranslate"><span class="pre">--enable-auto-tool-choice</span></code> is set.</p></td>
</tr>
<tr class="row-even"><td><p>logprobs (experimental)</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>top_logprobs (experimental)</p></td>
<td><p>integer</p></td>
<td><p>null</p></td>
<td></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="completions-api-post-v1-completions">
<h3>Completions API (<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/v1/completions</span></code>)<a class="headerlink" href="#completions-api-post-v1-completions" title="Link to this heading">#</a></h3>
<p>Parameters without descriptions inherit their behavior and functionality from the corresponding parameters in <a class="reference external" href="https://platform.openai.com/docs/api-reference/completions">OpenAI Completions API</a>.</p>
<div class="pst-scrollable-table-container"><table class="table table-center">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Name</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Default</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>model</p></td>
<td><p>string</p></td>
<td><p>required</p></td>
<td><p>Required by the client, but the value is ignored on the server.</p></td>
</tr>
<tr class="row-odd"><td><p>prompt</p></td>
<td><p>string or array</p></td>
<td><p>required</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>stream</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>stream_options</p></td>
<td><p>object</p></td>
<td><p>null</p></td>
<td></td>
</tr>
<tr class="row-even"><td><p>n</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>Currently limited to 1.</p></td>
</tr>
<tr class="row-odd"><td><p>best_of</p></td>
<td><p>integer</p></td>
<td><p>1</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>temperature</p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>top_p</p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>use_beam_search</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>top_k</p></td>
<td><p>integer</p></td>
<td><p>-1</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>min_p</p></td>
<td><p>float</p></td>
<td><p>0.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>length_penalty</p></td>
<td><p>float</p></td>
<td><p>1.0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>early_stopping</p></td>
<td><p>boolean</p></td>
<td><p>false</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-odd"><td><p>min_tokens</p></td>
<td><p>integer</p></td>
<td><p>0</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
<tr class="row-even"><td><p>max_tokens</p></td>
<td><p>integer</p></td>
<td><p>16</p></td>
<td></td>
</tr>
<tr class="row-odd"><td><p>logprobs (experimental)</p></td>
<td><p>integer</p></td>
<td><p>null</p></td>
<td><p>See <a class="reference internal" href="reference/sampling_params.html#samplingparams"><span class="std std-ref">Sampling Params</span></a>.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="additional-api-endpoints">
<h2>Additional API Endpoints<a class="headerlink" href="#additional-api-endpoints" title="Link to this heading">#</a></h2>
<p>In addition to the Chat and Completions APIs, the Furiosa-LLM server supports the following endpoints.</p>
<section id="models-endpoint">
<span id="modelsendpoint"></span><h3>Models Endpoint<a class="headerlink" href="#models-endpoint" title="Link to this heading">#</a></h3>
<p>The Models API enables you to retrieve information about available models through endpoints that are compatible with OpenAI’s <a class="reference external" href="https://platform.openai.com/docs/api-reference/models">Models API</a>. The following endpoints are supported:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">GET</span> <span class="pre">/v1/models</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">GET</span> <span class="pre">/v1/models/{model_id}</span></code></p></li>
</ul>
<p>You can access these endpoints using the OpenAI client’s <code class="docutils literal notranslate"><span class="pre">models.list()</span></code> and <code class="docutils literal notranslate"><span class="pre">models.retrieve()</span></code> methods.</p>
<p>The response includes the standard <a class="reference external" href="https://platform.openai.com/docs/api-reference/models/object">model object</a> as defined by OpenAI, along with the following Furiosa-LLM-specific extensions:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">artifact_id</span></code>: Unique identifier for the model artifact.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_prompt_len</span></code>: Maximum allowed length of input prompts.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_context_len</span></code>: Maximum allowed length of the total context window.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">runtime_config</span></code>: Model runtime configuration parameters, including bucket specifications.</p></li>
</ul>
</section>
<section id="version-endpoint">
<span id="versionendpoint"></span><h3>Version Endpoint<a class="headerlink" href="#version-endpoint" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">GET</span> <span class="pre">/version</span></code></p>
<p>Exposes version information for the Furiosa SDK components.</p>
</section>
<section id="metrics-endpoint">
<span id="metricsendpoint"></span><h3>Metrics Endpoint<a class="headerlink" href="#metrics-endpoint" title="Link to this heading">#</a></h3>
<p><code class="docutils literal notranslate"><span class="pre">GET</span> <span class="pre">/metrics</span></code></p>
<p>Exposes Prometheus-compatible metrics for monitoring server performance and health.</p>
<p>This endpoint is available when the server is launched with the <code class="docutils literal notranslate"><span class="pre">--enable-metrics</span></code> flag.
See <a class="reference internal" href="#monitoringopenaicompatibleserver"><span class="std std-ref">Monitoring the OpenAI-Compatible Server</span></a> for detailed information about available metrics and their usage.</p>
</section>
</section>
<section id="monitoring-the-openai-compatible-server">
<span id="monitoringopenaicompatibleserver"></span><h2>Monitoring the OpenAI-Compatible Server<a class="headerlink" href="#monitoring-the-openai-compatible-server" title="Link to this heading">#</a></h2>
<p>Furiosa-LLM exposes a Prometheus-compatible metrics endpoint at /metrics,
which provides various metrics compatible with vLLM. These metrics can be
used to monitor LLM serving workloads and the system health.
The metrics endpoint can be enabled with <code class="docutils literal notranslate"><span class="pre">--enable-metrics</span></code> option.</p>
<p>The following table shows Furiosa-LLM-specific collectors and metrics:</p>
<div class="pst-scrollable-table-container"><table class="table table-center">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 40.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Type</p></th>
<th class="head"><p>Metric Labels</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:num_requests_running</span></code></p></td>
<td><p>Gauge</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of requests currently running on RNGD.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:num_requests_waiting</span></code></p></td>
<td><p>Gauge</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of requests waiting to be processed.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:request_received_total</span></code></p></td>
<td><p>Counter</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of received requests in total.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:request_success_total</span></code></p></td>
<td><p>Counter</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of successfully processed requests in total.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:request_failure_total</span></code></p></td>
<td><p>Counter</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of request process failures in total.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:prompt_tokens_total</span></code></p></td>
<td><p>Counter</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Total number of prefill tokens processed.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:generation_tokens_total</span></code></p></td>
<td><p>Counter</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Total number of generation tokens processed.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:time_to_first_token_seconds</span></code></p></td>
<td><p>Histogram</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Time to first token (TTFT) in seconds.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:time_per_output_token_seconds</span></code></p></td>
<td><p>Histogram</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Time per output token (TPOT) in seconds.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:e2e_request_latency_seconds</span></code></p></td>
<td><p>Histogram</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>End-to-end request latency in seconds.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:request_prompt_tokens</span></code></p></td>
<td><p>Histogram</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of prefilled tokens processed per request.</p></td>
</tr>
<tr class="row-odd"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:request_generation_tokens</span></code></p></td>
<td><p>Histogram</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p>Number of generation tokens processed per request.</p></td>
</tr>
<tr class="row-even"><td><p><code class="docutils literal notranslate"><span class="pre">furiosa_llm:request_params_max_tokens</span></code></p></td>
<td><p>Histogram</p></td>
<td><p><code class="docutils literal notranslate"><span class="pre">model_name</span></code></p></td>
<td><p><cite>max_token</cite> request parameter received per request.</p></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="launching-the-openai-compatible-server-container">
<h2>Launching the OpenAI-Compatible Server Container<a class="headerlink" href="#launching-the-openai-compatible-server-container" title="Link to this heading">#</a></h2>
<p>FuriosaAI offers a containerized server that can be used for faster deployment.
Here is an example that launches the Furiosa-LLM server in a Docker container
(replace <code class="docutils literal notranslate"><span class="pre">$HF_TOKEN</span></code> with your Hugging Face Hub token):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>docker run -it --rm --privileged \
    --env HF_TOKEN=$HF_TOKEN \
    -v ./Llama-3.1-8B-Instruct:/model \
    -p 8000:8000 \
    furiosaai/furiosa-llm:latest \
    serve /model --devices &quot;npu:0&quot;
</pre></div>
</div>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Furiosa-LLM</p>
      </div>
    </a>
    <a class="right-next"
       href="model-preparation.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Model Preparation</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#using-the-openai-api">Using the OpenAI API</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-templates">Chat Templates</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tool-calling-support">Tool Calling Support</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#reasoning-support">Reasoning Support</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#supported-openai-api-parameters">Supported OpenAI API Parameters</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#chat-api-post-v1-chat-completions">Chat API (<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/v1/chat/completions</span></code>)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#completions-api-post-v1-completions">Completions API (<code class="docutils literal notranslate"><span class="pre">POST</span> <span class="pre">/v1/completions</span></code>)</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#additional-api-endpoints">Additional API Endpoints</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#models-endpoint">Models Endpoint</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#version-endpoint">Version Endpoint</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#metrics-endpoint">Metrics Endpoint</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#monitoring-the-openai-compatible-server">Monitoring the OpenAI-Compatible Server</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#launching-the-openai-compatible-server-container">Launching the OpenAI-Compatible Server Container</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By FuriosaAI, Inc.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2025 FuriosaAI Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>