
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-0HTTHGM3MD"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-0HTTHGM3MD');
    </script>
    
    <title>Model Preparation Workflow &#8212; FuriosaAI Developer Center 2025.1.0 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=b15d34b7" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=e72a64b0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'furiosa_llm/model-preparation-workflow';</script>
    <link rel="icon" href="../_static/favicon.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="OpenAI-Compatible Server" href="furiosa-llm-serve.html" />
    <link rel="prev" title="Furiosa LLM" href="intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="2025.1.0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search..."
         aria-label="Search..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>
<aside class="bd-header-announcement" aria-label="Announcement">
  <div class="bd-header-announcement__content">
<div>
ðŸ“£ SDK <b>2025.1.0</b> has been released on Feb 24, 2025.
Please checkout <a href="https://developer.furiosa.ai/v2025.1.0/en/whatsnew/index.html#furiosa-sdk-2025-1-0-beta1-2025-02-24">SDK Release Announcement of 2025.1.0</a>.
</div>
</div>
</aside>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/doc-logo-dark.svg" class="logo__image only-light" alt=""/>
    <img src="../_static/doc-logo-light.svg" class="logo__image only-dark pst-js-only" alt=""/>
  
  
    <p class="title logo__title">
            <div class='sidebar-title mr-auto'>            
                Furiosa Docs
            </div>
        </p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../overview/rngd.html">FuriosaAI RNGD</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/software_stack.html">FuriosaAIâ€™s Software Stack</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/supported_models.html">Supported Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../whatsnew/index.html">Whatâ€™s New</a></li>
<li class="toctree-l1"><a class="reference internal" href="../overview/roadmap.html">Roadmap</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../getting_started/prerequisites.html">Installing Prerequisites</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/furiosa_llm.html">Quick Start with Furiosa LLM</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/furiosa_mlperf.html">Running MLPerfâ„¢ Inference Benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/upgrade_guide.html">Upgrading FuriosaAIâ€™s Software</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Furiosa LLM</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="intro.html">Furiosa LLM</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Model Preparation Workflow</a></li>
<li class="toctree-l1"><a class="reference internal" href="furiosa-llm-serve.html">OpenAI-Compatible Server</a></li>
<li class="toctree-l1"><a class="reference internal" href="model-parallelism.html">Model Parallelism</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="reference.html">API Reference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="reference/llm.html">LLM class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/sampling_params.html">SamplingParams class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/artifact_builder.html">ArtifactBuilder</a></li>



<li class="toctree-l2"><a class="reference internal" href="reference/llm_engine.html">LLMEngine class</a></li>
<li class="toctree-l2"><a class="reference internal" href="reference/async_llm_engine.html">AsyncLLMEngine class</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Cloud Native Toolkit</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../cloud_native_toolkit/intro.html">Cloud Native Toolkit</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../cloud_native_toolkit/kubernetes.html">Kubernetes Support</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/feature_discovery.html">Installing Furiosa Feature Discovery</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/device_plugin.html">Installing Furiosa Device Plugin</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/metrics_exporter.html">Installing Furiosa Metrics Exporter</a></li>
<li class="toctree-l2"><a class="reference internal" href="../cloud_native_toolkit/kubernetes/scheduling_npus.html">Scheduling NPUs</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Device Management</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../device_management/system_management_interface.html">Furiosa SMI</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_cli.html">Furiosa SMI CLI</a></li>
<li class="toctree-l2"><a class="reference internal" href="../device_management/system_management_interface/furiosa_smi_lib.html">Furiosa SMI Library</a></li>
</ul>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Customer Support</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://forums.furiosa.ai">Forums</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.atlassian.net/servicedesk/customer/portals/">Customer Support</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other Links</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference external" href="https://furiosa.ai">FuriosaAI Homepage</a></li>
<li class="toctree-l1"><a class="reference external" href="https://furiosa-ai.github.io/docs/latest/en/">Furiosa Gen 1 NPU SDK Doc</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item"><img id='furiosa_logo' width="100" /></div>
      <div class="sidebar-primary-item">

  <p class="copyright">
    
      Â© Copyright 2025 FuriosaAI Inc.
      <br/>
    
  </p>
</div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<button class="btn btn-sm pst-navbar-icon search-button search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
</button></div>
      
        <div class="header-article-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="header-article-item"><button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Model Preparation Workflow</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2>  </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-a-model-from-hugging-face-hub">Loading a Model from Hugging Face Hub</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantizing-a-model">Quantizing a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-model-artifact">Building a Model Artifact</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deploying-model-artifacts">Deploying Model Artifacts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#current-limitations">Current Limitations</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="model-preparation-workflow">
<span id="modelpreparationworkflow"></span><h1>Model Preparation Workflow<a class="headerlink" href="#model-preparation-workflow" title="Link to this heading">#</a></h1>
<p>This document describes how to prepare a model to be served by Furiosa LLM.
At a high level, the workflow consists of the following four steps:</p>
<ol class="arabic simple">
<li><p>Loading a model from Hugging Face Hub or a local disk</p></li>
<li><p>Calibrating the model and exporting the quantized model to a checkpoint</p></li>
<li><p>Building a model artifact</p></li>
<li><p>Deploying the model</p></li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The latest release 2025.1.0 always requires the calibration and quantization steps for all
models. The 2025.2 release will allow BF16 models to run on Furiosa LLM
without the calibration and quantization steps.
The 2nd step will be required only for INT8, FP8, and INT4 models.</p>
</div>
<figure class="align-center">
<a class="only-light reference internal image-reference" href="../_images/model_preparation_workflow.png"><img alt="Model Preparation Workflow" class="only-light" src="../_images/model_preparation_workflow.png" style="width: 900px;" />
</a>
</figure>
<figure class="align-center">
<a class="only-dark reference internal image-reference" href="../_images/model_preparation_workflow.png"><img alt="Model Preparation Workflow" class="only-dark" src="../_images/model_preparation_workflow.png" style="width: 900px;" />
</a>
</figure>
<section id="prerequisites">
<h2>Prerequisites<a class="headerlink" href="#prerequisites" title="Link to this heading">#</a></h2>
<p>Please ensure you meet the prerequisites before starting the model preparation workflow:</p>
<ul class="simple">
<li><p>A system with the prerequisites installed (see <a class="reference internal" href="../getting_started/prerequisites.html#installingprerequisites"><span class="std std-ref">Installing Prerequisites</span></a>)</p></li>
<li><p>An installation of <a class="reference internal" href="../getting_started/furiosa_llm.html#installingfuriosallm"><span class="std std-ref">Furiosa LLM</span></a></p></li>
<li><p>A <a class="reference internal" href="../getting_started/furiosa_llm.html#authorizinghuggingfacehub"><span class="std std-ref">Hugging Face access token</span></a></p></li>
<li><p>Sufficient storage space for model weights, e.g., about 100 GB for the
Llama 3.1 70B model</p></li>
</ul>
</section>
<section id="loading-a-model-from-hugging-face-hub">
<h2>Loading a Model from Hugging Face Hub<a class="headerlink" href="#loading-a-model-from-hugging-face-hub" title="Link to this heading">#</a></h2>
<p>The Hugging Face Hub is a platform with over 900k models and 200k datasets which are open source
and publicly available. You can load a model from the Hugging Face Hub using the
<code class="docutils literal notranslate"><span class="pre">furiosa_llm.optimum</span></code> library. The <code class="docutils literal notranslate"><span class="pre">QuantizerForCausalLM</span></code> class provides a simple API to load a
model from the Hugging Face Hub and a <code class="docutils literal notranslate"><span class="pre">quantize()</span></code> method to calibrate and quantize the model.
<code class="docutils literal notranslate"><span class="pre">QuantizerForCausalLM</span></code> is a subclass of <a class="reference external" href="https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForCausalLM">AutoModelForCausalLM</a>,
so it finds automatically the model class from the Hugging Face model id in the same way as
<code class="docutils literal notranslate"><span class="pre">AutoModelForCausalLM</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">furiosa_llm.optimum</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizerForCausalLM</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">QuantizerForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="quantizing-a-model">
<h2>Quantizing a Model<a class="headerlink" href="#quantizing-a-model" title="Link to this heading">#</a></h2>
<p>Quantization is a popular technique to reduce the required computational and memory resources for
inference while preserving the accuracy of your model by mapping the high precision space of
activations, weights, and KV cache to lower precision INT8, FP8, or INT4 space.</p>
<p>Currently, Furiosa LLM provides the PTQ (Post Training Quantization) technique to quantize a model.
To quantize a model, you need to calibrate the model with a calibration dataset and export the
quantized model to a checkpoint. The <code class="docutils literal notranslate"><span class="pre">create_data_loader</span></code> function creates a dataloader for
calibration by specifying the tokenizer, dataset name or path, the dataset split, the number of
samples, and the maximum sample length.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p><code class="docutils literal notranslate"><span class="pre">create_data_loader</span></code> is based on the <code class="docutils literal notranslate"><span class="pre">datasets</span></code> library,
which provides easy access to datasets for audio, computer vision, and
natural language processing (NLP) tasks.
Learn more on the
<a class="reference external" href="https://huggingface.co/docs/datasets/en/index">datasets documentation</a>
and explore the datasets at <a class="reference external" href="https://huggingface.co/datasets">https://huggingface.co/datasets</a>.</p>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">QuantizerForCausalLM</span></code> class provides a <code class="docutils literal notranslate"><span class="pre">quantize()</span></code> method to quantize
text-generation models.
The first argument of <code class="docutils literal notranslate"><span class="pre">QuantizerForCausalLM</span></code> is the model id of Hugging Face
Hub or the path to the model artifact.
The <code class="docutils literal notranslate"><span class="pre">quantize()</span></code> method takes the quantized model, a data loader, and a
quantization configuration as arguments.</p>
<p>Here is an example of quantizing a model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">furiosa_llm.optimum.dataset_utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">create_data_loader</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">furiosa_llm.optimum</span><span class="w"> </span><span class="kn">import</span> <span class="n">QuantizerForCausalLM</span><span class="p">,</span> <span class="n">QuantizationConfig</span>

<span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;meta-llama/Meta-Llama-3.1-8B-Instruct&quot;</span>

<span class="c1"># Create a dataloader for calibration</span>
<span class="n">dataloader</span> <span class="o">=</span> <span class="n">create_data_loader</span><span class="p">(</span>
    <span class="n">tokenizer</span><span class="o">=</span><span class="n">model_id</span><span class="p">,</span>
    <span class="n">dataset_name_or_path</span><span class="o">=</span><span class="s2">&quot;mit-han-lab/pile-val-backup&quot;</span><span class="p">,</span>
    <span class="n">dataset_split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span>
    <span class="n">num_samples</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="c1"># Increase this number for better calibration</span>
    <span class="n">max_sample_length</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="s2">&quot;./quantized_model&quot;</span>
<span class="c1"># Load a pre-trained model from Hugging Face model hub</span>
<span class="n">quantizer</span> <span class="o">=</span> <span class="n">QuantizerForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
<span class="c1"># Calibrate, quantize the model, and save the quantized model</span>
<span class="n">quantizer</span><span class="o">.</span><span class="n">quantize</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">dataloader</span><span class="p">,</span> <span class="n">QuantizationConfig</span><span class="o">.</span><span class="n">w_f8_a_f8_kv_f8</span><span class="p">())</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">QuantizationConfig.w_f8_a_f8_kv_f8()</span></code> is a quantization configuration
that quantizes the weights, activations, and KV cache to 8-bit floats (FP8).
The quantized model is stored to the <code class="docutils literal notranslate"><span class="pre">save_dir</span></code> directory.</p>
</section>
<section id="building-a-model-artifact">
<span id="buildingmodelartifact"></span><h2>Building a Model Artifact<a class="headerlink" href="#building-a-model-artifact" title="Link to this heading">#</a></h2>
<p>The next step is to build a model artifact, which is a set of files required to
run the model on the Furiosa LLM engine.
A model artifact includes model weights, configuration files, compiled NPU
binary files, and other necessary metadata.</p>
<p>Once a model artifact is built, it can be deployed to any host with a Furiosa
NPU and Furiosa LLM installed without further work.</p>
<p>You can create a model artifact using either the <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API
or the <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span></code> command.
Here is an example using the <a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> API.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">furiosa_llm.artifact.builder</span><span class="w"> </span><span class="kn">import</span> <span class="n">ArtifactBuilder</span>

<span class="n">quantized_model</span> <span class="o">=</span> <span class="s2">&quot;./quantized_model&quot;</span>
<span class="n">compiled_model</span> <span class="o">=</span> <span class="s2">&quot;./Output-Llama-3.1-8B-Instruct&quot;</span>

<span class="n">builder</span> <span class="o">=</span> <span class="n">ArtifactBuilder</span><span class="p">(</span>
        <span class="n">quantized_model</span><span class="p">,</span>
        <span class="n">tensor_parallel_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
        <span class="n">max_seq_len_to_capture</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="c1"># Maximum sequence length covered by LLM engine</span>
<span class="p">)</span>
<span class="n">builder</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">compiled_model</span><span class="p">)</span>
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">ArtifactBuilder</span></code> class provides various options to build a model artifact.
The options include the set of devices, parallelism degrees, prefill buckets,
decode buckets, etc. You can find more details about the arguments in the
<a class="reference internal" href="reference/artifact_builder.html#artifactbuilderclass"><span class="std std-ref">ArtifactBuilder</span></a> section.</p>
<p>Alternatively, you can use the <code class="docutils literal notranslate"><span class="pre">furiosa-llm</span> <span class="pre">build</span></code> command to build a model
artifact.
Below is an example that produces an artifact that uses 4-way tensor
parallelism and will save the compiled artifact into the <code class="docutils literal notranslate"><span class="pre">./Llama-3.1-8B-Instruct</span></code> directory.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>furiosa-llm<span class="w"> </span>build<span class="w"> </span>./quantized_model<span class="w"> </span>./Output-Llama-3.1-8B-Instruct<span class="w"> </span><span class="se">\</span>
<span class="w">    </span>-tp<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--max-seq-len-to-capture<span class="w"> </span><span class="m">1024</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-pipeline-builder-workers<span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="se">\</span>
<span class="w">    </span>--num-compile-workers<span class="w"> </span><span class="m">4</span>
</pre></div>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>To achieve better performance or to run large language models on multiple NPUs,
you can take advantage of model parallelism in Furiosa LLM. To learn more about model
parallelism, please refer to the <a class="reference internal" href="model-parallelism.html#modelparallelism"><span class="std std-ref">Model Parallelism</span></a> section.</p>
</div>
</section>
<section id="deploying-model-artifacts">
<h2>Deploying Model Artifacts<a class="headerlink" href="#deploying-model-artifacts" title="Link to this heading">#</a></h2>
<p>Once you have a model artifact, you can copy and reuse it on any machine with a
Furiosa NPU and Furiosa LLM installed.
To transfer a model artifact:</p>
<ol class="arabic simple">
<li><p>Compress the model artifact directory using your preferred compression tool.</p></li>
<li><p>Copy the file to the target host.</p></li>
<li><p>Uncompress it on the target machine.</p></li>
<li><p>Run the model using either the <a class="reference internal" href="reference/llm.html#llmclass"><span class="std std-ref">LLM class</span></a> or the <a class="reference internal" href="furiosa-llm-serve.html#openaiserver"><span class="std std-ref">OpenAI-Compatible Server</span></a>.</p></li>
</ol>
<p>For quick examples of loading and running model artifacts, refer to the
<a class="reference internal" href="../getting_started/furiosa_llm.html#gettingstartedfuriosallm"><span class="std std-ref">Quick Start with Furiosa LLM</span></a> section.</p>
</section>
<section id="current-limitations">
<h2>Current Limitations<a class="headerlink" href="#current-limitations" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ArtifactBuilder</span></code> currently supports contexts lengths of up to 8k tokens
only, even though Furiosa LLM supports up to 32k.
FuriosaAI provides pre-built artifacts with higher context lengths for
selected models.
This limitation will be removed in the 2025.1 release.</p></li>
<li><p>The current version of ArtifactBuilder requires models quantized by <code class="docutils literal notranslate"><span class="pre">QuantizerForCausalLM</span></code>.
The 2025.2 release will allow BF16 models to run on Furiosa
LLM without the calibration and quantization steps.</p></li>
</ul>
</section>
</section>


                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Furiosa LLM</p>
      </div>
    </a>
    <a class="right-next"
       href="furiosa-llm-serve.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">OpenAI-Compatible Server</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> 
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prerequisites">Prerequisites</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#loading-a-model-from-hugging-face-hub">Loading a Model from Hugging Face Hub</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#quantizing-a-model">Quantizing a Model</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#building-a-model-artifact">Building a Model Artifact</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deploying-model-artifacts">Deploying Model Artifacts</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#current-limitations">Current Limitations</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By FuriosaAI, Inc.
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2025 FuriosaAI Inc.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>